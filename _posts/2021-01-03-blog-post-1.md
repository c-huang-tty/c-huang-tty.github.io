---
title: 'Statistics [03]: Some Common Discrete Distributions'
date: 2021-01-03
permalink: /posts/2021/01/03/some-common-discrete-distributions/
tags:
  - Statistics
---

This post will summarize some of the commonly used discrete distributions.

---
## Bernoulli Distribution
The Bernoulli distribution is a discrete distribution having two possible outcomes labelled by <img src="https://render.githubusercontent.com/render/math?math=n=0"> and <img src="https://render.githubusercontent.com/render/math?math=n=1"> in which <img src="https://render.githubusercontent.com/render/math?math=n=1"> occurs with probability <img src="https://render.githubusercontent.com/render/math?math=p"> and <img src="https://render.githubusercontent.com/render/math?math=n=0"> occurs with probability <img src="https://render.githubusercontent.com/render/math?math=q = 1 - p">. Therefore, it has __probability density function__

<img src="/images/statistics/Bernoulli1.png" alt="drawing" width="170"/>


which can also be written as

<img src="https://render.githubusercontent.com/render/math?math=P(n) = p^n(1-p)^{1-n}">

The __cumulative distribution function__ is

<img src="/images/statistics/Bernoulli2.png" width="170"/>

The performance of a fixed number of trials with fixed probability of success on each trial is known as a __Bernoulli trial__.

The distribution of heads and tails in coin tossing is an example of a Bernoulli distribution with <img src="https://render.githubusercontent.com/render/math?math=p=q=1/2">. The probability of A winning each game in the example give in [this](https://c-huang-tty.github.io/posts/2021/01/01/probability-and-statistics/) post is an example of Bernoulli distribution with <img src="https://render.githubusercontent.com/render/math?math=p=p, q=1-p">.

The Bernoulli distribution is the simplest discrete distribution, and it the building block for other more complicated discrete distributions. 

---
## Binomial Distribution
The binomial distribution gives the discrete probability distribution <img src="https://render.githubusercontent.com/render/math?math=P_p(n\,|\,N)"> of obtaining exactly <img src="https://render.githubusercontent.com/render/math?math=n"> successes out of <img src="https://render.githubusercontent.com/render/math?math=N"> Bernoulli trials. The __binomial distribution__ is therefore given by

<img src="https://render.githubusercontent.com/render/math?math=P_p(n\,|\,N) = \dbinom{N}{n} p^nq^{N-n} = \dfrac{N!}{n!(N-n)!}p^nq^{N-n}">

Given the binomial distribution, the probability of obtaining __more successes than__ the <img src="https://render.githubusercontent.com/render/math?math=n"> observed in a binomial distribution is

<img src="https://render.githubusercontent.com/render/math?math=P(X>n) ={\displaystyle\sum_{k=n%2B1}^N\dbinom{N}{k}p^k(1-p)^{N-k} = I_p(n%2B1, N-n)} ">

where

<img src="https://render.githubusercontent.com/render/math?math=I_p(a, b) = \dfrac{B(p%3Ba,b)}{B(a,b)}">

<img src="https://render.githubusercontent.com/render/math?math=B(a,b)"> is the __beta function__, and <img src="https://render.githubusercontent.com/render/math?math=B(p%3Ba,b)"> is the __incomplete beta function__, with 

<img src="https://render.githubusercontent.com/render/math?math=B(a,b) = \dfrac{(a %2B b - 1)!}{(a-1)!(b-1)!}">

and

<img src="https://render.githubusercontent.com/render/math?math=B(p%3Ba,b) = {\displaystyle \int_0^px^{a-1}(1-x)^{b-1}dx}">

To prove the above relationship, first differentiate both sides wrt <img src="https://render.githubusercontent.com/render/math?math=p">.

<img src="/images/statistics/Binomial1.png" width="450"/>

Then integrate, we get

<img src="/images/statistics/Binomial2.png" width="300"/>

---
## Geometric Distribution
The geometric distribution gives the discrete probability distribution <img src="https://render.githubusercontent.com/render/math?math=P(n)"> of obtaining exactly <img src="https://render.githubusercontent.com/render/math?math=n"> failures of Bernoulli trials before first success. The __geometric distribution__ is therefore given by

<img src="https://render.githubusercontent.com/render/math?math=P(n) = p(1-p)^n = pq^n">

The __cumulative distribution function__ is

<img src="https://render.githubusercontent.com/render/math?math=D(n) = {\displaystyle\sum_{k=0}^nP(k) = p\dfrac{1 - q^{n%2B1}}{1 - q} = 1 - q^{n%2B1}}">

The geometric distribution is __memoryless__. It is a discrete analog of the exponential distribution.

---
## Negative Binomial Distribution
The negative binomial distribution is a combination of binomial distribution and geometric distribution, which gives the probability of <img src="https://render.githubusercontent.com/render/math?math=r-1"> successes and <img src="https://render.githubusercontent.com/render/math?math=x"> failures in <img src="https://render.githubusercontent.com/render/math?math=x%2Br-1"> trials, and success on the <img src="https://render.githubusercontent.com/render/math?math=(x%2Br)\text{th}"> trial. The probability density function is therefore given by

<img src="https://render.githubusercontent.com/render/math?math=P_{p\text{,}r}(x) = p\left[\dbinom{x%2Br-1}{r-1}p^{r-1}(1-p)^{x}\right] = \dbinom{x%2Br-1}{r-1}p^r(1-p)^{x}">

The __cumulative distribution function__ is (Proof?)

<img src="https://render.githubusercontent.com/render/math?math=D(x) = {\displaystyle \sum_{n=0}^x\dbinom{n%2Br-1}{r-1}p^r(1-p)^{n} = I_p(r, x%2B1)}"> 

The negative binomial distribution gets its name from the following relationship ([Proof](https://math.stackexchange.com/questions/85708/negative-exponents-in-binomial-theorem)):

<img src="https://render.githubusercontent.com/render/math?math=\dbinom{k%2Br-1}{k} = (-1)^k\dbinom{-r}{k}">

With this relationship, we have

<img src="https://render.githubusercontent.com/render/math?math={\displaystyle \sum_{n=0}^\infty\dbinom{n%2Br-1}{r-1}p^r(1-p)^{n} = p^r\sum_{n=0}^\infty\dbinom{n%2Br-1}{n}(1-p)^{n} = p^r\sum_{n=0}^\infty\dbinom{-r}{n}(-1)^n(1-p)^{n} = p^rp^{-r} = 1}">

---
## Uniform Distribution
The discrete uniform distribution is also known as the "equally likely outcomes" distribution. Letting a set <img src="https://render.githubusercontent.com/render/math?math=S"> have <img src="https://render.githubusercontent.com/render/math?math=N"> elements, each of them having the same probability. Then, the probability distribution function and cumulative distributions function are respectively

<img src="https://render.githubusercontent.com/render/math?math=P(n) = \dfrac{1}{N}">

<img src="https://render.githubusercontent.com/render/math?math=D(n) = \dfrac{1}{N}">

---
## Poisson Distribution
Possion distribution is often used to describe comparatively rare cases, and when the <img src="https://render.githubusercontent.com/render/math?math=N"> becomes very large in binomial distribution, possion distribution can be a good approximation. 

Viewing the binomial distribution as a function of the expected number of successes

<img src="https://render.githubusercontent.com/render/math?math=\nu = Np">

instead of the sample size <img src="https://render.githubusercontent.com/render/math?math=N"> for fixed <img src="https://render.githubusercontent.com/render/math?math=p">. Then, the equation for binomial distribution becomes

<img src="https://render.githubusercontent.com/render/math?math=P_{\nu\text{/}N}(n\,|\,N) \dfrac{N!}{n!(N-n)!}\left(\dfrac{\nu}{N}\right)^n\left(1 - \dfrac{\nu}{N}\right)^{N-n}">

Letting the sample size N become large, the distribution then approaches

<img src="/images/statistics/Poisson1.png" width="380"/>

which is known as the Poisson distribution.

---
## Hypergeometric Distribution
Let there be <img src="https://render.githubusercontent.com/render/math?math=n"> ways for a "good" selection and <img src="https://render.githubusercontent.com/render/math?math=m"> ways for a "bad" selection out of a total of <img src="https://render.githubusercontent.com/render/math?math=n%2m m"> possibilities. Take <img src="https://render.githubusercontent.com/render/math?math=N"> samples and let <img src="https://render.githubusercontent.com/render/math?math=x_i"> equal 1 if selection <img src="https://render.githubusercontent.com/render/math?math=i"> is successful and 0 if it is not. Let <img src="https://render.githubusercontent.com/render/math?math=x"> be the total number of successful selections,

<img src="https://render.githubusercontent.com/render/math?math=x = {\displaystyle \sum_{i=1}^Nx_i}">

The probability of <img src="https://render.githubusercontent.com/render/math?math=i"> successful selections is

<img src="https://render.githubusercontent.com/render/math?math=P(x=i) = \dfrac{\dbinom{n}{i}\dbinom{m}{N-i}}{\dbinom{m%2Bn}{N}}">

---
## Multinomial Distribution
Let a set of random variates <img src="https://render.githubusercontent.com/render/math?math=X_1, X_2, ..., X_n"> have a probability function

<img src="https://render.githubusercontent.com/render/math?math=P(X_1=x_1, ..., X_n=x_n) = \dfrac{N!}{\prod_{i=1}^{n}x_i}\prod_{i=1}^n\theta_i^{x_i}">

and <img src="https://render.githubusercontent.com/render/math?math=\theta_i"> are constants with <img src="https://render.githubusercontent.com/render/math?math=\theta_i>0"> and 

<img src="https://render.githubusercontent.com/render/math?math={\displaystyle \sum_{i=1}^n\theta_i = 1}">

Then the joint distribution of <img src="https://render.githubusercontent.com/render/math?math=X_1, X_2, ..., X_n"> is a multinomial distribution and <img src="https://render.githubusercontent.com/render/math?math=P(X_1=x_1, ..., X_n=x_n)"> is given by the corresponding coefficient of the multinomial series

<img src="https://render.githubusercontent.com/render/math?math=(\theta_1 %2B \theta_2 %2B ... %2B \theta_n)^N">

---
