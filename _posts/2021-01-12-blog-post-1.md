---
title: 'Statistics [12]: Parameter Hypothesis Test'
date: 2021-01-12
permalink: /posts/2021/01/12/parameter-hypothesis-test/
tags:
  - Statistics
---

Basics of parameter hypothesis test.

---
## Steps of Parameter Hypothesis Test
1. Establish hypothesis 
   - Null hypothesis: <img src="https://render.githubusercontent.com/render/math?math=H_0: \theta\in \Theta_0">
   - Alternate hypothesis: <img src="https://render.githubusercontent.com/render/math?math=H_1: \theta\in \Theta_1">
2. Choose the test statistics and give the form of the rejection region <img src="https://render.githubusercontent.com/render/math?math=W">.
3. Decide the significance level <img src="https://render.githubusercontent.com/render/math?math=\alpha"> and rejection region (acceptance region).
   - <img src="https://render.githubusercontent.com/render/math?math=\alpha = \max\{P(\text{reject} \ \ H_0|H_0 \ \ \text{ is true})\} = \max\{P_{\theta}(x\in W),\theta \in \Theta_0\}">

### Example 1
Assume a factory is producing a product obeying the norrmal distribution <img src="https://render.githubusercontent.com/render/math?math=N(\mu,2^2)">, <img src="https://render.githubusercontent.com/render/math?math=\mu"> is the average quality standard, <img src="https://render.githubusercontent.com/render/math?math=\mu>10"> is considered as high quality. To test the quality of the products, the buyer samples 16 products at random from a batch of products, with the following quality standard:

<img src="https://render.githubusercontent.com/render/math?math=9.6, 9.2, 9.3, 9.8, 10.1, 8.5, 9.8, 8.4, 9.2, 9.1, 8.7, 9.3, 8.6, 9.8, 9.2, 10.2">

Test whether the products are in high quality.

__Solution__. 
Firstly, establish hypothesis <img src="https://render.githubusercontent.com/render/math?math=H_0: \mu \geq 10,\ \ H_1: \mu < 10">

Secondly, choose <img src="https://render.githubusercontent.com/render/math?math=\bar{X}"> as test statistic, <img src="https://render.githubusercontent.com/render/math?math=\bar{X} = N\left(\mu, 0.5^2\right)"> and the form of the rejection region would be 

<img src="https://render.githubusercontent.com/render/math?math=\{(x_1,x_2,...,x_n): \bar{x} < c\}">

Thirdly, decide the significance level and rejection region.

<img src="https://render.githubusercontent.com/render/math?math=P(\bar{x} < c | \mu = 10) = P\left(\dfrac{\bar{x} - 10}{0.5} < \dfrac{c-10}{0.5}\right) = \alpha">

<img src="https://render.githubusercontent.com/render/math?math=\dfrac{c-10}{0.5} = u_{\alpha} = -u_{1-\alpha} \Rightarrow c = 10 - \dfrac{u_{1-\alpha}}{2}">

Hence, the rejection region is 

<img src="https://render.githubusercontent.com/render/math?math=\{(x_1,x_2,...,x_n): \bar{x} < 10 - \dfrac{u_{1-\alpha}}{2}\}">

When <img src="https://render.githubusercontent.com/render/math?math=\alpha = 0.05">, the rejection region would be <img src="https://render.githubusercontent.com/render/math?math=\{(x_1,x_2,...,x_n): \bar{x} < 9.1775\}">.

---
## Two Types of Errors
There are two types of mistakes in hypothesis testing:
1. False rejection (type I error): <img src="https://render.githubusercontent.com/render/math?math=\alpha(\theta) = \{P(\text{reject} \ \ H_0|H_0 \ \ \text{ is true})\} = \{P_{\theta}(x\in W),\theta \in \Theta_0\}">
2. False acceptance (type II error): <img src="https://render.githubusercontent.com/render/math?math=\beta(\theta) = \{P(\text{accept} \ \ H_0|H_1 \ \ \text{ is true})\} = \{P_{\theta}(x\in \bar{W}),\theta \in \Theta_1\}">

### Example 2
A factory is producing screws with tandard length of 35 mm, which obeys normal distribution <img src="https://render.githubusercontent.com/render/math?math=N(\mu,3^2)">. Assume the sample size is 36, <img src="https://render.githubusercontent.com/render/math?math=H_0: \mu = 35, H_1: \mu\neq 35">, rejection region is <img src="https://render.githubusercontent.com/render/math?math=W=\{\bar{x}:|\bar{x}-35|>1\}">, find the type I error, and find type II error when <img src="https://render.githubusercontent.com/render/math?math=\mu = 36">.

__Solution__. Test statistic is <img src="https://render.githubusercontent.com/render/math?math=\bar{X} \sim N\left(\mu,0.5^2\right)">.

False rejection rate would be 

<img src="https://render.githubusercontent.com/render/math?math=\alpha = P(|\bar{X} - 35| > 1|\mu = 35) = 1 - P(|\bar{X} - 35| \leq 1|\mu = 35)">

<img src="https://render.githubusercontent.com/render/math?math== 1 - P\left(-2\leq \dfrac{\bar{X}-35}{0.5} \leq 2|\mu = 35\right) = 1 - (\Phi(2)-\Phi(-2)) = 0.0455">

False acceptance rate would be 

<img src="https://render.githubusercontent.com/render/math?math=\beta = P(|\bar{X} - 35| > 1|\mu = 36) = P(-1\leq \bar{X}\leq 1 |\mu = 36)">

<img src="https://render.githubusercontent.com/render/math?math== P\left(-4\leq \dfrac{\bar{X}-36}{0.5} \leq 0|\mu = 36\right) = \Phi(0)-\Phi(-4) = 0.5">

---
## Three Types of Hypothesis Test Problems
1. One-tailed test: <img src="https://render.githubusercontent.com/render/math?math=H_0:\mu\leq \mu_0, \ \ H_1:\mu > \mu_0">
2. One-tailed test: <img src="https://render.githubusercontent.com/render/math?math=H_0:\mu\geq \mu_0, \ \ H_1:\mu < \mu_0">
3. Two-tailed test: <img src="https://render.githubusercontent.com/render/math?math=H_0:\mu= \mu_0, \ \ H_1:\mu \neq \mu_0">

### Example 3
For normal distribution <img src="https://render.githubusercontent.com/render/math?math=N(\mu,\sigma^2)">, <img src="https://render.githubusercontent.com/render/math?math=\mu"> is unknown and <img src="https://render.githubusercontent.com/render/math?math=\sigma^2"> is known, perform hypothesis to <img src="https://render.githubusercontent.com/render/math?math=\mu">.

__Solution__. Test statistic <img src="https://render.githubusercontent.com/render/math?math=u = \dfrac{\bar{x}-\mu_0}{\sigma/\sqrt{n}} = \dfrac{\sqrt{n}(\bar{x}-\mu_0)}{\sigma}\sim N(0,1)">

<img src="https://render.githubusercontent.com/render/math?math=H_0:\mu= \mu_0, \ \ H_1:\mu \neq \mu_0 \Rightarrow \bar{x} > \mu_0 %2B \dfrac{\sigma}{\sqrt{n}}u_{1-{\alpha}\text{/}{2}}\ \  \text{or}\ \  \bar{x} < \mu_0 - \dfrac{\sigma}{\sqrt{n}}u_{1-{\alpha}\text{/}{2}}"> 

<img src="https://render.githubusercontent.com/render/math?math=H_0:\mu\geq \mu_0, \ \ H_1:\mu < \mu_0 \Rightarrow \bar{x} < \mu_0 - \dfrac{\sigma}{\sqrt{n}}u_{1-{\alpha}}"> 

<img src="https://render.githubusercontent.com/render/math?math=H_0:\mu\leq \mu_0, \ \ H_1:\mu > \mu_0 \Rightarrow \bar{x} > \mu_0 %2B \dfrac{\sigma}{\sqrt{n}}u_{1-{\alpha}}"> 

### Example 4
For normal distribution <img src="https://render.githubusercontent.com/render/math?math=N(\mu,\sigma^2)">, <img src="https://render.githubusercontent.com/render/math?math=\mu"> is unknown and <img src="https://render.githubusercontent.com/render/math?math=\sigma^2"> is unknown, perform hypothesis to <img src="https://render.githubusercontent.com/render/math?math=\mu">.

__Solution__. Test statistic <img src="https://render.githubusercontent.com/render/math?math=t = \dfrac{\bar{x}-\mu_0}{s/\sqrt{n}} = \dfrac{\sqrt{n}(\bar{x}-\mu_0)}{s}\sim t(n-1)">

<img src="https://render.githubusercontent.com/render/math?math=H_0:\mu= \mu_0, \ \ H_1:\mu \neq \mu_0 \Rightarrow \bar{x} > \mu_0 %2B \dfrac{s}{\sqrt{n}}t_{1-{\alpha}\text{/}{2}}\ \  \text{or}\ \  \bar{x} < \mu_0 - \dfrac{s}{\sqrt{n}}t_{1-{\alpha}\text{/}{2}}"> 

<img src="https://render.githubusercontent.com/render/math?math=H_0:\mu\geq \mu_0, \ \ H_1:\mu < \mu_0 \Rightarrow \bar{x} < \mu_0 - \dfrac{s}{\sqrt{n}}t_{1-{\alpha}}"> 

<img src="https://render.githubusercontent.com/render/math?math=H_0:\mu\leq \mu_0, \ \ H_1:\mu > \mu_0 \Rightarrow \bar{x} > \mu_0 %2B \dfrac{s}{\sqrt{n}}t_{1-{\alpha}}"> 

---
## p Value
<img src="https://render.githubusercontent.com/render/math?math=p"> value of a test refers to the smallest significance level obtained through the samples at which the null hypothesis would be rejected. 

### Example 5
Assume the population obeys the normal distribution <img src="https://render.githubusercontent.com/render/math?math=N(\mu,3^2)"> and the sample size is 36, <img src="https://render.githubusercontent.com/render/math?math=H_0: \mu = 35, H_1: \mu\neq 35">. Noe suppose the sample value is 36.5 mm, calculate its <img src="https://render.githubusercontent.com/render/math?math=p"> value.

__Solution__. 

<img src="https://render.githubusercontent.com/render/math?math=p = P(|\bar{X} - 35|\geq 1.5|\mu=35) = 1 - P(|\bar{X}-35| < 1.5 | \mu=35)">

<img src="https://render.githubusercontent.com/render/math?math== 1 - P\left(-3 < \dfrac{\bar{X}-35}{0.5} < 3|\mu=35\right) = 1 - (\Phi(3)-\Phi(-3)) = 0.0027 ">

---
## Suppliment: Neyman-Pearson Lemma 
Let <img src="https://render.githubusercontent.com/render/math?math=C"> be a critical regionfor a hypothesis test with significance level <img src="https://render.githubusercontent.com/render/math?math=\alpha">, where the hypothesis are <img src="https://render.githubusercontent.com/render/math?math=H_0: \theta = \theta_0"> and <img src="https://render.githubusercontent.com/render/math?math=H_1: \theta=\theta_1">. Then we say that <img src="https://render.githubusercontent.com/render/math?math=C"> is a *best critical region of size* <img src="https://render.githubusercontent.com/render/math?math=\alpha"> if whenever <img src="https://render.githubusercontent.com/render/math?math=D"> is another critical region with size <img src="https://render.githubusercontent.com/render/math?math=\alpha">, there is <img src="https://render.githubusercontent.com/render/math?math=P(C|\theta=\theta_1)\geq P(D|\theta=\theta_1)">.

What this means is that if the alternative hypothesis is true, then the probability that we reject the null hypothesis is greatest if we use the critical region <img src="https://render.githubusercontent.com/render/math?math=C">.

### Neyman-Pearson Lemma 
Let <img src="https://render.githubusercontent.com/render/math?math=CX_1,X_2,...,X_n"> be a sample from a distribution with pdf <img src="https://render.githubusercontent.com/render/math?math=f(x%3B \theta)">, and let <img src="https://render.githubusercontent.com/render/math?math=L(\theta) {\displaystyle \prod_{i=1}^n f(x_i%3B\theta)}">. If there exists a positive constant <img src="https://render.githubusercontent.com/render/math?math=k"> and a region <img src="https://render.githubusercontent.com/render/math?math=C"> such that (1) <img src="https://render.githubusercontent.com/render/math?math=P((X_1,X_2,...,X_n)\in C|\theta=\theta_0) = \alpha">, (2) <img src="https://render.githubusercontent.com/render/math?math=\dfrac{L(\theta_0)}{L(\theta_1)}\leq k"> for <img src="https://render.githubusercontent.com/render/math?math=(X_1,X_2,...,X_n)\in C">, and (3) <img src="https://render.githubusercontent.com/render/math?math=\dfrac{L(\theta_0)}{L(\theta_1)}\geq k"> for <img src="https://render.githubusercontent.com/render/math?math=(X_1,X_2,...,X_n)\in C^\text{'}">, then <img src="https://render.githubusercontent.com/render/math?math=C"> is the best critical region of size <img src="https://render.githubusercontent.com/render/math?math=\alpha"> for testing <img src="https://render.githubusercontent.com/render/math?math=H_0: \theta=\theta_0"> against <img src="https://render.githubusercontent.com/render/math?math=H_1: \theta=\theta_1">.

### Example 6
Let <img src="https://render.githubusercontent.com/render/math?math=CX_1,X_2,...,X_n"> be a random sample from a normal distribution having known variance <img src="https://render.githubusercontent.com/render/math?math=\sigma^2">. Consider two simple hypothesis

<img src="https://render.githubusercontent.com/render/math?math=H_0: \mu=\mu_0,\ \ H_1: \mu=\mu_1">

where <img src="https://render.githubusercontent.com/render/math?math=\mu_0"> and <img src="https://render.githubusercontent.com/render/math?math=\mu_1"> are given constants. Let the significance level be prescribed. The Neyman-Pearson Lemma states that among all tests with significance level <img src="https://render.githubusercontent.com/render/math?math=\alpha">, the tests that rejects for small values of the lieklihood ratio is most powerful. We calculate the likelihood ratio

<img src="https://render.githubusercontent.com/render/math?math=\dfrac{f_0(X)}{f_1(X)} ={\displaystyle \dfrac{\exp\left[-\dfrac{1}{2\sigma^2}\sum_{i=1}^n(X_i-\mu_0)^2\right]}{\exp\left[-\dfrac{1}{2\sigma^2}\sum_{i=1}^n(X_i-\mu_1)^2\right]} }">

Ignore the constant term <img src="https://render.githubusercontent.com/render/math?math=\dfrac{1}{2\sigma^2}">

<img src="https://render.githubusercontent.com/render/math?math=\dfrac{f_0(X)}{f_1(X)} = {\displaystyle \exp\left[ \sum_{i=1}^n(2X_i(\mu_0-\mu_1) %2B \mu_1^2 - \mu_0^2)\right] = \exp\left[2n\bar{X}(\mu_0-\mu_1) %2B n\mu_1^2 - n\mu_0^2)\right]}">

If <img src="https://render.githubusercontent.com/render/math?math=\mu_0-\mu_1 > 0">, the likelihood ratio is small if <img src="https://render.githubusercontent.com/render/math?math=\bar{X}"> is small; if <img src="https://render.githubusercontent.com/render/math?math=\mu_0-\mu_1 < 0">, the likelihood ratio is small if <img src="https://render.githubusercontent.com/render/math?math=\bar{X}"> is large. Now assume the latter case, the Neyman-Pearson lemma thus tells us that the most powerful test rejects for <img src="https://render.githubusercontent.com/render/math?math=\bar{X} > x_0"> for some <img src="https://render.githubusercontent.com/render/math?math=x_0">, we choose <img src="https://render.githubusercontent.com/render/math?math=x_0"> so as to give the test the desired significance level <img src="https://render.githubusercontent.com/render/math?math=\alpha">. Since

<img src="https://render.githubusercontent.com/render/math?math=P(\bar{X} > x_0) = P\left(\dfrac{\bar{X}-\mu_0}{\sigma/\sqrt{n}} > \dfrac{\bar{x_0}-\mu_0}{\sigma/\sqrt{n}}\right)">

We can solve

<img src="https://render.githubusercontent.com/render/math?math=\dfrac{\bar{x_0}-\mu_0}{\sigma/\sqrt{n}} = z(\alpha) = u_{\alpha}">

for <img src="https://render.githubusercontent.com/render/math?math=x_0"> to find the rejection region for a significance level <img src="https://render.githubusercontent.com/render/math?math=\alpha">. 

---
## Table of Contents
- [Probability vs Statistics](https://c-huang-tty.github.io/posts/2021/01/01/probability-and-statistics/)
- [Shakespear's New Poem](https://c-huang-tty.github.io/posts/2021/01/02/application-of-statistics/)
- [Some Common Discrete Distributions](https://c-huang-tty.github.io/posts/2021/01/03/some-common-discrete-distributions/)
- [Some Common Continuous Distributions](https://c-huang-tty.github.io/posts/2021/01/04/some-common-continuous-distributions/)
- [Statistical Quantities](https://c-huang-tty.github.io/posts/2021/01/05/statistical-quantities/)
- [Order Statistics](https://c-huang-tty.github.io/posts/2021/01/06/order-statistics/)
- [Multivariate Normal Distributions](https://c-huang-tty.github.io/posts/2021/01/07/multivariate-normal-distributions/)
- [Conditional Distributions and Expectation](https://c-huang-tty.github.io/posts/2021/01/08/conditonal-distributions-and-expectation/)
- [Problem Set [01] - Probabilities](https://c-huang-tty.github.io/posts/2021/01/21/problem-set-probabilities/)
- [Parameter Point Estimation](https://c-huang-tty.github.io/posts/2021/01/09/parameter-point-estimation/)
- [Evaluation of Point Estimation](https://c-huang-tty.github.io/posts/2021/01/10/evaluation-point-estimation/)
- [Parameter Interval Estimation](https://c-huang-tty.github.io/posts/2021/01/11/parameter-interval-estimation/)
- [Problem Set [02] - Parameter Estimation](https://c-huang-tty.github.io/posts/2021/01/22/problem-set-parameter-estimation/)
- [Parameter Hypothesis Test](https://c-huang-tty.github.io/posts/2021/01/12/parameter-hypothesis-test/)
- [t Test](https://c-huang-tty.github.io/posts/2021/01/13/t-test/)
- [Chi-Squared Test](https://c-huang-tty.github.io/posts/2021/01/14/chi-squared-test/)
- [Analysis of Variance](https://c-huang-tty.github.io/posts/2021/01/15/analysis-of-variance/)
- [Summary of Statistical Tests](https://c-huang-tty.github.io/posts/2021/01/16/summary-of-statistical-tests/)
- [Python [01] - Data Representation](https://c-huang-tty.github.io/posts/2021/01/17/statistics-python-data-representation/)
- [Python [02] - t Test & F Test](https://c-huang-tty.github.io/posts/2021/01/18/statistics-python-t-F-test/)
- [Python [03] - Chi-Squared Test](https://c-huang-tty.github.io/posts/2021/01/19/statistics-chi-squared-test/)
- [Experimental Design](https://c-huang-tty.github.io/posts/2021/01/20/experimental-design/)
- [Monte Carlo](https://c-huang-tty.github.io/posts/2021/01/23/monte-carlo/)
- [Variance Reducing Techniques](https://c-huang-tty.github.io/posts/2021/01/24/variance-reducing-techniques/)
- [From Uniform to General Distributions](https://c-huang-tty.github.io/posts/2021/01/25/from-uniform-to-general-distributions/)
- [Problem Set [03] - Monte Carlo](https://c-huang-tty.github.io/posts/2021/01/26/problem-set-monte-carlo/)
- [Unitary Regression Model](https://c-huang-tty.github.io/posts/2021/01/27/unitary-regression-model/)
- [Multiple Regression Model](https://c-huang-tty.github.io/posts/2021/01/28/multiple-regression-model/)
- [Factor and Principle Component Analysis](https://c-huang-tty.github.io/posts/2021/01/29/factor-principle-component-analysis/)
- [Clustering Analysis](https://c-huang-tty.github.io/posts/2021/01/30/clustering-analysis/)
- [Summary](https://c-huang-tty.github.io/posts/2021/01/31/summary/)
