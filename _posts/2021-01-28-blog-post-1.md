---
title: 'Statistics [28]: Multiple Regression Model'
date: 2021-01-28
permalink: /posts/2021/01/28/multiple-regression-model/
tags:
  - Statistics
---

The purpose of multiple regression model is to estimate the dependent variable (response variable) using multiple independent variables (explanatory variables).

---
## Multiple Regression Model

### Notation
Suppose there are <img src="https://render.githubusercontent.com/render/math?math=n"> subjects and data is collected from these subjects. 

Data on the response variable is <img src="https://render.githubusercontent.com/render/math?math=y_1,y_2,...,y_n">, which is represented by the column vector <img src="https://render.githubusercontent.com/render/math?math=Y=(y_1,y_2,...,y_n)^T">. 

Data on the <img src="https://render.githubusercontent.com/render/math?math=j\text{th}\,(j=1,2,...,p)"> explanatory variable <img src="https://render.githubusercontent.com/render/math?math=x_j"> is <img src="https://render.githubusercontent.com/render/math?math=x_{1j},x_{2j},...,x_{nj}">, which is represented by the <img src="https://render.githubusercontent.com/render/math?math=n\times p"> matrix <img src="https://render.githubusercontent.com/render/math?math=X">. The <img src="https://render.githubusercontent.com/render/math?math=i\text{th}"> row of <img src="https://render.githubusercontent.com/render/math?math=X"> has data collected from the <img src="https://render.githubusercontent.com/render/math?math=i\text{th}"> subject and the <img src="https://render.githubusercontent.com/render/math?math=j\text{th}"> column of <img src="https://render.githubusercontent.com/render/math?math=X"> has data for the <img src="https://render.githubusercontent.com/render/math?math=j\text{th}"> variable.

### The Linear Model
The linear model is assumed to have the following form:

<img src="https://render.githubusercontent.com/render/math?math=y_i = \beta_1x_{i1} %2B \beta_2x_{i2} %2B \cdots %2B \beta_px_{ip}%2B e_i,\ \ i=1,2,...,n">

where <img src="https://render.githubusercontent.com/render/math?math=e_i \sim N(0,\sigma^2)">.

In matrix notation, the model can be written as 

<img src="https://render.githubusercontent.com/render/math?math=Y = X\beta %2B e">

### The Intercept Term
The linear model stipulates that <img src="https://render.githubusercontent.com/render/math?math=E(y_i) = \beta_1x_{i1} %2B \beta_2x_{i2} %2B \cdots %2B \beta_px_{ip}">, which implies that when <img src="https://render.githubusercontent.com/render/math?math=x_{i1}=x_{i2}=\cdots = x_{ip}=0">, then <img src="https://render.githubusercontent.com/render/math?math=E(y_i)=0">. This is not a reasonable assumption. Therefore, we usually add a intercept term to the model such that 

<img src="https://render.githubusercontent.com/render/math?math=E(y_i) = \beta_0 %2B \beta_1x_{i1} %2B \beta_2x_{i2} %2B \cdots %2B \beta_px_{ip}">

Further, let <img src="https://render.githubusercontent.com/render/math?math=x_{i0}=1">, the above equation can be rewritten as 

<img src="https://render.githubusercontent.com/render/math?math=E(y_i) =\beta_0x_{i0}%2B \beta_1x_{i1} %2B \beta_2x_{i2} %2B \cdots %2B \beta_px_{ip}">

In matrix notation,

<img src="https://render.githubusercontent.com/render/math?math=Y = X\beta %2B e">

where <img src="https://render.githubusercontent.com/render/math?math=X"> now denotes the <img src="https://render.githubusercontent.com/render/math?math=n\times (p%2B 1)"> matrix whose first column consists of all ones and <img src="https://render.githubusercontent.com/render/math?math=\beta = (\beta_0,beta_1,...\beta_p)^T">.

---
## Estimation
Similar to the unitary regression model, <img src="https://render.githubusercontent.com/render/math?math=\beta"> is estimated by minimizing the residual sum of squares:

<img src="https://render.githubusercontent.com/render/math?math=S(\beta) = {\displaystyle \sum_{i=1}^n(y_i-\beta_0-\beta_1x_{i1}-\cdots -\beta_px_{ip})^2}">

In matrix notation,

<img src="https://render.githubusercontent.com/render/math?math=S(\beta) = \left\|Y-X\beta\right\|^2">

Using this notation, we have 

<img src="https://render.githubusercontent.com/render/math?math=S(\beta) = (Y-X\beta)^T(Y-X\beta) = Y^TY - 2\beta^TX^TY %2B \beta^TX^TX\beta">

Take partial derivatives with respect to <img src="https://render.githubusercontent.com/render/math?math=\beta_i"> yields

<img src="https://render.githubusercontent.com/render/math?math=\nabla S(\beta) = 2X^TX\beta - 2X^TY = 0 \Rightarrow X^TX\beta = X^TY">

This gives <img src="https://render.githubusercontent.com/render/math?math=p"> linear equations for the <img src="https://render.githubusercontent.com/render/math?math=p"> unknown <img src="https://render.githubusercontent.com/render/math?math=\beta_i,i=1,2,...,p">. The solution is denoted as <img src="https://render.githubusercontent.com/render/math?math=\hat{\beta} = (\hat{\beta}_1,\hat{\beta}_2,...,\hat{\beta}_p)^T">.

As <img src="https://render.githubusercontent.com/render/math?math=X^TY"> lies in the column space of <img src="https://render.githubusercontent.com/render/math?math=X^T">, there is always solution(s). And if <img src="https://render.githubusercontent.com/render/math?math=X^TX"> is invertible, there will be a unique solution, which is given by <img src="https://render.githubusercontent.com/render/math?math=\hat{\beta} = (X^TX)^{-1}X^TY">.

In fact, <img src="https://render.githubusercontent.com/render/math?math=X^TX"> being invertible is equivalent to <img src="https://render.githubusercontent.com/render/math?math=\text{rank}(X) = p%2B 1">. Thus when <img src="https://render.githubusercontent.com/render/math?math=X^TX"> is non-invertible, <img src="https://render.githubusercontent.com/render/math?math=\text{rank}(X) < p%2B 1">. In other words, some columns of <img src="https://render.githubusercontent.com/render/math?math=X"> is a linear combination of the other columns. Hence, some explanatory variables would be redundant. 

---
## Properties of the Estimator
Assume <img src="https://render.githubusercontent.com/render/math?math=X^TX"> is invertible, the estimator <img src="https://render.githubusercontent.com/render/math?math=\hat{\beta}"> has the following properties.

### Linearity 
An estimator is said to be linear if it can be written as <img src="https://render.githubusercontent.com/render/math?math=AY">.

### Unbiasedness
<img src="https://render.githubusercontent.com/render/math?math=E(\hat{\beta}) = E((X^TX)^{-1}X^TY) = (X^TX)^{-1}X^TE(Y) = (X^TX)^{-1}X^TX\beta = \beta">

### Covariance
<img src="https://render.githubusercontent.com/render/math?math=cov(\hat{\beta}) = cov((X^TX)^{-1}X^TY) = (X^TX)^{-1}X^Tcov(Y)X(X^TX)^{-1} = \sigma^2(X^TX)^{-1}">

where 

<img src="https://render.githubusercontent.com/render/math?math=cov(Y) = E[(Y-E(Y))(Y-E(Y))^T] = E[(X\beta%2B e - X\beta)(X\beta%2B e - X\beta)^T] = \sigma^2I">

### Optimality
The Gaussian-Markov Theorem states that <img src="https://render.githubusercontent.com/render/math?math=\beta"> is the best linear unbiased estimator.


---
## Fitted Values
Given the model, the fitted value can be written in matrix form as

<img src="https://render.githubusercontent.com/render/math?math=\hat{Y} = X\hat{\beta} = X(X^TX)^{-1}X^TY">

which can be seen as the orthogonal projection of <img src="https://render.githubusercontent.com/render/math?math=Y"> onto the column space of <img src="https://render.githubusercontent.com/render/math?math=X">.

Let <img src="https://render.githubusercontent.com/render/math?math=H = X(X^TX)^{-1}X^T"> such that <img src="https://render.githubusercontent.com/render/math?math=\hat{Y} = HY">, the matrix <img src="https://render.githubusercontent.com/render/math?math=H"> is called the __Hat Matrix__, which has the following properties:
- It is summetric
- It is idempotent, <img src="https://render.githubusercontent.com/render/math?math=H^2=H"> 
- <img src="https://render.githubusercontent.com/render/math?math=HX=X">
- <img src="https://render.githubusercontent.com/render/math?math=\text{rank}(H) = \text{rank}(X)">  

With these properties, we can easily get

<img src="https://render.githubusercontent.com/render/math?math=E(\hat{Y}) = E(HY) = HE(Y) = HX\beta = X\beta = E(Y)">

<img src="https://render.githubusercontent.com/render/math?math=cov(\hat{Y}) = cov(HY) = Hcov(Y)H^T = \sigma^2H"> 

---
## Residual
The residual of multiple regression

<img src="https://render.githubusercontent.com/render/math?math=\hat{e} = Y - \hat{Y} = (I-H)Y"> 

It can be verified that the residuals are orthogonal to the column space of <img src="https://render.githubusercontent.com/render/math?math=X">, such that

<img src="https://render.githubusercontent.com/render/math?math=\hat{e}^TX = ((I-H)Y)^TX = Y^T(I-H)X = Y^T(X-HX) = 0"> 

As the first column of <img src="https://render.githubusercontent.com/render/math?math=X"> is all ones, it implies that 

<img src="https://render.githubusercontent.com/render/math?math={\displaystyle \sum_{i=1}^n\hat{e}_i = 0}">

Because <img src="https://render.githubusercontent.com/render/math?math=\hat{Y} = X\hat{\beta}">, <img src="https://render.githubusercontent.com/render/math?math=\hat{e}"> is also orthogonal to <img src="https://render.githubusercontent.com/render/math?math=\hat{Y}">.

The expectation of <img src="https://render.githubusercontent.com/render/math?math=\hat{e}"> is 

<img src="https://render.githubusercontent.com/render/math?math=E(\hat{e}) = E((I-H)Y) = (I-H)E(Y) = (I-H)X\beta = (X-HX)\beta=0"> 

The covariance matrix of <img src="https://render.githubusercontent.com/render/math?math=\hat{e}"> is 

<img src="https://render.githubusercontent.com/render/math?math=cov(\hat{e}) = cov((I-H)Y) = (I-H)cov(Y)(I-H) = \sigma^2(I-H)"> 

---
## Analysis of Variance
Similar to the unitary regression model, 

<img src="https://render.githubusercontent.com/render/math?math={\displaystyle \sum_{i=1}^n(y_i-\bar{y})^2 = \sum_{i=1}^n(\hat{y}_i-\bar{y})^2 %2B \sum_{i=1}^n(y_i-\hat{y}_i)^2} \Rightarrow \text{TSS} = \text{RegSS} %2B \text{RSS}"> 

with

<img src="https://render.githubusercontent.com/render/math?math=\text{TSS} = {\displaystyle \sum_{i=1}^n(y_i-\bar{y})^2} "> 

<img src="https://render.githubusercontent.com/render/math?math=\text{RSS} = {\displaystyle \sum_{i=1}^n(y_i-\hat{y})^2} ">

<img src="https://render.githubusercontent.com/render/math?math=\text{RegSS} = {\displaystyle \sum_{i=1}^n(\hat{y}_i-\bar{y})^2} "> 

As there are <img src="https://render.githubusercontent.com/render/math?math=p%2B 1"> parameters in the model, we have

<img src="https://render.githubusercontent.com/render/math?math=\dfrac{\text{RSS}}{\sigma^2}\sim \chi^2(n-p-1)">

Thus, the confidence interval of <img src="https://render.githubusercontent.com/render/math?math=\sigma^2"> is 

<img src="https://render.githubusercontent.com/render/math?math=\left[\dfrac{\text{RSS}}{\chi^2_{1-\alpha\text{/}2}(n-p-1)},\dfrac{\text{RSS}}{\chi^2_{\alpha\text{/}2}(n-p-1)}\right]"> 

The R-Squared is similarly defined as

<img src="https://render.githubusercontent.com/render/math?math=R^2 = \dfrac{\text{RegSS}}{\text{TSS}} = 1 - \dfrac{\text{RSS}}{\text{TSS}}"> 

and the adjusted R-Squared is defined as

<img src="https://render.githubusercontent.com/render/math?math=R_a^2 =1 - \dfrac{\text{RSS}/(n-p-1)}{\text{TSS}/(n-1)}"> 

__If <img src="https://render.githubusercontent.com/render/math?math=R^2"> is high, it means that RSS is much smaller compared to TSS and hence the explanatory variables are really useful in predicting the response.__

### Expected Value of RSS
Firstly, 

<img src="https://render.githubusercontent.com/render/math?math=\text{RSS} = \hat{e}^T\hat{e} = Y^T(I-H)Y = e^T(I-H)e">

Hence,

<img src="https://render.githubusercontent.com/render/math?math=E(RSS) = E(e^T(I-H)e) = E\left(\sum_{i\text{,}j}(I-H)_{i\text{,}j}e_ie_j\right) = \sum_{i\text{,}j}(I-H)_{i\text{,}j}E(e_ie_j)">

As <img src="https://render.githubusercontent.com/render/math?math=E(e_ie_j)=0, i\neq j%3B E(e_ie_j)\sigma^2, i=j">, we have

<img src="https://render.githubusercontent.com/render/math?math=E(RSS) = {\displaystyle \sigma^2\sum_{i=1}^n(I-H)_{i\text{,}i} = \sigma^2\left(n-\sum_{i=1}^nH_{i\text{,}i}\right) = \sigma^2(n-tr(H))}">

As <img src="https://render.githubusercontent.com/render/math?math=tr(H) = tr(X(X^TX)^{-1}X^T) = tr(X^TX(X^TX)^{-1}) = tr(I_{p%2B 1}) = p%2B 1">, we get

<img src="https://render.githubusercontent.com/render/math?math=E(RSS) = \sigma^2(n-p-1)">

Therefore, an unbiased estimator of <img src="https://render.githubusercontent.com/render/math?math=\sigma^2"> is given by 

<img src="https://render.githubusercontent.com/render/math?math=\hat{\sigma}^2 = \dfrac{\text{RSS}}{n-p-1}">

This also implies that 

<img src="https://render.githubusercontent.com/render/math?math=\dfrac{\text{RSS}}{\sigma^2} \sim \chi^2(n-p-1)">

---
## Properties
We assume that <img src="https://render.githubusercontent.com/render/math?math=e\sim N_n(0,\sigma^2I_n)">. Equivalently, <img src="https://render.githubusercontent.com/render/math?math=e_1,e_2,...,e_n"> are independent normals with mean 0 and variance <img src="https://render.githubusercontent.com/render/math?math=\sigma^2">. 

__Distribution of__ <img src="https://render.githubusercontent.com/render/math?math=Y">

Since <img src="https://render.githubusercontent.com/render/math?math=Y=X\beta %2B e">, we have <img src="https://render.githubusercontent.com/render/math?math=Y\sim N_n(X\beta,\sigma^2I_n)">

__Distribution of__ <img src="https://render.githubusercontent.com/render/math?math=\hat{\beta}">

Since <img src="https://render.githubusercontent.com/render/math?math=E(\hat{\beta}) = \beta"> and <img src="https://render.githubusercontent.com/render/math?math=cov(\hat{\beta}) = \sigma^2(X^TX)^{-1}">, we have <img src="https://render.githubusercontent.com/render/math?math=\hat{\beta} \sim N_{p%2B 1}(\beta,\sigma^2(X^TX)^{-1})">

__Distribution of Fitted Values__

As <img src="https://render.githubusercontent.com/render/math?math=E(\hat{Y})=E(HY) = X\beta"> and <img src="https://render.githubusercontent.com/render/math?math=cov(\hat{Y})=cov(HY)=\sigma^2H">, we have <img src="https://render.githubusercontent.com/render/math?math=\hat{Y} \sim N_n(X\beta,\sigma^2H)">

__Distribution of Residuals__

As <img src="https://render.githubusercontent.com/render/math?math=E(\hat{e}) = E((I-H)Y) = 0"> and <img src="https://render.githubusercontent.com/render/math?math=cov(\hat{e}) = \sigma^2(I-H)">, we have <img src="https://render.githubusercontent.com/render/math?math=\hat{e} \sim N_n(0,\sigma^2(I-H))">

__Distributions of RSS__

As <img src="https://render.githubusercontent.com/render/math?math=\text{RSS} = \hat{e}^T\hat{e} = Y^T(I-H)Y = e^T(I-H)e">, we have <img src="https://render.githubusercontent.com/render/math?math=\dfrac{\text{RSS}}{\sigma^2} = \left(\dfrac{e}{\sigma}\right)^T(I-H)\left(\dfrac{e}{\sigma}\right)">. And because <img src="https://render.githubusercontent.com/render/math?math=\dfrac{e}{\sigma}\sim N_n(0,1)"> and <img src="https://render.githubusercontent.com/render/math?math=I-H"> is symmetric and idempodent with rank <img src="https://render.githubusercontent.com/render/math?math=n-p-1">, we have <img src="https://render.githubusercontent.com/render/math?math=\dfrac{\text{RSS}}{n-p-1}\sim \chi^2(n-p-1)">

__Independence of Residuals__ <img src="https://render.githubusercontent.com/render/math?math=\hat{e}"> __and__ <img src="https://render.githubusercontent.com/render/math?math=\hat{\beta}">

To prove the independence of <img src="https://render.githubusercontent.com/render/math?math=\hat{e}"> and <img src="https://render.githubusercontent.com/render/math?math=\hat{\beta}">, we can use the theorem below.

__Theorem__. Let <img src="https://render.githubusercontent.com/render/math?math=U\sim N_n(\mu,\Sigma)"> and <img src="https://render.githubusercontent.com/render/math?math=A"> be a fixed <img src="https://render.githubusercontent.com/render/math?math=k\times n"> matrix and <img src="https://render.githubusercontent.com/render/math?math=B"> be a fixed <img src="https://render.githubusercontent.com/render/math?math=l\times n"> matrix. Then, <img src="https://render.githubusercontent.com/render/math?math=AU"> and <img src="https://render.githubusercontent.com/render/math?math=BU"> are independent if and only if <img src="https://render.githubusercontent.com/render/math?math=A\Sigma B^T = 0">.

To prove the theorem, let <img src="https://render.githubusercontent.com/render/math?math=Y = [A%3B B] U = [AU%3B BU] ">. From the properties of the multivariate normal distribution, <img src="https://render.githubusercontent.com/render/math?math=cov(Y) = [A%3B B]\Sigma[A^T\ \ B^T] = [A\Sigma A^T\ \ A\Sigma B^Y%3B B\Sigma A^T\ \ B\Sigma B^T]">. Hence, <img src="https://render.githubusercontent.com/render/math?math=AU"> and <img src="https://render.githubusercontent.com/render/math?math=BU"> are uncorrelated if and only if <img src="https://render.githubusercontent.com/render/math?math=A\Sigma B^T = 0">.

Because <img src="https://render.githubusercontent.com/render/math?math=\hat{e} = (I-H)Y"> and <img src="https://render.githubusercontent.com/render/math?math=\hat{\beta} = (X^TX)^{-1}X^TY">, let <img src="https://render.githubusercontent.com/render/math?math=A = I-H">, <img src="https://render.githubusercontent.com/render/math?math=B=(X^TX)^{-1}X^T">, <img src="https://render.githubusercontent.com/render/math?math=U=Y"> and <img src="https://render.githubusercontent.com/render/math?math=\Sigma = \sigma^2I">, then

<img src="https://render.githubusercontent.com/render/math?math=A\Sigma B^T = \sigma^2(I-H)X(X^TX)^{-1} = (X-HX)(X^TX)^{-1} = 0">

Thus, we can conclude that <img src="https://render.githubusercontent.com/render/math?math=\hat{e}"> and <img src="https://render.githubusercontent.com/render/math?math=\hat{\beta}"> are independent.

Simipliarly, as <img src="https://render.githubusercontent.com/render/math?math=\hat{Y} = HY">, let <img src="https://render.githubusercontent.com/render/math?math=B=H">, we have

<img src="https://render.githubusercontent.com/render/math?math=A\Sigma B^T = \sigma^2(I-H)H^T = H-H=0">

Thus, we can also conclude that <img src="https://render.githubusercontent.com/render/math?math=\hat{e}"> and <img src="https://render.githubusercontent.com/render/math?math=\hat{Y}"> are independent. 


---
## Significance Test
### F-Test
The null hypothesis would be 

<img src="https://render.githubusercontent.com/render/math?math=H_0: \beta_j = 0">

This means that the explanatory variable <img src="https://render.githubusercontent.com/render/math?math=x_{j}"> can be dropped from the linear model, let's call this reduced model <img src="https://render.githubusercontent.com/render/math?math=m">, and let's call the original model <img src="https://render.githubusercontent.com/render/math?math=M">

It is always true that <img src="https://render.githubusercontent.com/render/math?math=\text{RSS}(M)\leq \text{RSS}(m)">. If <img src="https://render.githubusercontent.com/render/math?math=\text{RSS}(M)"> is much smaller than <img src="https://render.githubusercontent.com/render/math?math=\text{RSS}(m)">, it means that the explanatory variable <img src="https://render.githubusercontent.com/render/math?math=x_j"> contributes a lot to the regression and hence cannot be dropped. Therefore, we can test <img src="https://render.githubusercontent.com/render/math?math=H_0"> via the test statistic:

<img src="https://render.githubusercontent.com/render/math?math=\text{RSS}(m) - \text{RSS}(M)">

It can be proved that 

<img src="https://render.githubusercontent.com/render/math?math=\dfrac{\text{RSS}(m)-\text{RSS}(M)}{\sigma^2} \sim \chi^2(1)">

Hence, the <img src="https://render.githubusercontent.com/render/math?math=F"> statistica would be

<img src="https://render.githubusercontent.com/render/math?math=\dfrac{\text{RSS}(m)-\text{RSS}(M)}{\text{RSS}(M)/(n-p-1)}\sim F(1,n-p-1)">

More generally, <img src="https://render.githubusercontent.com/render/math?math=q"> be the number of explanatory variables in <img src="https://render.githubusercontent.com/render/math?math=m">, we have 

<img src="https://render.githubusercontent.com/render/math?math=\dfrac{\text{RSS}(m)-\text{RSS}(M)}{\sigma^2} \sim \chi^2(p-q)">

To prove this, denote the hat matrix <img src="https://render.githubusercontent.com/render/math?math=H"> in the two models by <img src="https://render.githubusercontent.com/render/math?math=H(m)"> and <img src="https://render.githubusercontent.com/render/math?math=H(M)">, then

<img src="https://render.githubusercontent.com/render/math?math=\text{RSS}(m) = Y^T(I-H(m))Y, \text{RSS}(M)=Y^T(I_H(M))Y \Rightarrow \text{RSS}(m)-\text{RSS}(M) = Y^T(H(M) - H(m))Y">

Assume <img src="https://render.githubusercontent.com/render/math?math=Y=X(m)\beta(m)%2B e">, we should have <img src="https://render.githubusercontent.com/render/math?math=H(m)X(m)=X(m)"> and <img src="https://render.githubusercontent.com/render/math?math=H(M)X(m)=X(m)">. Hence,

<img src="https://render.githubusercontent.com/render/math?math=\text{RSS}(m)-\text{RSS}(M) = e^T(H(M) - H(m))e">

where <img src="https://render.githubusercontent.com/render/math?math=\text{rank}(H(M) - H(m))=p-q"> and it is also idempotent since

<img src="https://render.githubusercontent.com/render/math?math=(H(M)-H(m))^2 = H(M) %2B H(m) - 2H(M)H(m) = H(M)-H(m)">

Therefore, under the null hypothesis 

<img src="https://render.githubusercontent.com/render/math?math=\dfrac{\text{RSS}(m)-\text{RSS}(M)}{\sigma^2} = \left(\dfrac{e}{\sigma}\right)^T(H(M)-H(m))\dfrac{e}{\sigma} \sim \chi^2(p-q)">

We can thus obtain the <img src="https://render.githubusercontent.com/render/math?math=F"> statistic

<img src="https://render.githubusercontent.com/render/math?math=\dfrac{(\text{RSS}(m)-\text{RSS}(M))/(p-q)}{\text{RSS}(M)/(n-p-1)} \sim F(p-q,n-p-1)">


Specially, when <img src="https://render.githubusercontent.com/render/math?math=\beta_1=\beta_2=\cdots = \beta_p=0">, the model becomes <img src="https://render.githubusercontent.com/render/math?math=y_i=\beta_0 %2B e_i">. In this case, <img src="https://render.githubusercontent.com/render/math?math=\text{RSS}(m)=\text{TSS}, \text{RSS}(M)=\text{RSS}"> and <img src="https://render.githubusercontent.com/render/math?math=q=0">. Thus the <img src="https://render.githubusercontent.com/render/math?math=F"> statistic would be

<img src="https://render.githubusercontent.com/render/math?math=\dfrac{(\text{TSS}-\text{RSS})/p}{\text{RSS}/(n-p-1)} \sim F(p,n-p-1)">

And the <img src="https://render.githubusercontent.com/render/math?math=p"> value would be 

<img src="https://render.githubusercontent.com/render/math?math=P\left(F(p,n-p-1) > \dfrac{(\text{TSS}-\text{RSS})/p}{\text{RSS}/(n-p-1)}\right)">


### t-Test
Under normality of the errors, we have <img src="https://render.githubusercontent.com/render/math?math=\hat{\beta} \sim N_{p%2B 1}(\beta,\sigma^2(X^TX)^{-1})">, Hence,

<img src="https://render.githubusercontent.com/render/math?math=\hat{\beta}_j\sim N(\beta_j,\sigma^2v_j)">

where <img src="https://render.githubusercontent.com/render/math?math=v_j"> is the <img src="https://render.githubusercontent.com/render/math?math=j\text{th}"> diagonal entry of <img src="https://render.githubusercontent.com/render/math?math=(X^TX)^{-1}">. Under the null hypothesis when <img src="https://render.githubusercontent.com/render/math?math=\beta_j = 0">, we have

<img src="https://render.githubusercontent.com/render/math?math=\dfrac{\hat{\beta}_j}{\sigma\sqrt{v_j}}\sim N(0,1)">

This can be used to construct the <img src="https://render.githubusercontent.com/render/math?math=t"> statistic

<img src="https://render.githubusercontent.com/render/math?math=\dfrac{\hat{\beta}_j/(\sigma\sqrt{v_j})}{\sqrt{\text{RSS}/((n-p-1)\sigma^2)}} = \dfrac{\hat{\beta}_j/\sqrt{v_j}}{\sqrt{\text{RSS}/(n-p-1)}}=\sim t(n-p-1)">

<img src="https://render.githubusercontent.com/render/math?math=p"> value for testing <img src="https://render.githubusercontent.com/render/math?math=H_0: \beta_j=0"> can be got by 

<img src="https://render.githubusercontent.com/render/math?math=P\left(|t(n-p-1)|>\left|\dfrac{\hat{\beta}_j/\sqrt{v_j}}{\sqrt{\text{RSS}/(n-p-1)}}\right|\right)">


---
## Example
### Data
Below is the medical test results of 14 diabetes, build a model to predict the blood sugar level based on other factors. 

| Number  | Cholesterol (mmol/L)  | Triglycerides (mmol/L) | Insulin (muU/ml) | Glycated Hemoglobin (%) | Blood Sugar (mmol/L) |
|---|---|---|---|---|---|
| 1  | 5.68  | 1.90  | 4.53  | 8.20  | 11.20  |
| 2  | 3.79  | 1.64  | 7.32  | 6.90  | 8.80  |
| 3  | 6.02  | 3.56  | 6.95  | 10.80 | 12.30  |
| 4  | 4.85  | 1.07  | 5.88  | 8.30  | 11.60  |
| 5  | 4.60  | 2.32  | 4.05  | 7.50  | 13.40  |
| 6  | 6.05  | 0.64  | 1.42  | 13.60 | 18.30  |
| 7  | 4.90  | 8.50  | 12.60 | 8.50  | 11.10  |
| 8  | 5.78  | 3.36  | 2.96  | 8.00  | 13.60  |
| 9  | 5.43  | 1.13  | 4.31  | 11.30  |14.90  |
| 10  |6.50   |6.21   |3.47   |12.30  |16.00   |
| 11  |7.98   |7.92   |3.37   |9.80   |13.20   |
| 12  |11.54  |10.89  |1.20   |10.50  |20.00   |
| 13  |5.84   |0.92   |8.61   |6.40   |13.30   |
| 14  |3.84   |1.20   |6.45   |9.60   |10.40   |

### Regression
Linear regression using `statsmodels`.
```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import statsmodels.api as sm

data = np.array([[5.68,3.79,6.02,4.85,4.60,6.05,4.90,5.78,5.43,6.50,7.98,11.54,5.84,3.84],
                 [1.90,1.64,3.56,1.07,2.32,0.64,8.50,3.36,1.13,6.21,7.92,10.89,0.92,1.20],
                 [4.53,7.32,6.95,5.88,4.05,1.42,12.60,2.96,4.31,3.47,3.37,1.20,8.61,6.45],
                 [8.20,6.90,10.8,8.30,7.50,13.6,8.50,8.00,11.3,12.3,9.80,10.50,6.40,9.60],
                 [11.2,8.80,12.3,11.6,13.4,18.3,11.1,13.6,14.9,16.0,13.20,20.0,13.3,10.4]])
dataDF = pd.DataFrame(np.transpose(data),columns = ['X1','X2','X3','X4','Y'])  

Y = dataDF['Y']
X = dataDF[['X1','X2','X3','X4']]
X = sm.add_constant(X)
model = sm.OLS(Y,X)
results = model.fit()

results.summary()
```
Results:
```
                            OLS Regression Results                            
==============================================================================
Dep. Variable:                      Y   R-squared:                       0.797
Model:                            OLS   Adj. R-squared:                  0.707
Method:                 Least Squares   F-statistic:                     8.844
Date:                Wed, 09 Mar 2022   Prob (F-statistic):            0.00349
Time:                        21:23:23   Log-Likelihood:                -23.816
No. Observations:                  14   AIC:                             57.63
Df Residuals:                       9   BIC:                             60.83
Df Model:                           4                                         
Covariance Type:            nonrobust                                         
==============================================================================
                 coef    std err          t      P>|t|      [0.025      0.975]
------------------------------------------------------------------------------
const          3.1595      4.308      0.733      0.482      -6.587      12.906
X1             1.1329      0.492      2.304      0.047       0.021       2.245
X2            -0.2042      0.245     -0.833      0.427      -0.759       0.351
X3            -0.1349      0.237     -0.570      0.583      -0.670       0.400
X4             0.5345      0.257      2.080      0.067      -0.047       1.116
==============================================================================
Omnibus:                        2.606   Durbin-Watson:                   1.337
Prob(Omnibus):                  0.272   Jarque-Bera (JB):                1.114
Skew:                          -0.221   Prob(JB):                        0.573
Kurtosis:                       1.691   Cond. No.                         128.
==============================================================================

Warnings:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
```
```python
plt.scatter(range(len(Y)),Y,label='actual')
plt.scatter(range(len(Y)),results.fittedvalues,c="r",marker='*',label="fitted",s=10**2)
plt.legend()
plt.show()
```
<img src="/images/statistics/regression2.png" alt="drawing" width="500"/>

An alternate way is to use functions provided in `statsmodels.formula.api`, which has the constant term by default.
```python
###
from statsmodels.formula.api import ols
model = ols('Y ~ X1 + X2 + X3 + X4', data=dataDF).fit()
```
Results:
```
                            OLS Regression Results                            
==============================================================================
Dep. Variable:                      Y   R-squared:                       0.797
Model:                            OLS   Adj. R-squared:                  0.707
Method:                 Least Squares   F-statistic:                     8.844
Date:                Wed, 09 Mar 2022   Prob (F-statistic):            0.00349
Time:                        21:55:34   Log-Likelihood:                -23.816
No. Observations:                  14   AIC:                             57.63
Df Residuals:                       9   BIC:                             60.83
Df Model:                           4                                         
Covariance Type:            nonrobust                                         
==============================================================================
                 coef    std err          t      P>|t|      [0.025      0.975]
------------------------------------------------------------------------------
Intercept      3.1595      4.308      0.733      0.482      -6.587      12.906
X1             1.1329      0.492      2.304      0.047       0.021       2.245
X2            -0.2042      0.245     -0.833      0.427      -0.759       0.351
X3            -0.1349      0.237     -0.570      0.583      -0.670       0.400
X4             0.5345      0.257      2.080      0.067      -0.047       1.116
==============================================================================
Omnibus:                        2.606   Durbin-Watson:                   1.337
Prob(Omnibus):                  0.272   Jarque-Bera (JB):                1.114
Skew:                          -0.221   Prob(JB):                        0.573
Kurtosis:                       1.691   Cond. No.                         128.
==============================================================================

Warnings:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
```

### Variable Selection
From the results, we can see that <img src="https://render.githubusercontent.com/render/math?math=p"> value of <img src="https://render.githubusercontent.com/render/math?math=X2"> and <img src="https://render.githubusercontent.com/render/math?math=X3"> are 0.426 and 0.583 respectively, meaning that their influence may not be that dignificant than <img src="https://render.githubusercontent.com/render/math?math=X1"> and <img src="https://render.githubusercontent.com/render/math?math=X4">. Therefore, let's do the regression again considering only <img src="https://render.githubusercontent.com/render/math?math=X1"> and <img src="https://render.githubusercontent.com/render/math?math=X4">.

```python
model = ols('Y ~ X1 + X4', data=dataDF).fit()
```
Results:
```
                            OLS Regression Results                            
==============================================================================
Dep. Variable:                      Y   R-squared:                       0.745
Model:                            OLS   Adj. R-squared:                  0.699
Method:                 Least Squares   F-statistic:                     16.07
Date:                Wed, 09 Mar 2022   Prob (F-statistic):           0.000545
Time:                        22:08:30   Log-Likelihood:                -25.420
No. Observations:                  14   AIC:                             56.84
Df Residuals:                      11   BIC:                             58.76
Df Model:                           2                                         
Covariance Type:            nonrobust                                         
==============================================================================
                 coef    std err          t      P>|t|      [0.025      0.975]
------------------------------------------------------------------------------
Intercept      1.8257      2.253      0.810      0.435      -3.133       6.784
X1             0.9517      0.255      3.729      0.003       0.390       1.513
X4             0.6358      0.237      2.686      0.021       0.115       1.157
==============================================================================
Omnibus:                        1.107   Durbin-Watson:                   1.652
Prob(Omnibus):                  0.575   Jarque-Bera (JB):                0.726
Skew:                           0.048   Prob(JB):                        0.695
Kurtosis:                       1.888   Cond. No.                         57.4
==============================================================================

Warnings:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
```
```python
plt.scatter(range(len(Y)),Y,label='actual')
plt.scatter(range(len(Y)),results.fittedvalues,c="r",marker='*',label="fitted-full",s=10**2)
plt.scatter(range(len(Y)),model.fittedvalues,c="g",marker='h',label="fitted-reduced",s=10**2,alpha=0.5)
plt.legend()
plt.show()
```
<img src="/images/statistics/regression3.png" alt="drawing" width="500"/>

---
