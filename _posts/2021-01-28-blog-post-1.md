---
title: 'Statistics [28]: Multiple Regression Model'
date: 2021-01-28
permalink: /posts/2021/01/28/multiple-regression-model/
tags:
  - Statistics
---

The purpose of multiple regression model is to estimate the dependent variable (response variable) using multiple independent variables (explanatory variables).

---
## Multiple Regression Model

### Notation
Suppose there are <img src="https://render.githubusercontent.com/render/math?math=n"> subjects and data is collected from these subjects. 

Data on the response variable is <img src="https://render.githubusercontent.com/render/math?math=y_1,y_2,...,y_n">, which is represented by the column vector <img src="https://render.githubusercontent.com/render/math?math=Y=(y_1,y_2,...,y_n)^T">. 

Data on the <img src="https://render.githubusercontent.com/render/math?math=j\text{th}\,(j=1,2,...,p)"> explanatory variable <img src="https://render.githubusercontent.com/render/math?math=x_j"> is <img src="https://render.githubusercontent.com/render/math?math=x_{1j},x_{2j},...,x_{nj}">, which is represented by the <img src="https://render.githubusercontent.com/render/math?math=n\times p"> matrix <img src="https://render.githubusercontent.com/render/math?math=X">. The <img src="https://render.githubusercontent.com/render/math?math=i\text{th}"> row of <img src="https://render.githubusercontent.com/render/math?math=X"> has data collected from the <img src="https://render.githubusercontent.com/render/math?math=i\text{th}"> subject and the <img src="https://render.githubusercontent.com/render/math?math=j\text{th}"> column of <img src="https://render.githubusercontent.com/render/math?math=X"> has data for the <img src="https://render.githubusercontent.com/render/math?math=j\text{th}"> variable.

### The Linear Model
The linear model is assumed to have the following form:

<img src="https://render.githubusercontent.com/render/math?math=y_i = \beta_1x_{i1} %2B \beta_2x_{i2} %2B \cdots %2B \beta_px_{ip}%2B e_i,\ \ i=1,2,...,n">

where <img src="https://render.githubusercontent.com/render/math?math=e_i \sim N(0,\sigma^2)">.

In matrix notation, the model can be written as 

<img src="https://render.githubusercontent.com/render/math?math=Y = X\beta %2B e">

### The Intercept Term
The linear model stipulates that <img src="https://render.githubusercontent.com/render/math?math=E(y_i) = \beta_1x_{i1} %2B \beta_2x_{i2} %2B \cdots %2B \beta_px_{ip}">, which implies that when <img src="https://render.githubusercontent.com/render/math?math=x_{i1}=x_{i2}=\cdots = x_{ip}=0">, then <img src="https://render.githubusercontent.com/render/math?math=E(y_i)=0">. This is not a reasonable assumption. Therefore, we usually add a intercept term to the model such that 

<img src="https://render.githubusercontent.com/render/math?math=E(y_i) = \beta_0 %2B \beta_1x_{i1} %2B \beta_2x_{i2} %2B \cdots %2B \beta_px_{ip}">

Further, let <img src="https://render.githubusercontent.com/render/math?math=x_{i0}=1">, the above equation can be rewritten as 

<img src="https://render.githubusercontent.com/render/math?math=E(y_i) =\beta_0x_{i0}%2B \beta_1x_{i1} %2B \beta_2x_{i2} %2B \cdots %2B \beta_px_{ip}">

In matrix notation,

<img src="https://render.githubusercontent.com/render/math?math=Y = X\beta %2B e">

where <img src="https://render.githubusercontent.com/render/math?math=X"> now denotes the <img src="https://render.githubusercontent.com/render/math?math=n\times (p%2B 1)"> matrix whose first column consists of all ones and <img src="https://render.githubusercontent.com/render/math?math=\beta = (\beta_0,beta_1,...\beta_p)^T">.

---
## Estimation
Similar to the unitary regression model, <img src="https://render.githubusercontent.com/render/math?math=\beta"> is estimated by minimizing the residual sum of squares:

<img src="https://render.githubusercontent.com/render/math?math=S(\beta) = {\displaystyle \sum_{i=1}^n(y_i-\beta_0-beta_1x_{i1}-\cdots -\beta_px_{ip})^2}">

In matrix notation,

<img src="https://render.githubusercontent.com/render/math?math=S(\beta) = \left\|Y-X\beta\right\|^2">

Using this notation, we have 

<img src="https://render.githubusercontent.com/render/math?math=S(\beta) = (Y-X\beta)^T(Y-X\beta) = Y^TY - 2\beta^TX^TY %2B \beta^TX^TX\beta">

Take partial derivatives with respect to <img src="https://render.githubusercontent.com/render/math?math=\beta_i"> yields

<img src="https://render.githubusercontent.com/render/math?math=\nabla S(\beta) = 2X^TX\beta - 2X^TY = 0 \Rightarrow X^TX\beta = X^TY">

This gives <img src="https://render.githubusercontent.com/render/math?math=p"> linear equations for the <img src="https://render.githubusercontent.com/render/math?math=p"> unknown <img src="https://render.githubusercontent.com/render/math?math=\beta_i,i=1,2,...,p">. The solution is denoted as <img src="https://render.githubusercontent.com/render/math?math=\hat{\beta} = (\hat{\beta}_1,\hat{\beta}_2,...,\hat{\beta}_p)^T">.

As <img src="https://render.githubusercontent.com/render/math?math=X^TY"> lies in the column space of <img src="https://render.githubusercontent.com/render/math?math=X^T">, there is always solution(s). And if <img src="https://render.githubusercontent.com/render/math?math=X^TX"> is invertible, there will be a unique solution, which is given by <img src="https://render.githubusercontent.com/render/math?math=\hat{\beta} = (X^TX)^{-1}X^TY">.

In fact, <img src="https://render.githubusercontent.com/render/math?math=X^TX"> being invertible is equivalent to <img src="https://render.githubusercontent.com/render/math?math=\text{rank}(X) = p%2B 1">. Thus when <img src="https://render.githubusercontent.com/render/math?math=X^TX"> is non-invertible, <img src="https://render.githubusercontent.com/render/math?math=\text{rank}(X) < p%2B 1">. In other words, some columns of <img src="https://render.githubusercontent.com/render/math?math=X"> is a linear combination of the other columns. Hence, some explanatory variables would be redundant. 

---
## Properties of the Estimator
Assume <img src="https://render.githubusercontent.com/render/math?math=X^TX"> is invertible, the estimator <img src="https://render.githubusercontent.com/render/math?math=\hat{\beta}"> has the following properties.

### Linearity 
An estimator is said to be linear if it can be written as <img src="https://render.githubusercontent.com/render/math?math=AY">.

### Unbiasedness
<img src="https://render.githubusercontent.com/render/math?math=E(\hat{\beta}) = E((X^TX)^{-1}X^TY) = (X^TX)^{-1}X^TE(Y) = (X^TX)^{-1}X^TX\beta = \beta">

### Covariance
<img src="https://render.githubusercontent.com/render/math?math=cov(\hat{\beta}) = cov((X^TX)^{-1}X^TY) = (X^TX)^{-1}X^Tcov(Y)X(X^TX)^{-1} = \sigma^2(X^TX)^{-1}">

where 

<img src="https://render.githubusercontent.com/render/math?math=cov(Y) = E[(Y-E(Y))(Y-E(Y))^T] = E[(X\beta%2B e - X\beta)(X\beta%2B e - X\beta)^T] = \sigma^2I">

### Optimality
The Gaussian-Markov Theorem states that <img src="https://render.githubusercontent.com/render/math?math=\beta"> is the best linear unbiased estimator.


---
## Fitted Values
Given the model, the fitted value can be written in matrix form as

<img src="https://render.githubusercontent.com/render/math?math=\hat{Y} = X\hat{\beta} = X(X^TX)^{-1}X^TY">

which can be seen as the orthogonal projection of <img src="https://render.githubusercontent.com/render/math?math=Y"> onto the column space of <img src="https://render.githubusercontent.com/render/math?math=X">.

Let <img src="https://render.githubusercontent.com/render/math?math=H = X(X^TX)^{-1}X^T"> such that <img src="https://render.githubusercontent.com/render/math?math=\hat{Y} = HY">, the matrix <img src="https://render.githubusercontent.com/render/math?math=H"> is called the __Hat Matrix__, which has the following properties:
- It is summetric
- It is idempotent, <img src="https://render.githubusercontent.com/render/math?math=H^2=H"> 
- <img src="https://render.githubusercontent.com/render/math?math=HX=X">
- <img src="https://render.githubusercontent.com/render/math?math=\text{rank}(H) = \text{rank}(X)">  

With these properties, we can easily get

<img src="https://render.githubusercontent.com/render/math?math=E(\hat{Y}) = E(HY) = HE(Y) = HX\beta = X\beta = E(Y)">

<img src="https://render.githubusercontent.com/render/math?math=cov(\hat{Y}) = cov(HY) = Hcov(Y)H^T = \sigma^2H"> 

---
## Residual
The residual of multiple regression

<img src="https://render.githubusercontent.com/render/math?math=\hat{e} = Y - \hat{Y} = (I-H)Y"> 

It can be verified that the residuals are orthogonal to the column space of <img src="https://render.githubusercontent.com/render/math?math=X">, such that

<img src="https://render.githubusercontent.com/render/math?math=\hat{e}^TX = ((I-H)Y)^TX = Y^T(I-H)X = Y^T(X-HX) = 0"> 

As the first column of <img src="https://render.githubusercontent.com/render/math?math=X"> is all ones, it implies that 

<img src="https://render.githubusercontent.com/render/math?math={\displaystyle \sum_{i=1}^n\hat{e}_i = 0}">

Because <img src="https://render.githubusercontent.com/render/math?math=\hat{Y} = X\hat{\beta}">, <img src="https://render.githubusercontent.com/render/math?math=\hat{e}"> is also orthogonal to <img src="https://render.githubusercontent.com/render/math?math=\hat{Y}">.

The expectation of <img src="https://render.githubusercontent.com/render/math?math=\hat{e}"> is 

<img src="https://render.githubusercontent.com/render/math?math=E(\hat{e}) = E((I-H)Y) = (I-H)E(Y) = (I-H)X\beta = (X-HX)\beta=0"> 

The covariance matrix of <img src="https://render.githubusercontent.com/render/math?math=\hat{e}"> is 

<img src="https://render.githubusercontent.com/render/math?math=cov(\hat{e}) = cov((I-H)Y) = (I-H)cov(Y)(I-H) = \sigma^2(I-H)"> 

---
## Analysis of Variance
Similiar to the unitary regression model, 

<img src="https://render.githubusercontent.com/render/math?math={\displaystyle \sum_{i=1}^n(y_i-\bar{y})^2 = \sum_{i=1}^n(\hat{y}_i-\bar{y})^2 %2B \sum_{i=1}^n(y_i-\hat{y}_i)^2} \Rightarrow \text{TSS} = \text{RegSS} %2B \text{RSS}"> 

with

<img src="https://render.githubusercontent.com/render/math?math=\text{TSS} = {\displaystyle \sum_{i=1}^n(y_i-\bar{y})^2} "> 

<img src="https://render.githubusercontent.com/render/math?math=\text{RSS} = {\displaystyle \sum_{i=1}^n(y_i-\hat{y})^2} ">

<img src="https://render.githubusercontent.com/render/math?math=\text{RegSS} = {\displaystyle \sum_{i=1}^n(\hat{y}_i-\bar{y})^2} "> 

As there are <img src="https://render.githubusercontent.com/render/math?math=p%2B 1"> parameters in the model, we have

<img src="https://render.githubusercontent.com/render/math?math=\dfrac{\text{RSS}}{\sigma^2}\sim \chi^2(n-p-1)">

Thus, the confidence interval of <img src="https://render.githubusercontent.com/render/math?math=\sigma^2"> is 

<img src="https://render.githubusercontent.com/render/math?math=\left[\dfrac{\text{RSS}}{\chi^2_{1-\alpha\text{/}2}(n-p-1)},\dfrac{\text{RSS}}{\chi^2_{\alpha\text{/}2}(n-p-1)}\right]"> 

The R-Squared is similiarly defined as

<img src="https://render.githubusercontent.com/render/math?math=R^2 = \dfrac{\text{RegSS}}{\text{TSS}} = 1 - \dfrac{\text{RSS}}{\text{TSS}}"> 

and the adjusted R-Squared is defined as

<img src="https://render.githubusercontent.com/render/math?math=R_a^2 =1 - \dfrac{\text{RSS}/(n-p-1)}{\text{TSS}/(n-1)}"> 

__If <img src="https://render.githubusercontent.com/render/math?math=R^2"> is high, it means that RSS is much smaller compared to TSS and hence the explanatory variables are really useful in predicting the response.__

### Expected Value of RSS
Firstly, 

<img src="https://render.githubusercontent.com/render/math?math=\text{RSS} = \hat{e}^T\hat{e} = Y^T(I-H)Y = e^T(I-H)e">

Hence,

<img src="https://render.githubusercontent.com/render/math?math=E(RSS) = E(e^T(I-H)e) = E\left(\sum_{i\text{,}j}(I-H)_{i\text{,}j}e_ie_j\right) = \sum_{i\text{,}j}(I-H)_{i\text{,}j}E(e_ie_j)">

As <img src="https://render.githubusercontent.com/render/math?math=E(e_ie_j)=0, i\neq j%3B E(e_ie_j)\sigma^2, i=j">, we have

<img src="https://render.githubusercontent.com/render/math?math=E(RSS) = {\displaystyle \sigma^2\sum_{i=1}^n(I-H)_{i\text{,}i} = \sigma^2\left(n-\sum_{i=1}^nH_{i\text{,}i}\right) = \sigma^2(n-tr(H))}">

As <img src="https://render.githubusercontent.com/render/math?math=tr(H) = tr(X(X^TX)^{-1}X^T) = tr(X^TX(X^TX)^{-1}) = tr(I_{p%2B 1}) = p%2B 1">, we get

<img src="https://render.githubusercontent.com/render/math?math=E(RSS) = \sigma^2(n-p-1)">

Therefore, an unbiased estimator of <img src="https://render.githubusercontent.com/render/math?math=\sigma^2"> is given by 

<img src="https://render.githubusercontent.com/render/math?math=\hat{\sigma}^2 = \dfrac{\text{RSS}}{n-p-1}">

This also implies that 

<img src="https://render.githubusercontent.com/render/math?math=\dfrac{\text{RSS}}{\sigma^2} \sim \chi^2(n-p-1)">

---
## Properties
We assume that <img src="https://render.githubusercontent.com/render/math?math=e\sim N_n(0,\sigma^2I_n)">. Equivalently, <img src="https://render.githubusercontent.com/render/math?math=e_1,e_2,...,e_n"> are independent normals with mean 0 and variance <img src="https://render.githubusercontent.com/render/math?math=\sigma^2">. 

__Distribution of__ <img src="https://render.githubusercontent.com/render/math?math=Y">

Since <img src="https://render.githubusercontent.com/render/math?math=Y=X\beta %2B e">, we have <img src="https://render.githubusercontent.com/render/math?math=Y\sim N_n(X\beta,\sigma^2I_n)">

__Distribution of__ <img src="https://render.githubusercontent.com/render/math?math=\hat{\beta}">

Since <img src="https://render.githubusercontent.com/render/math?math=E(\hat{\beta}) = \beta"> and <img src="https://render.githubusercontent.com/render/math?math=cov(\hat{\beta}) = \sigma^2(X^TX)^{-1}">, we have <img src="https://render.githubusercontent.com/render/math?math=\hat{\beta} \sim N_{p%2B 1}(\beta,\sigma^2(X^TX)^{-1})">

__Distribution of Fitted Values__

As <img src="https://render.githubusercontent.com/render/math?math=E(\hat{Y})=E(HY) = X\beta"> and <img src="https://render.githubusercontent.com/render/math?math=cov(\hat{Y})=cov(HY)=\sigma^2H">, we have <img src="https://render.githubusercontent.com/render/math?math=\hat{Y} \sim N_n(X\beta,\sigma^2H)">

__Distribution of Residuals__

As <img src="https://render.githubusercontent.com/render/math?math=E(\hat{e}) = E((I-H)Y) = 0"> and <img src="https://render.githubusercontent.com/render/math?math=cov(\hat{e}) = \sigma^2(I-H)">, we have <img src="https://render.githubusercontent.com/render/math?math=\hat{e} \sim N_n(0,\sigma^2(I-H))">

__Distributions of RSS__

As <img src="https://render.githubusercontent.com/render/math?math=\text{RSS} = \hat{e}^T\hat{e} = Y^T(I-H)Y = e^T(I-H)e">, we have <img src="https://render.githubusercontent.com/render/math?math=\dfrac{\text{RSS}}{\sigma^2} = \left(\dfrac{e}{\sigma}\right)^T(I-H)\left(\dfrac{e}{\sigma}\right)">. And because <img src="https://render.githubusercontent.com/render/math?math=\dfrac{e}{\sigma}\sim N_n(0,1)"> and <img src="https://render.githubusercontent.com/render/math?math=I-H"> is symmetric and idempodent with rank <img src="https://render.githubusercontent.com/render/math?math=n-p-1">, we have <img src="https://render.githubusercontent.com/render/math?math=\dfrac{\text{RSS}}{n-p-1}\sim \chi^2(n-p-1)">

__Independence of Residuals__ <img src="https://render.githubusercontent.com/render/math?math=\hat{e}"> __and__ <img src="https://render.githubusercontent.com/render/math?math=\hat{\beta}">

To prove the independence of <img src="https://render.githubusercontent.com/render/math?math=\hat{e}"> and <img src="https://render.githubusercontent.com/render/math?math=\hat{\beta}">, we can use the theorem below.

__Theorem__. Let <img src="https://render.githubusercontent.com/render/math?math=U\sim N_n(\mu,\Sigma)"> and <img src="https://render.githubusercontent.com/render/math?math=A"> be a fixed <img src="https://render.githubusercontent.com/render/math?math=k\times n"> matrix and <img src="https://render.githubusercontent.com/render/math?math=B"> be a fixed <img src="https://render.githubusercontent.com/render/math?math=l\times n"> matrix. Then, <img src="https://render.githubusercontent.com/render/math?math=AU"> and <img src="https://render.githubusercontent.com/render/math?math=BU"> are independent if and only if <img src="https://render.githubusercontent.com/render/math?math=A\Sigma B^T = 0">.

To prove the theorem, let <img src="https://render.githubusercontent.com/render/math?math=Y = [A%3B B] U = [AU%3B BU] ">. From the properties of the multivariate normal distribution, <img src="https://render.githubusercontent.com/render/math?math=cov(Y) = [A%3B B]\Sigma[A^T\ \ B^T] = [A\Sigma A^T\ \ A\Sigma B^Y%3B B\Sigma A^T\ \ B\Sigma B^T]">. Hence, <img src="https://render.githubusercontent.com/render/math?math=AU"> and <img src="https://render.githubusercontent.com/render/math?math=BU"> are uncorrelated if and only if <img src="https://render.githubusercontent.com/render/math?math=A\Sigma B^T = 0">.

Because <img src="https://render.githubusercontent.com/render/math?math=\hat{e} = (I-H)Y"> and <img src="https://render.githubusercontent.com/render/math?math=\hat{\beta} = (X^TX)^{-1}X^TY">, let <img src="https://render.githubusercontent.com/render/math?math=A = I-H">, <img src="https://render.githubusercontent.com/render/math?math=B=(X^TX)^{-1}X^T">, <img src="https://render.githubusercontent.com/render/math?math=U=Y"> and <img src="https://render.githubusercontent.com/render/math?math=\Sigma = \sigma^2I">, then

<img src="https://render.githubusercontent.com/render/math?math=A\Sigma B^T = \sigma^2(I-H)X(X^TX)^{-1} = (X-HX)(X^TX)^{-1} = 0">

Thus, we can conclude that <img src="https://render.githubusercontent.com/render/math?math=\hat{e}"> and <img src="https://render.githubusercontent.com/render/math?math=\hat{\beta}"> are independent.

Simipliarly, as <img src="https://render.githubusercontent.com/render/math?math=\hat{Y} = HY">, let <img src="https://render.githubusercontent.com/render/math?math=B=H">, we have

<img src="https://render.githubusercontent.com/render/math?math=A\Sigma B^T = \sigma^2(I-H)H^T = H-H=0">

Thus, we can also conclude that <img src="https://render.githubusercontent.com/render/math?math=\hat{e}"> and <img src="https://render.githubusercontent.com/render/math?math=\hat{Y}"> are independent. 


---
## Significance Test
### F-Test
The null hypothesis would be 

<img src="https://render.githubusercontent.com/render/math?math=H_0: \beta_j = 0">

This means that the explanatory variable <img src="https://render.githubusercontent.com/render/math?math=x_{j}"> can be dropped from the linear model, let's call this reduced model <img src="https://render.githubusercontent.com/render/math?math=m">, and let's call the original model <img src="https://render.githubusercontent.com/render/math?math=M">

It is always true that <img src="https://render.githubusercontent.com/render/math?math=\text{RSS}(M)\leq \text{RSS}(m)">. If <img src="https://render.githubusercontent.com/render/math?math=\text{RSS}(M)"> is much smaller than <img src="https://render.githubusercontent.com/render/math?math=\text{RSS}(m)">, it means that the explanatory variable <img src="https://render.githubusercontent.com/render/math?math=x_j"> contributes a lot to the regression and hence cannot be dropped. Therefore, we can test <img src="https://render.githubusercontent.com/render/math?math=H_0"> via the test statistic:

<img src="https://render.githubusercontent.com/render/math?math=\text{RSS}(m) - \text{RSS}(M)">

It can be proved that 

<img src="https://render.githubusercontent.com/render/math?math=\dfrac{\text{RSS}(m)-\text{RSS}(M)}{\sigma^2} \sim \chi^2(1)">

Hence, the <img src="https://render.githubusercontent.com/render/math?math=F"> statistica would be

<img src="https://render.githubusercontent.com/render/math?math=\dfrac{\text{RSS}(m)-\text{RSS}(M)}{\text{RSS}(M)/(n-p-1)}\sim F(1,n-p-1)">

More generally, <img src="https://render.githubusercontent.com/render/math?math=q"> be the number of explanatory variables in <img src="https://render.githubusercontent.com/render/math?math=m">, we have 

<img src="https://render.githubusercontent.com/render/math?math=\dfrac{\text{RSS}(m)-\text{RSS}(M)}{\sigma^2} \sim \chi^2(p-q)">

To prove this, denote the hat matrix <img src="https://render.githubusercontent.com/render/math?math=H"> in the two models by <img src="https://render.githubusercontent.com/render/math?math=H(m)"> and <img src="https://render.githubusercontent.com/render/math?math=H(M)">, then

<img src="https://render.githubusercontent.com/render/math?math=\text{RSS}(m) = Y^T(I-H(m))Y, \text{RSS}(M)=Y^T(I_H(M))Y \Rightarrow \text{RSS}(m)-\text{RSS}(M) = Y^T(H(M) - H(m))Y">

Assume <img src="https://render.githubusercontent.com/render/math?math=Y=X(m)\beta(m)%2B e">, we should have <img src="https://render.githubusercontent.com/render/math?math=H(m)X(m)=X(m)"> and <img src="https://render.githubusercontent.com/render/math?math=H(M)X(m)=X(m)">. Hence,

<img src="https://render.githubusercontent.com/render/math?math=\text{RSS}(m)-\text{RSS}(M) = e^T(H(M) - H(m))e">

where <img src="https://render.githubusercontent.com/render/math?math=\text{rank}(H(M) - H(m))=p-q"> and it is also idempotent since

<img src="https://render.githubusercontent.com/render/math?math=(H(M)-H(m))^2 = H(M) %2B H(m) - 2H(M)H(m) = H(M)-H(m)">

Therefore, under the null hypothesis 

<img src="https://render.githubusercontent.com/render/math?math=\dfrac{\text{RSS}(m)-\text{RSS}(M)}{\sigma^2} = \left(\dfrac{e}{\sigma}\right)^T(H(M)-H(m))\dfrac{e}{\sigma} \sim \chi^2(p-q)">

We can thus obtain the <img src="https://render.githubusercontent.com/render/math?math=F"> statistic

<img src="https://render.githubusercontent.com/render/math?math=\dfrac{(\text{RSS}(m)-\text{RSS}(M))/(p-q)}{\text{RSS}(M)/(n-p-1)} \sim F(p-q,n-p-1)">


Specially, when <img src="https://render.githubusercontent.com/render/math?math=\beta_1=\beta_2=\cdots = \beta_p=0">, the model becomes <img src="https://render.githubusercontent.com/render/math?math=y_i=\beta_0 %2B e_i">. In this case, <img src="https://render.githubusercontent.com/render/math?math=\text{RSS}(m)=\text{TSS}, \text{RSS}(M)=\text{RSS}"> and <img src="https://render.githubusercontent.com/render/math?math=q=0">. Thus the <img src="https://render.githubusercontent.com/render/math?math=F"> statistic would be

<img src="https://render.githubusercontent.com/render/math?math=\dfrac{(\text{TSS}-\text{RSS})/p}{\text{RSS}/(n-p-1)} \sim F(p,n-p-1)">

And the <img src="https://render.githubusercontent.com/render/math?math=p"> value would be 

<img src="https://render.githubusercontent.com/render/math?math=P\left(F(p,n-p-1) > \dfrac{(\text{TSS}-\text{RSS})/p}{\text{RSS}/(n-p-1)}\right)">


### t-Test
Under normality of the errors, we have <img src="https://render.githubusercontent.com/render/math?math=\hat{\beta} \sim N_{p%2B 1}(\beta,\sigma^2(X^TX)^{-1})">, Hence,

<img src="https://render.githubusercontent.com/render/math?math=\hat{\beta}_j\sim N(\beta_j,\sigma^2v_j)">

where <img src="https://render.githubusercontent.com/render/math?math=v_j"> is the <img src="https://render.githubusercontent.com/render/math?math=j\text{th}"> diagonal entry of <img src="https://render.githubusercontent.com/render/math?math=(X^TX)^{-1}">. Under the null hypothesis when <img src="https://render.githubusercontent.com/render/math?math=\beta_j = 0">, we have

<img src="https://render.githubusercontent.com/render/math?math=\dfrac{\hat{\beta}_j}{\sigma\sqrt{v_j}}\sim N(0,1)">

This can be used to construct the <img src="https://render.githubusercontent.com/render/math?math=t"> statistic

<img src="https://render.githubusercontent.com/render/math?math=\dfrac{\hat{\beta}_j/(\sigma\sqrt{v_j})}{\sqrt{\text{RSS}/((n-p-1)\sigma^2)}} = \dfrac{\hat{\beta}_j/\sqrt{v_j}}{\sqrt{\text{RSS}/(n-p-1)}}=\sim t(n-p-1)">

<img src="https://render.githubusercontent.com/render/math?math=p"> value for testing <img src="https://render.githubusercontent.com/render/math?math=H_0: \beta_j=0"> can be got by 

<img src="https://render.githubusercontent.com/render/math?math=P\left(|t(n-p-1)|>\left|\dfrac{\hat{\beta}_j/\sqrt{v_j}}{\sqrt{\text{RSS}/(n-p-1)}}\right|\right)">


---
## Example
Below is the medical test results of 14 diabetes, build a model to predict the blood sugar level based on other factors. 

| Number  | Cholesterol (mmol/L)  | Triglycerides (mmol/L) | Insulin (muU/ml) | Glycated Hemoglobin (%) | Blood Sugar (mmol/L) |
|---|---|---|---|---|---|
| 1  | 5.68  | 1.90  | 4.53  | 8.20  | 11.20  |
| 2  | 3.79  | 1.64  | 7.32  | 6.90  | 8.80  |
| 3  | 6.02  | 3.56  | 6.95  | 10.80 | 12.30  |
| 4  | 4.85  | 1.07  | 5.88  | 8.30  | 11.60  |
| 5  | 4.60  | 2.32  | 4.05  | 7.50  | 13.40  |
| 6  | 6.05  | 0.64  | 1.42  | 13.60 | 18.30  |
| 7  | 4.90  | 8.50  | 12.60 | 8.50  | 11.10  |
| 8  | 5.78  | 3.36  | 2.96  | 8.00  | 13.60  |
| 9  | 5.43  | 1.13  | 4.31  | 11.30  |14.90  |
| 10  |6.50   |6.21   |3.47   |12.30  |16.00   |
| 11  |7.98   |7.92   |3.37   |9.80   |13.20   |
| 12  |11.54  |10.89  |1.20   |10.50  |20.00   |
| 13  |5.84   |0.92   |8.61   |6.40   |13.30   |
| 14  |3.84   |1.20   |6.45   |9.60   |10.40   |

---
