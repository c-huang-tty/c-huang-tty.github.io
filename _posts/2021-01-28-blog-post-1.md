---
title: 'Statistics [28]: Multiple Regression Model'
date: 2021-01-28
permalink: /posts/2021/01/28/multiple-regression-model/
tags:
  - Statistics
---

The purpose of multiple regression model is to estimate the dependent variable (response variable) using multiple independent variables (explanatory variables).

---
## Multiple Regression Model

### Notation
Suppose there are <img src="https://render.githubusercontent.com/render/math?math=n"> subjects and data is collected from these subjects. 

Data on the response variable is <img src="https://render.githubusercontent.com/render/math?math=y_1,y_2,...,y_n">, which is represented by the column vector <img src="https://render.githubusercontent.com/render/math?math=Y=(y_1,y_2,...,y_n)^T">. 

Data on the <img src="https://render.githubusercontent.com/render/math?math=j\text{th}\,(j=1,2,...,p)"> explanatory variable <img src="https://render.githubusercontent.com/render/math?math=x_j"> is <img src="https://render.githubusercontent.com/render/math?math=x_{1j},x_{2j},...,x_{nj}">, which is represented by the <img src="https://render.githubusercontent.com/render/math?math=n\times p"> matrix <img src="https://render.githubusercontent.com/render/math?math=X">. The <img src="https://render.githubusercontent.com/render/math?math=i\text{th}"> row of <img src="https://render.githubusercontent.com/render/math?math=X"> has data collected from the <img src="https://render.githubusercontent.com/render/math?math=i\text{th}"> subject and the <img src="https://render.githubusercontent.com/render/math?math=j\text{th}"> column of <img src="https://render.githubusercontent.com/render/math?math=X"> has data for the <img src="https://render.githubusercontent.com/render/math?math=j\text{th}"> variable.

### The Linear Model
The linear model is assumed to have the following form:

<img src="https://render.githubusercontent.com/render/math?math=y_i = \beta_1x_{i1} %2B \beta_2x_{i2} %2B \cdots %2B \beta_px_{ip}%2B e_i,\ \ i=1,2,...,n">

where <img src="https://render.githubusercontent.com/render/math?math=e_i \sim N(0,\sigma^2)">.

In matrix notation, the model can be written as 

<img src="https://render.githubusercontent.com/render/math?math=Y = X\beta %2B e">

### The Intercept Term
The linear model stipulates that <img src="https://render.githubusercontent.com/render/math?math=E(y_i) = \beta_1x_{i1} %2B \beta_2x_{i2} %2B \cdots %2B \beta_px_{ip}">, which implies that when <img src="https://render.githubusercontent.com/render/math?math=x_{i1}=x_{i2}=\cdots = x_{ip}=0">, then <img src="https://render.githubusercontent.com/render/math?math=E(y_i)=0">. This is not a reasonable assumption. Therefore, we usually add a intercept term to the model such that 

<img src="https://render.githubusercontent.com/render/math?math=E(y_i) = \beta_0 %2B \beta_1x_{i1} %2B \beta_2x_{i2} %2B \cdots %2B \beta_px_{ip}">

Further, let <img src="https://render.githubusercontent.com/render/math?math=x_{i0}=1">, the above equation can be rewritten as 

<img src="https://render.githubusercontent.com/render/math?math=E(y_i) =\beta_0x_{i0}%2B \beta_1x_{i1} %2B \beta_2x_{i2} %2B \cdots %2B \beta_px_{ip}">

In matrix notation,

<img src="https://render.githubusercontent.com/render/math?math=Y = X\beta %2B e">

where <img src="https://render.githubusercontent.com/render/math?math=X"> now denotes the <img src="https://render.githubusercontent.com/render/math?math=n\times (p%2B 1)"> matrix whose first column consists of all ones and <img src="https://render.githubusercontent.com/render/math?math=\beta = (\beta_0,beta_1,...\beta_p)^T">.

---
## Estimation
Similar to the unitary regression model, <img src="https://render.githubusercontent.com/render/math?math=\beta"> is estimated by minimizing the residual sum of squares:

<img src="https://render.githubusercontent.com/render/math?math=S(\beta) = {\displaystyle \sum_{i=1}^n(y_i-\beta_0-beta_1x_{i1}-\cdots -\beta_px_{ip})^2}">

In matrix notation,

<img src="https://render.githubusercontent.com/render/math?math=S(\beta) = \left\|Y-X\beta\right\|^2">

Using this notation, we have 

<img src="https://render.githubusercontent.com/render/math?math=S(\beta) = (Y-X\beta)^T(Y-X\beta) = Y^TY - 2\beta^TX^TY %2B \beta^TX^TX\beta">

Take partial derivatives with respect to <img src="https://render.githubusercontent.com/render/math?math=\beta_i"> yields

<img src="https://render.githubusercontent.com/render/math?math=\nabla S(\beta) = 2X^TX\beta - 2X^TY = 0 \Rightarrow X^TX\beta = X^TY">

This gives <img src="https://render.githubusercontent.com/render/math?math=p"> linear equations for the <img src="https://render.githubusercontent.com/render/math?math=p"> unknown <img src="https://render.githubusercontent.com/render/math?math=\beta_i,i=1,2,...,p">. The solution is denoted as <img src="https://render.githubusercontent.com/render/math?math=\hat{\beta} = (\hat{\beta}_1,\hat{\beta}_2,...,\hat{\beta}_p)^T">.

As <img src="https://render.githubusercontent.com/render/math?math=X^TY"> lies in the column space of <img src="https://render.githubusercontent.com/render/math?math=X^T">, there is always solution(s). And if <img src="https://render.githubusercontent.com/render/math?math=X^TX"> is invertible, there will be a unique solution, which is given by <img src="https://render.githubusercontent.com/render/math?math=\hat{\beta} = (X^TX)^{-1}X^TY">.

In fact, <img src="https://render.githubusercontent.com/render/math?math=X^TX"> being invertible is equivalent to <img src="https://render.githubusercontent.com/render/math?math=\text{rank}(X) = p%2B 1">. Thus when <img src="https://render.githubusercontent.com/render/math?math=X^TX"> is non-invertible, <img src="https://render.githubusercontent.com/render/math?math=\text{rank}(X) < p%2B 1">. In other words, some columns of <img src="https://render.githubusercontent.com/render/math?math=X"> is a linear combination of the other columns. Hence, some explanatory variables would be redundant. 

---
## Properties of the Estimator
Assume <img src="https://render.githubusercontent.com/render/math?math=X^TX"> is invertible, the estimator <img src="https://render.githubusercontent.com/render/math?math=\hat{\beta}"> has the following properties.

### Linearity 
An estimator is said to be linear if it can be written as <img src="https://render.githubusercontent.com/render/math?math=AY">.

### Unbiasedness
<img src="https://render.githubusercontent.com/render/math?math=E(\hat{\beta}) = E((X^TX)^{-1}X^TY) = (X^TX)^{-1}X^TE(Y) = (X^TX)^{-1}X^TX\beta = \beta">

### Covariance
<img src="https://render.githubusercontent.com/render/math?math=cov(\hat{\beta}) = cov((X^TX)^{-1}X^TY) = (X^TX)^{-1}X^Tcov(Y)X(X^TX)^{-1} = \sigma^2(X^TX)^{-1}">

where 

<img src="https://render.githubusercontent.com/render/math?math=cov(Y) = E[(Y-E(Y))(Y-E(Y))^T] = E[(X\beta%2B e - X\beta)(X\beta%2B e - X\beta)^T] = \sigma^2I">

### Optimality
The Gaussian-Markov Theorem states that <img src="https://render.githubusercontent.com/render/math?math=\beta"> is the best linear unbiased estimator.


---
## Fitted Values
Given the model, the fitted value can be written in matrix form as

<img src="https://render.githubusercontent.com/render/math?math=\hat{Y} = X\hat{\beta} = X(X^TX)^{-1}X^TY">

which can be seen as the orthogonal projection of <img src="https://render.githubusercontent.com/render/math?math=Y"> onto the column space of <img src="https://render.githubusercontent.com/render/math?math=X">.

Let <img src="https://render.githubusercontent.com/render/math?math=H = X(X^TX)^{-1}X^T"> such that <img src="https://render.githubusercontent.com/render/math?math=\hat{Y} = HY">, the matrix <img src="https://render.githubusercontent.com/render/math?math=H"> is called the __Hat Matrix__, which has the following properties:
- It is summetric
- It is idempotent, <img src="https://render.githubusercontent.com/render/math?math=H^2=H"> 
- <img src="https://render.githubusercontent.com/render/math?math=HX=X">
- <img src="https://render.githubusercontent.com/render/math?math=\text{rank}(H) = \text{rank}(X)">  

With these properties, we can easily get

<img src="https://render.githubusercontent.com/render/math?math=E(\hat{Y}) = E(HY) = HE(Y) = HX\beta = X\beta = E(Y)">

<img src="https://render.githubusercontent.com/render/math?math=cov(\hat{Y}) = cov(HY) = Hcov(Y)H^T = \sigma^2H"> 

---
## Residual
The residual of multiple regression

<img src="https://render.githubusercontent.com/render/math?math=\hat{e} = Y - \hat{Y} = (I-H)Y"> 

It can be verified that the residuals are orthogonal to the column space of <img src="https://render.githubusercontent.com/render/math?math=X">, such that

<img src="https://render.githubusercontent.com/render/math?math=\hat{e}^TX = ((I-H)Y)^TX = Y^T(I-H)X = Y^T(X-HX) = 0"> 

As the first column of <img src="https://render.githubusercontent.com/render/math?math=X"> is all ones, it implies that 

<img src="https://render.githubusercontent.com/render/math?math={\displaystyle \sum_{i=1}^n\hat{e}_i = 0}">

Because <img src="https://render.githubusercontent.com/render/math?math=\hat{Y} = X\hat{\beta}">, <img src="https://render.githubusercontent.com/render/math?math=\hat{e}"> is also orthogonal to <img src="https://render.githubusercontent.com/render/math?math=\hat{Y}">.

The expectation of <img src="https://render.githubusercontent.com/render/math?math=\hat{e}"> is 

<img src="https://render.githubusercontent.com/render/math?math=E(\hat{e}) = E((I-H)Y) = (I-H)E(Y) = (I-H)X\beta = (X-HX)\beta=0"> 

The covariance matrix of <img src="https://render.githubusercontent.com/render/math?math=\hat{e}"> is 

<img src="https://render.githubusercontent.com/render/math?math=cov(\hat{e}) = cov((I-H)Y) = (I-H)cov(Y)(I-H) = \sigma^2(I-H)"> 

---
## Analysis of Variance
Similiar to the unitary regression model, 

<img src="https://render.githubusercontent.com/render/math?math={\displaystyle \sum_{i=1}^n(y_i-\bar{y})^2 = \sum_{i=1}^n(\hat{y}_i-\bar{y})^2 %2B \sum_{i=1}^n(y_i-\hat{y}_i)^2} \Rightarrow \text{TSS} = \text{RegSS} %2B \text{RSS}"> 

with

<img src="https://render.githubusercontent.com/render/math?math=\text{TSS} = {\displaystyle \sum_{i=1}^n(y_i-\bar{y})^2} "> 

<img src="https://render.githubusercontent.com/render/math?math=\text{RSS} = {\displaystyle \sum_{i=1}^n(y_i-\hat{y})^2} ">

<img src="https://render.githubusercontent.com/render/math?math=\text{RegSS} = {\displaystyle \sum_{i=1}^n(\hat{y}_i-\bar{y})^2} "> 

As there are <img src="https://render.githubusercontent.com/render/math?math=p%2B 1"> parameters in the model, we have

<img src="https://render.githubusercontent.com/render/math?math=\dfrac{\text{RSS}}{\sigma^2}\sim \chi^2(n-p-1)">

Thus, the confidence interval of <img src="https://render.githubusercontent.com/render/math?math=\sigma^2"> is 

<img src="https://render.githubusercontent.com/render/math?math=\left[\dfrac{\text{RSS}}{\chi^2_{1-\alpha\text{/}2}(n-p-1)},\dfrac{\text{RSS}}{\chi^2_{\alpha\text{/}2}(n-p-1)}\right]"> 

The R-Squared is similiarly defined as

<img src="https://render.githubusercontent.com/render/math?math=R^2 = \dfrac{\text{RegSS}}{\text{TSS}} = 1 - \dfrac{\text{RSS}}{\text{TSS}}"> 

and the adjusted R-Squared is defined as

<img src="https://render.githubusercontent.com/render/math?math=R_a^2 =1 - \dfrac{\text{RSS}/(n-p-1)}{\text{TSS}/(n-1)}"> 

__If <img src="https://render.githubusercontent.com/render/math?math=R^2"> is high, it means that RSS is much smaller compared to TSS and hence the explanatory variables are really useful in predicting the response.__

---
## Significance Test
### F-Test

### t-Test

---
## Variable Selection


---
