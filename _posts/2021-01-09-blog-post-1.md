---
title: 'Statistics [09]: Parameter Point Estimation'
date: 2021-01-09
permalink: /posts/2021/01/09/parameter-point-estimation/
tags:
  - Statistics
---

Assume <img src="https://render.githubusercontent.com/render/math?math=X_1,X_2,...,X_n"> are samples from a population, __point estimation__ refers to constructing certain statistical quantities <img src="https://render.githubusercontent.com/render/math?math=\hat{\theta} = \theta(X_1,X_2,...,X_n)"> that can be used to estimate the distribution of the population. The method is not unique, the most commonly used methods are the method of moments and the method of maximum likelihood. 

---
## Method of Moments
The basic idea is to equate sample moments with theoretical moments, where the moments can be __raw moments__ or __central moments__ or variance.

### Example 1
Assume <img src="https://render.githubusercontent.com/render/math?math=X_1,X_2,...,X_n"> are samples from uniform distribution <img src="https://render.githubusercontent.com/render/math?math=X\sim U(-a,a)">, estimate <img src="https://render.githubusercontent.com/render/math?math=a"> using the method of moments.

__Solution 1__.

<img src="https://render.githubusercontent.com/render/math?math=E(X)=0,\ \ var(X) = \dfrac{a^2}{3} = s^2 \Rightarrow \hat{a} = \sqrt{3}s">

__Solution 2__.

<img src="https://render.githubusercontent.com/render/math?math=E(X)=0,\ \ E(X^2) = \dfrac{a^2}{3} = \dfrac{X_1^2 %2B X_2^2 %2B ... %2B X_n^2 }{n}">

<img src="https://render.githubusercontent.com/render/math?math=\hat{a} = \sqrt{3}\sqrt{\dfrac{X_1^2 %2B X_2^2 %2B ... %2B X_n^2 }{n}}">

### Example 2
Assume <img src="https://render.githubusercontent.com/render/math?math=X_1,X_2,...,X_m"> are samples from binomial distribution <img src="https://render.githubusercontent.com/render/math?math=X\sim b(n,p)">, estimate <img src="https://render.githubusercontent.com/render/math?math=n,p"> using the method of moments.

__Solution 1__.

<img src="https://render.githubusercontent.com/render/math?math=E(X)=np = \bar{x},\ \ var(X) = np(1-p) = s^2">

<img src="https://render.githubusercontent.com/render/math?math=\hat{p} = 1 - \dfrac{s^2}{\bar{x}},\ \ \hat{n} = \dfrac{\bar{x}^2}{\bar{x}-s^2}">

__Solution 2__.

<img src="https://render.githubusercontent.com/render/math?math=E(X)=np = \bar{x},\ \ E(X^2) = np(1-p) %2B n^2p^2 = \dfrac{X_1^2 %2B X_2^2 %2B ... %2B X_m^2 }{m} = \bar{x}(1-p) %2B \bar{x}^2">

<img src="https://render.githubusercontent.com/render/math?math=\hat{p} = 1 %2B \bar{x} - \dfrac{X_1^2 %2B X_2^2 %2B ... %2B X_m^2 }{m\bar{x}},\ \ \hat{n} = \dfrac{m\bar{x}^2}{m\bar{x} %2B m\bar{x}^2 - X_1^2 %2B X_2^2 %2B ... %2B X_m^2 }">

---
## Method of Maximum Likelihood
The abasic idea of maximum likelihood is to estimate the unknown parameters <img src="https://render.githubusercontent.com/render/math?math=\theta"> that would maximize the probability of getting the data we have observed. 

Assume we have a random sample <img src="https://render.githubusercontent.com/render/math?math=X_1,X_2,...,X_n">, each with a probability function <img src="https://render.githubusercontent.com/render/math?math=p(x_i%3B \theta)">. Then, the likelihood function can be expressed as 

<img src="https://render.githubusercontent.com/render/math?math=L(\theta%3B x_1,x_2,...,x_n) = {\displaystyle \prod_{k=1}^n p(x_k%3B \theta)}">

If <img src="https://render.githubusercontent.com/render/math?math=\hat{\theta} = \hat{\theta}(x_1,x_2,...,x_n)"> satisfies <img src="https://render.githubusercontent.com/render/math?math=L(\hat{\theta}) = {\displaystyle \max_{\theta\in\Theta}L(\theta)}">, then <img src="https://render.githubusercontent.com/render/math?math=\hat{\theta}"> is called __maximum likelihood estimation (MLE)__.

In practice, we often use the log form of the likelihood function, then it becomes

<img src="https://render.githubusercontent.com/render/math?math=\ln(L(\theta%3B x_1,x_2,...,x_n)) = {\displaystyle \sum_{k=1}^n \ln(p(x_k%3B \theta))}">

### Example 3
Assume <img src="https://render.githubusercontent.com/render/math?math=x_1,x_2,...,x_n"> are samples from exponential distribution <img src="https://render.githubusercontent.com/render/math?math=\Exp(\lambda)">, find <img src="https://render.githubusercontent.com/render/math?math=\lambda">.

__Solution__.

<img src="https://render.githubusercontent.com/render/math?math=L(\lambda%3B x_1,x_2,...,x_n) = {\displaystyle \prod_{k=1}^n \lambda\exp(-\lambda x_i) = \lambda^n\exp(-\lambda(x_1%2Bx_2%2B+...+x_n))}">.

<img src="https://render.githubusercontent.com/render/math?math=\ln(L(\lambda%3B x_1,x_2,...,x_n)) = {\displaystyle n\ln\lambda - \lambda(x_1%2Bx_2%2B+...+x_n)}">

<img src="https://render.githubusercontent.com/render/math?math=\max\ln L(\lambda)\Rightarrow {\displaystyle \dfrac{d\ln L(\lambda)}{d\lambda}= \dfrac{n}{\lambda} - (x_1%2Bx_2%2B+...+x_n) = 0}">

<img src="https://render.githubusercontent.com/render/math?math=\hat{\lambda} = \dfrac{1}{(x_1%2Bx_2%2B+...+x_n)/n} \Rightarrow \hat{\lambda} = \dfrac{1}{\bar{x}}">

### Example 4
Assume <img src="https://render.githubusercontent.com/render/math?math=x_1,x_2,...,x_n"> are samples from Poisson distribution <img src="https://render.githubusercontent.com/render/math?math=P(\lambda)">, find <img src="https://render.githubusercontent.com/render/math?math=\lambda">.

__Solution__.

<img src="https://render.githubusercontent.com/render/math?math=L(\lambda%3B x_1,x_2,...,x_n) = {\displaystyle \prod_{k=1}^n \dfrac{\lambda^{x_k}}{x_k!}e^{-\lambda} = \dfrac{1}{x_1!x_2!...x_n!}\lambda^{x_1%2Bx_2%2B+...+x_n}e^{-n\lambda}}">.

<img src="https://render.githubusercontent.com/render/math?math=\ln L(\lambda) = {\displaystyle C%2B (x_1%2Bx_2%2B+...+x_n)\ln\lambda - n\lambda}">

<img src="https://render.githubusercontent.com/render/math?math=\max\ln L(\lambda)\Rightarrow {\displaystyle \dfrac{d\ln L(\lambda)}{d\lambda}= \dfrac{1}{\lambda}(x_1%2Bx_2%2B+...+x_n) - n = 0}">

<img src="https://render.githubusercontent.com/render/math?math=\hat{\lambda} = \dfrac{x_1%2Bx_2%2B+...+x_n}{n}">

### Example 5
Assume <img src="https://render.githubusercontent.com/render/math?math=x_1,x_2,...,x_n"> are samples from normal distribution <img src="https://render.githubusercontent.com/render/math?math=N(\mu,\sigma^2)">, find <img src="https://render.githubusercontent.com/render/math?math=\mu, \sigma^2">.

__Solution__.

<img src="https://render.githubusercontent.com/render/math?math=L(\mu,\sigma^2%3B x_1,x_2,...,x_n) = {\displaystyle \prod_{k=1}^n \dfrac{1}{\sqrt{2\pi}\sigma}\exp{\left(-\dfrac{(x_k-\mu)^2}{2\sigma^2}\right)} = \left(\dfrac{1}{\sqrt{2\pi}\sigma}\right)^n\exp\left(-\dfrac{1}{2\sigma^2}\sum_{i=1}^n(x_i-\mu)^2\right)}">.

<img src="https://render.githubusercontent.com/render/math?math=\ln L(\mu,\sigma^2) = {\displaystyle n\ln\left(\dfrac{1}{\sqrt{2\pi}}\right) - \dfrac{n}{2}\ln\sigma^2 - \dfrac{1}{2\sigma^2}\sum_{i=1}^n(x_i-\mu)^2}">

For <img src="https://render.githubusercontent.com/render/math?math=\mu">,

<img src="https://render.githubusercontent.com/render/math?math={\displaystyle \dfrac{\partial\ln L(\mu,\sigma^2)}{\partial\mu}= \dfrac{1}{\sigma^2}\sum_{i=1}^n(x_i-\mu) = 0 \Rightarrow \hat{\mu} = \dfrac{1}{n}\sum_{i=1}^n x_i = \bar{x}}">

For <img src="https://render.githubusercontent.com/render/math?math=\sigma^2">,

<img src="https://render.githubusercontent.com/render/math?math={\displaystyle \dfrac{\partial\ln L(\mu,\sigma^2)}{\partial\sigma^2}= -\dfrac{n}{2}\dfrac{1}{\sigma^2} %2B \dfrac{1}{2(\sigma^2)^2}\sum_{i=1}^n(x_i-\mu)^2 = 0 \Rightarrow \hat{\sigma}^2 = \dfrac{1}{n}\sum_{i=1}^n (x_i - \mu)^2 = \dfrac{1}{n}\sum_{i=1}^n (x_i - \bar{x})^2}">

---
