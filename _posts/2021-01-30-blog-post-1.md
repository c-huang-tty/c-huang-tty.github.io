---
title: 'Statistics [30]: Clustering Analysis'
date: 2021-01-30
permalink: /posts/2021/01/30/clustering-analysis/
tags:
  - Statistics
---

Clustering analysis is the task of grouping a set of objects in such a way that objects in the same group are more similar to each other than to those in other groups.

---
---
# Measurement of Association

---
## Metrics
### Minkowski Distance
<img src="https://render.githubusercontent.com/render/math?math=d(\textbf{X}_i,\textbf{X}_j) = {\displaystyle \left[\sum_{k=1}^p\left|X_{ik}-X_{jk}\right|^m\right]^{1\text{/}m}}">


When <img src="https://render.githubusercontent.com/render/math?math=m=1">, it is __Absolute Distance__

<img src="https://render.githubusercontent.com/render/math?math=d(\textbf{X}_i,\textbf{X}_j) = {\displaystyle \sum_{k=1}^p\left|X_{ik}-X_{jk}\right|}">

When <img src="https://render.githubusercontent.com/render/math?math=m=2">, it is __Euclidean Distance__

<img src="https://render.githubusercontent.com/render/math?math=d(\textbf{X}_i,\textbf{X}_j) = {\displaystyle \sqrt{\sum_{i=k}^p(X_{ik}-X_{jk})^2}}">


When <img src="https://render.githubusercontent.com/render/math?math=m=\infty">, it is __Chebysheve Distance__

<img src="https://render.githubusercontent.com/render/math?math=d(\textbf{X}_i,\textbf{X}_j) = {\displaystyle \max_{1\leq k\leq p}\left|X_{ik}-X_{jk}\right|}">

### Standardized Euclidean Distance
Let the covariance matrix of <img src="https://render.githubusercontent.com/render/math?math=X"> be <img src="https://render.githubusercontent.com/render/math?math=S">, which is

<img src="https://render.githubusercontent.com/render/math?math=S = {\displaystyle \dfrac{1}{n-1}\sum_{i=1}^{n}(\textbf{X}_i-\bar{\textbf{X}})(\textbf{X}_i-\bar{\textbf{X}})^T}">

Standardized Euclidean distance:

<img src="https://render.githubusercontent.com/render/math?math=d(\textbf{X}_i,\textbf{X}_j) = {\displaystyle \sqrt{\sum_{i=k}^p\dfrac{(X_{ik}-X_{jk})^2}{S_{ll}}}}">

### Mahalanobis Distance
<img src="https://render.githubusercontent.com/render/math?math=d(\textbf{X}_i,\textbf{X}_j) = {\displaystyle \left[  (\textbf{X}_i-\textbf{X}_j)^TS^{-1}(\textbf{X}_i-\textbf{X}_j) \right]^{1\text{/}2}}">

If <img src="https://render.githubusercontent.com/render/math?math=\textbf{X}_i,\textbf{X}_j"> are independent, namely, the covariance matrix <img src="https://render.githubusercontent.com/render/math?math=S"> is a diagonal matrix, then Mahalanobis distance becomes standardized Euclidean distance.

### Canberra Metric
<img src="https://render.githubusercontent.com/render/math?math=d(\textbf{X}_i,\textbf{X}_j) = {\displaystyle \sum_{k=1}^p\dfrac{|X_{ik}-X_{jk}|}{X_{ik}%2B X_{jk}}}">

### Czekanowski Coefficient
<img src="https://render.githubusercontent.com/render/math?math=d(\textbf{X}_i,\textbf{X}_j) = 1 - {\displaystyle \dfrac{2\sum_{k=1}^p\min(X_{ik},X_{jk})}{\sum_{k=1}^p(X_{ik}%2B X_{jk})} = \dfrac{\sum_{k=1}^p\left[\max(X_{ik},X_{jk})-\min(X_{ik},X_{jk})\right]}{\sum_{k=1}^p(X_{ik}%2B X_{jk})}}">

---
## Properties of Metrics
1. Symmetry: <img src="https://render.githubusercontent.com/render/math?math=d(\textbf{X}_i,\textbf{X}_j) = d(\textbf{X}_j,\textbf{X}_i)">
2. Positivity: <img src="https://render.githubusercontent.com/render/math?math=d(\textbf{X}_i,\textbf{X}_j)>0,\ \ \textbf{X}_i\neq \textbf{X}_j">
3. Identity: <img src="https://render.githubusercontent.com/render/math?math=d(\textbf{X}_i,\textbf{X}_j)=0,\ \ \textbf{X}_i= \textbf{X}_j">
4. Triangle Inequality: <img src="https://render.githubusercontent.com/render/math?math=d(\textbf{X}_i,\textbf{X}_k)\leq d(\textbf{X}_i,\textbf{X}_j) %2B d(\textbf{X}_j,\textbf{X}_k)">

---
---
# Agglomerative Clustering

---
## Basic Ideas
In agglomerative hierarchical algorithms, we start by defining each data point as a cluster. Then, the two closest clusters are combined into a new cluster. In each subsequent step, two existing clusters are merged into a single cluster.

There are several methods for measuering association between the clusters:
- Single Linkage: <img src="https://render.githubusercontent.com/render/math?math=d_{12} = \min_{i\text{,}j}d(\textbf{X}_i,\textbf{X}_j)">
- Complete Linkage: <img src="https://render.githubusercontent.com/render/math?math=d_{12} = \max_{i\text{,}j}d(\textbf{X}_i,\textbf{X}_j)">
- Average Linkage: <img src="https://render.githubusercontent.com/render/math?math=d_{12} = \dfrac{1}{kl}{\displaystyle \sum_{i=1}^k\sum_{j=1}^ld(\textbf{X}_i,\textbf{X}_j)}">
- Centroid Method: <img src="https://render.githubusercontent.com/render/math?math=d_{12} = d(bar{X},\bar{Y})">
- Wardâ€™s Method: ANOVA based approach

---
## Example 
Below is an example using package `sklearn` and the `Iris` dataset.
```python
class sklearn.cluster.AgglomerativeClustering(n_clusters=2, *, affinity='euclidean', memory=None, connectivity=None, compute_full_tree='auto', linkage='ward', distance_threshold=None, compute_distances=False)
```

### Data
```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import normalize
from sklearn.cluster import AgglomerativeClustering
from sklearn.datasets import load_iris

# load data
iris = load_iris()
data = iris.data
data[:5]

# normalize data
data_scaled = normalize(data)
data_scaled = pd.DataFrame(data_scaled, columns=['sepalL','sepalW','petalL','petalW'])
data_scaled.head()
```
```
# Before normalization
array([[5.1, 3.5, 1.4, 0.2],
       [4.9, 3. , 1.4, 0.2],
       [4.7, 3.2, 1.3, 0.2],
       [4.6, 3.1, 1.5, 0.2],
       [5. , 3.6, 1.4, 0.2]])

# After nromalization
     sepalL    sepalW    petalL    petalW
0  0.803773  0.551609  0.220644  0.031521
1  0.828133  0.507020  0.236609  0.033801
2  0.805333  0.548312  0.222752  0.034269
3  0.800030  0.539151  0.260879  0.034784
4  0.790965  0.569495  0.221470  0.031639
```
Draw the dendrogram to help us decide the number of clusters:
```python
from scipy.cluster.hierarchy import dendrogram,linkage
plt.figure()  
plt.title("Dendrograms")  
dend = dendrogram(linkage(data_scaled, method='ward'))
```

<img src="/images/cluster1.png" alt="drawing" width="500"/>

The x-axis contains the samples and y-axis represents the distance between these samples. If the threshold is 1.0, there will be two clusters; if the threshold is 0.5, there will be 3 clusters.

<img src="/images/cluster2.png" alt="drawing" width="500"/>

### 2 Clusters
First, let's see the results of two clusters. 
```python
cluster = AgglomerativeClustering(n_clusters=2, affinity='euclidean', linkage='ward')  
cluster.fit_predict(data_scaled)
```
```
array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
       1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int64)
```
Visualization:
```python
plt.figure()  
plt.scatter(data_scaled['sepalL'], data_scaled['sepalW'], c=cluster.labels_) 
```
<img src="/images/cluster3.png" alt="drawing" width="500"/>

### 3 Clusters
```python
cluster = AgglomerativeClustering(n_clusters=3, affinity='euclidean', linkage='ward')  
cluster.labels_
```
```
array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
       1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
       0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 0,
       2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], dtype=int64)
```
Visualization:
```python
plt.figure()  
plt.scatter(data_scaled['sepalL'], data_scaled['sepalW'], c=cluster.labels_) 
```
<img src="/images/cluster4.png" alt="drawing" width="500"/>

### Comparison of Different Linkage Methods

**Single**

<img src="/images/cluster5.png" alt="drawing" width="500"/>

**Complete**

<img src="/images/cluster6.png" alt="drawing" width="500"/>

**Average**

<img src="/images/cluster7.png" alt="drawing" width="500"/>

**Weighted**

<img src="/images/cluster8.png" alt="drawing" width="500"/>

**Centroid**

<img src="/images/cluster9.png" alt="drawing" width="500"/>

**Median**

<img src="/images/cluster10.png" alt="drawing" width="500"/>

**Ward**

<img src="/images/cluster11.png" alt="drawing" width="500"/>

### ANOVA
After obtaining the clusters, ANOVA test can be done for each of the plants, comparing the means across clusters. If the <img src="https://render.githubusercontent.com/render/math?math=p"> value is small, we may conclude that there are significant differences across clusters.

# K-Means CLustering
## Procedures
1. Partition the items into <img src="https://render.githubusercontent.com/render/math?math=K"> initial clusters.
2. Loop thorugh the items and assign each item to the cluster whose center is closest. (Accordingly, the centroids of the clusters receiving and losing the item should be recalculated.)
3. Repeat 2 until no more assignments.

## Example
We repeat the above example using the `Iris` dataset.
```python
from sklearn.cluster import KMeans

kmeans2 = KMeans(n_clusters=2, random_state=0).fit(data_scaled)
kmeans3 = KMeans(n_clusters=3, random_state=0).fit(data_scaled)
kmeans4 = KMeans(n_clusters=4, random_state=0).fit(data_scaled)


plt.figure()  
plt.scatter(data_scaled['sepalL'], data_scaled['sepalW'], c=kmeans2.labels_) 

plt.figure()  
plt.scatter(data_scaled['sepalL'], data_scaled['sepalW'], c=kmeans3.labels_) 

plt.figure()  
plt.scatter(data_scaled['sepalL'], data_scaled['sepalW'], c=kmeans4.labels_) 
```

**2 clusters**

<img src="/images/cluster12.png" alt="drawing" width="500"/>

**3 clusters**

<img src="/images/cluster13.png" alt="drawing" width="500"/>

**4 clusters**

<img src="/images/cluster14.png" alt="drawing" width="500"/>

The results are similiar to the agglomerative clustering and we can also run ANOVA test to compare the means of diffearent clusters.


---
## Table of Contents
- [Probability vs Statistics](https://c-huang-tty.github.io/posts/2021/01/01/probability-and-statistics/)
- [Shakespear's New Poem](https://c-huang-tty.github.io/posts/2021/01/02/application-of-statistics/)
- [Some Common Discrete Distributions](https://c-huang-tty.github.io/posts/2021/01/03/some-common-discrete-distributions/)
- [Some Common Continuous Distributions](https://c-huang-tty.github.io/posts/2021/01/04/some-common-continuous-distributions/)
- [Statistical Quantities](https://c-huang-tty.github.io/posts/2021/01/05/statistical-quantities/)
- [Order Statistics](https://c-huang-tty.github.io/posts/2021/01/06/order-statistics/)
- [Multivariate Normal Distributions](https://c-huang-tty.github.io/posts/2021/01/07/multivariate-normal-distributions/)
- [Conditional Distributions and Expectation](https://c-huang-tty.github.io/posts/2021/01/08/conditonal-distributions-and-expectation/)
- [Problem Set [01] - Probabilities](https://c-huang-tty.github.io/posts/2021/01/21/problem-set-probabilities/)
- [Parameter Point Estimation](https://c-huang-tty.github.io/posts/2021/01/09/parameter-point-estimation/)
- [Evaluation of Point Estimation](https://c-huang-tty.github.io/posts/2021/01/10/evaluation-point-estimation/)
- [Parameter Interval Estimation](https://c-huang-tty.github.io/posts/2021/01/11/parameter-interval-estimation/)
- [Problem Set [02] - Parameter Estimation](https://c-huang-tty.github.io/posts/2021/01/22/problem-set-parameter-estimation/)
- [Parameter Hypothesis Test](https://c-huang-tty.github.io/posts/2021/01/12/parameter-hypothesis-test/)
- [t Test](https://c-huang-tty.github.io/posts/2021/01/13/t-test/)
- [Chi-Squared Test](https://c-huang-tty.github.io/posts/2021/01/14/chi-squared-test/)
- [Analysis of Variance](https://c-huang-tty.github.io/posts/2021/01/15/analysis-of-variance/)
- [Summary of Statistical Tests](https://c-huang-tty.github.io/posts/2021/01/16/summary-of-statistical-tests/)
- [Python [01] - Data Representation](https://c-huang-tty.github.io/posts/2021/01/17/statistics-python-data-representation/)
- [Python [02] - t Test & F Test](https://c-huang-tty.github.io/posts/2021/01/18/statistics-python-t-F-test/)
- [Python [03] - Chi-Squared Test](https://c-huang-tty.github.io/posts/2021/01/19/statistics-chi-squared-test/)
- [Experimental Design](https://c-huang-tty.github.io/posts/2021/01/20/experimental-design/)
- [Monte Carlo](https://c-huang-tty.github.io/posts/2021/01/23/monte-carlo/)
- [Variance Reducing Techniques](https://c-huang-tty.github.io/posts/2021/01/24/variance-reducing-techniques/)
- [From Uniform to General Distributions](https://c-huang-tty.github.io/posts/2021/01/25/from-uniform-to-general-distributions/)
- [Problem Set [03] - Monte Carlo](https://c-huang-tty.github.io/posts/2021/01/26/problem-set-monte-carlo/)
- [Unitary Regression Model](https://c-huang-tty.github.io/posts/2021/01/27/unitary-regression-model/)
- [Multiple Regression Model](https://c-huang-tty.github.io/posts/2021/01/28/multiple-regression-model/)
- [Factor and Principle Component Analysis](https://c-huang-tty.github.io/posts/2021/01/29/factor-principle-component-analysis/)
- [Clustering Analysis](https://c-huang-tty.github.io/posts/2021/01/30/clustering-analysis/)
- [Summary](https://c-huang-tty.github.io/posts/2021/01/31/summary/)
