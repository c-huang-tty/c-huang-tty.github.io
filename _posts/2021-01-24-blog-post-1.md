---
title: 'Statistics [24]: Variance Reducing Techniques'
date: 2021-01-24
permalink: /posts/2021/01/24/variance-reducing-techniques/
tags:
  - Statistics
---

---
## Antithetic Variates
Consider we have an even number of samples, <img src="https://render.githubusercontent.com/render/math?math=2n">, from <img src="https://render.githubusercontent.com/render/math?math=p(x)">. One approach is to generate correlated samples to reduce the variance by cancellations in thier sum. The estimate:

<img src="https://render.githubusercontent.com/render/math?math=I = \dfrac{1}{2n}{\displaystyle \sum_{i=1}^{2n}f(x_i) \dfrac{1}{n}\sum_{i=1}^n g(x_{2i})}">

where

<img src="https://render.githubusercontent.com/render/math?math=g(x_{2i}) = \dfrac{f(x_{2i-1}) %2B f(x_{2i})}{2}">

The variance <img src="https://render.githubusercontent.com/render/math?math=var(g(x_{2i}))">,

<img src="https://render.githubusercontent.com/render/math?math=var(g(x_{2i})) = var\left(\dfrac{f(x_{2i-1}) %2B f(x_{2i})}{2}\right) = \dfrac{1}{4}\left[var(f(x_{2i-1})) %2B var(x_{2i}) %2B 2cov(f(x_{2i-1}),f(x_{2i}))\right]">

Conclusion:
1. When <img src="https://render.githubusercontent.com/render/math?math=cov(f(x_{2i-1}),f(x_{2i}))=0">, the variance remains the same;
2. When <img src="https://render.githubusercontent.com/render/math?math=cov(f(x_{2i-1}),f(x_{2i}))<0">, the variance decreases;
3. WHen <img src="https://render.githubusercontent.com/render/math?math=cov(f(x_{2i-1}),f(x_{2i}))>0">, the variance increases.

### Example 1
Estimate <img src="https://render.githubusercontent.com/render/math?math=\theta=E(e^Y) = \int_{0}^1e^xdx, Y\sim U[0,1]">. 

Assume <img src="https://render.githubusercontent.com/render/math?math=Y_1"> and <img src="https://render.githubusercontent.com/render/math?math=Y_2"> are two samples, let the two samaples be <img src="https://render.githubusercontent.com/render/math?math=Y"> and <img src="https://render.githubusercontent.com/render/math?math=1-Y">, then

<img src="https://render.githubusercontent.com/render/math?math=cov(e^Y,e^{1-Y})=E(e^Ye^{1-Y}) - E(e^Y)E(e^{1-Y}) = e - (e-1)^2 = -0.2342">

Hence, 

<img src="https://render.githubusercontent.com/render/math?math=var\left(\dfrac{e^Y%2B e^{1-Y}}{2}\right) = \dfrac{var(e^Y)}{2}%2B \dfrac{cov(e^Y,e^{1-Y})}{2} = 0.0039">


---
## Control Variates
Suppose we have a function <img src="https://render.githubusercontent.com/render/math?math=g(x)"> for which <img src="https://render.githubusercontent.com/render/math?math=E(g(x))"> is known. Let 

<img src="https://render.githubusercontent.com/render/math?math=h(x) = f(x) - c(g(x)-E(g(x)))">

The estimate can be given by 

<img src="https://render.githubusercontent.com/render/math?math=I = \dfrac{1}{n}{\displaystyle \sum_{i=1}^n(f(x_i)-cg(x_i)) %2B cE(g(x))}">

The variance of <img src="https://render.githubusercontent.com/render/math?math=h(x)"> is given by

<img src="https://render.githubusercontent.com/render/math?math=var(h(x)) = var(f(x)) %2B c^2var(g(x))-2c\cdot cov(f(x),g(x))">

Differentiate with respect to <img src="https://render.githubusercontent.com/render/math?math=c">

<img src="https://render.githubusercontent.com/render/math?math=\dfrac{d\,var(h(x))}{d\,c} = 2c\cdot var(g(x)) - 2cov(f(x),g(x))=0\Rightarrow c = \dfrac{cov(f(x),g(x))}{var(g(x))}">

Then,

<img src="https://render.githubusercontent.com/render/math?math=var(h(x)) = var(f(x)) - \dfrac{cov^2(f(x),g(x))}{var(g(x))} = var(f(x))\left[1 - corr^2(f(x),g(x))\right]">

where

<img src="https://render.githubusercontent.com/render/math?math=corr(f(x),g(x))=\dfrac{cov(f(x),g(x))}{\sqrt{var(f(x)\cdot var(g(x)}}">

### Example 2
Estimate <img src="https://render.githubusercontent.com/render/math?math=\theta=E(e^Y) = \int_{0}^1e^xdx, Y\sim U[0,1]">. 

Let <img src="https://render.githubusercontent.com/render/math?math=f(x) = e^Y, g(x) = Y">, there is 

<img src="https://render.githubusercontent.com/render/math?math=cov(f(x),g(x)) = cov(e^Y,Y) = 0.14086">

Then

<img src="https://render.githubusercontent.com/render/math?math=var(e^Y - c(Y - \dfrac{1}{2})) = var(e^Y) - 12\times 0.14086^2 = 0.0039">

---
## Stratified Sampling
Let's divide the whole space into <img src="https://render.githubusercontent.com/render/math?math=k"> subspaces, the final results would be the sum of all partial results. 

<img src="https://render.githubusercontent.com/render/math?math=E(f(x) = {\displaystyle \int_Rf(x)p(x)dx = \sum_{i=1}^k\int_{R_j}f(x)p(x)dx}">

The MC estimate becomes

<img src="https://render.githubusercontent.com/render/math?math=I = {\displaystyle \sum_{j=1}^k\dfrac{vol(R_j)}{n_j}\sum_{i=1}^n_jf(x_i)}">

where <img src="https://render.githubusercontent.com/render/math?math=n_j"> is the number of points on <img src="https://render.githubusercontent.com/render/math?math=R_j">, and <img src="https://render.githubusercontent.com/render/math?math=vol(R_j)"> is the volume of the subspace. 

The variance becomes

<img src="https://render.githubusercontent.com/render/math?math=\sigma^2 = {\displaystyle \sum_{j=1}^k\dfrac{vol^2(R_j)}{n_j}var_{R_j}(f(x))}">

where 

<img src="https://render.githubusercontent.com/render/math?math=var_{R_j}(f(x)) = {\displaystyle \dfrac{1}{vol(R_j)}\int_{R_j}\left(f(x) - \dfrac{1}{vol(R_j)}\int_{R_j}f(x)p(x)dx\right)^2p(x)dx}">

---
## Importance Sampling
The pdf under the integral, <img src="https://render.githubusercontent.com/render/math?math=p(x)">, may not be the best pdf for MC integration. In this case, we can use a different and simpler pdf <img src="https://render.githubusercontent.com/render/math?math=q(x)"> from which we can draw the samples. <img src="https://render.githubusercontent.com/render/math?math=q(x)"> is called the __importance density__. Hence,

<img src="https://render.githubusercontent.com/render/math?math=E(f(X)) = {\displaystyle \int_a^b f(x)p(x)dx = \int_a^b\dfrac{f(x)p(x)}{q(x)}q(x)dx = E\left(\dfrac{f(x)p(x)}{q(x)}\right)}">

By generating <img src="https://render.githubusercontent.com/render/math?math=n"> samples <img src="https://render.githubusercontent.com/render/math?math=x_i\sim q(x)">, the estimate becomes

<img src="https://render.githubusercontent.com/render/math?math={\displaystyle I = \dfrac{1}{n}\sum_{i=1}^n\dfrac{f(x_i)p(x_i)}{q(x_i)} = \dfrac{1}{n}\sum_{i=1}^nW(x_i)f(x_i)}">

where 

<img src="https://render.githubusercontent.com/render/math?math=W(x_i) = \dfrac{p(x_i)}{q(x_i)}"> 

is called __importance weight__. 

Notice that <img src="https://render.githubusercontent.com/render/math?math=p(x_i) = W(x_i)q(x_i)">, we have

<img src="https://render.githubusercontent.com/render/math?math={\displaystyle E(f(x)) = \dfrac{\int_a^bW(x)f(x)q(x)}{\int_a^bp(x)dx} = \dfrac{\int_a^bW(x)f(x)q(x)}{\int_a^bW(x)q(x)dx}}">

Hence,

<img src="https://render.githubusercontent.com/render/math?math={\displaystyle I = \dfrac{\dfrac{1}{n}\sum_{i=1}^nW(x_i)f(x_i)}{\dfrac{1}{n}\sum_{i=1}^nW(x_i)} = \sum_{i=1}^nW_n(x_i)f(x_i)}">

where 

<img src="https://render.githubusercontent.com/render/math?math={\displaystyle W_n(x_i) = \dfrac{W(x_i)}{\sum_{i=1}^nW(x_i)}}"> 

are the __normalized importance weights__. We can see that the average sum becomes weighted sum, reflecting the relative importance of the sample (point). This is the basis for particle filtering.

### Example 3
Estimate <img src="https://render.githubusercontent.com/render/math?math=\theta=E(e^Y) = \int_{0}^1e^xdx, Y\sim U[0,1]">. 

Let <img src="https://render.githubusercontent.com/render/math?math=q(x) = \dfrac{2}{3}(1%2B x), x\in[0,1]"> and <img src="https://render.githubusercontent.com/render/math?math=\theta=p(x) = 1">, then 

<img src="https://render.githubusercontent.com/render/math?math={\displaystyle \theta = E\left(\dfrac{e^X}{\dfrac{2}{3}(1%2B X)}\right) = \int_0^1e^xdx}">

<img src="https://render.githubusercontent.com/render/math?math={\displaystyle var\left(\dfrac{e^X}{\dfrac{2}{3}(1%2B X)}\right) = \dfrac{3}{2}\int_{0}^1\dfrac{e^{2x}}{1%2B x}dx - (e-1)^2 = 0.0269}">

---




