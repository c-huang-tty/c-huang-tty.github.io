---
title: 'Statistics [24]: Variance Reducing Techniques'
date: 2021-01-24
permalink: /posts/2021/01/24/variance-reducing-techniques/
tags:
  - Statistics
---

---
## Antithetic Variates
Consider we have an even number of samples, <img src="https://render.githubusercontent.com/render/math?math=2n">, from <img src="https://render.githubusercontent.com/render/math?math=p(x)">. One approach is to generate correlated samples to reduce the variance by cancellations in thier sum. The estimate:

<img src="https://render.githubusercontent.com/render/math?math=I = \dfrac{1}{2n}{\displaystyle \sum_{i=1}^{2n}f(x_i) \dfrac{1}{n}\sum_{i=1}^n g(x_{2i})}">

where

<img src="https://render.githubusercontent.com/render/math?math=g(x_{2i}) = \dfrac{f(x_{2i-1}) %2B f(x_{2i})}{2}">

The variance <img src="https://render.githubusercontent.com/render/math?math=var(g(x_{2i}))">,

<img src="https://render.githubusercontent.com/render/math?math=var(g(x_{2i})) = var\left(\dfrac{f(x_{2i-1}) %2B f(x_{2i})}{2}\right) = \dfrac{1}{4}\left[var(f(x_{2i-1})) %2B var(x_{2i}) %2B 2cov(f(x_{2i-1}),f(x_{2i}))\right]">

Conclusion:
1. When <img src="https://render.githubusercontent.com/render/math?math=cov(f(x_{2i-1}),f(x_{2i}))=0">, the variance remains the same;
2. When <img src="https://render.githubusercontent.com/render/math?math=cov(f(x_{2i-1}),f(x_{2i}))<0">, the variance decreases;
3. WHen <img src="https://render.githubusercontent.com/render/math?math=cov(f(x_{2i-1}),f(x_{2i}))>0">, the variance increases.

### Example 1
Estimate <img src="https://render.githubusercontent.com/render/math?math=\theta=E(e^U) = \int_{0}^1e^xdx">. 

Assume <img src="https://render.githubusercontent.com/render/math?math=U_1"> and <img src="https://render.githubusercontent.com/render/math?math=U_2"> are two samples, let the two samaples be <img src="https://render.githubusercontent.com/render/math?math=U"> and <img src="https://render.githubusercontent.com/render/math?math=1-U">, then

<img src="https://render.githubusercontent.com/render/math?math=cov(e^U,e^{1-U})=E(e^Ue^{1-U}) - E(e^U)E(e^{1-U}) = e - (e-1)^2 = -0.2342">

Hence, 

<img src="https://render.githubusercontent.com/render/math?math=var\left(\dfrac{e^U%2B e^{1-U}}{2}\right) = \dfrac{var(e^U)}{2}%2B \dfrac{cov(e^U,e^{1-U})}{2} = 0.0039">


---
## Control Variates
Suppose we have a function <img src="https://render.githubusercontent.com/render/math?math=g(x)"> for which <img src="https://render.githubusercontent.com/render/math?math=E(g(x))"> is known. Let 

<img src="https://render.githubusercontent.com/render/math?math=h(x) = f(x) - c(g(x)-E(g(x)))">

The estimate can be given by 

<img src="https://render.githubusercontent.com/render/math?math=I = \dfrac{1}{n}{\displaystyle \sum_{i=1}^n(f(x_i)-cg(x_i)) %2B cE(g(x))}">

The variance of <img src="https://render.githubusercontent.com/render/math?math=h(x)"> is given by

<img src="https://render.githubusercontent.com/render/math?math=var(h(x)) = var(f(x)) %2B c^2var(g(x))-2c\cdot cov(f(x),g(x))">

Differentiate with respect to <img src="https://render.githubusercontent.com/render/math?math=c">

<img src="https://render.githubusercontent.com/render/math?math=\dfrac{d\,var(h(x))}{d\,c} = 2c\cdot var(g(x)) - 2cov(f(x),g(x))=0\Rightarrow c = \dfrac{cov(f(x),g(x))}{var(g(x))}">

Then,

<img src="https://render.githubusercontent.com/render/math?math=var(h(x)) = var(f(x)) - \dfrac{cov^2(f(x),g(x))}{var(g(x))} = var(f(x))\left[1 - corr^2(f(x),g(x))\right]">

where

<img src="https://render.githubusercontent.com/render/math?math=corr(f(x),g(x))=\dfrac{cov(f(x),g(x))}{\sqrt{var(f(x)\cdot var(g(x)}}">

### Example 2
Estimate <img src="https://render.githubusercontent.com/render/math?math=\theta=E(e^U) = \int_{0}^1e^xdx">. 

Let <img src="https://render.githubusercontent.com/render/math?math=f(x) = e^U, g(x) = U">, there is 

<img src="https://render.githubusercontent.com/render/math?math=cov(f(x),g(x)) = cov(e^U,U) = 0.14086">

Then

<img src="https://render.githubusercontent.com/render/math?math=var(e^U - c(U - \dfrac{1}{2})) = var(e^U) - 12\times 0.14086^2 = 0.0039">

---
## Stratified Sampling
Let's divide the whole space into <img src="https://render.githubusercontent.com/render/math?math=k"> subspaces, the final results would be the sum of all partial results. 

<img src="https://render.githubusercontent.com/render/math?math=E(f(x) = {\displaystyle \int_Rf(x)p(x)dx = \sum_{i=1}^k\int_{R_j}f(x)p(x)dx}">

The MC estimate becomes

<img src="https://render.githubusercontent.com/render/math?math=I = {\displaystyle \sum_{j=1}^k\dfrac{vol(R_j)}{n_j}\sum_{i=1}^n_jf(x_i)}">

where <img src="https://render.githubusercontent.com/render/math?math=n_j"> is the number of points on <img src="https://render.githubusercontent.com/render/math?math=R_j">, and <img src="https://render.githubusercontent.com/render/math?math=vol(R_j)"> is the volume of the subspace. 

The variance becomes

<img src="https://render.githubusercontent.com/render/math?math=\sigma^2 = {\displaystyle \sum_{j=1}^k\dfrac{vol^2(R_j)}{n_j}var_{R_j}(f(x))}">

where 

<img src="https://render.githubusercontent.com/render/math?math=var_{R_j}(f(x)) = {\displaystyle \dfrac{1}{vol(R_j)}\int_{R_j}\left(f(x) - \dfrac{1}{vol(R_j)}\int_{R_j}f(x)p(x)dx\right)^2p(x)dx}">




---
## Importance Sampling
The pdf under the integral, <img src="https://render.githubusercontent.com/render/math?math=p(x)">, may not be the best pdf for MC integration. In this case, we can use a different and simpler pdf <img src="https://render.githubusercontent.com/render/math?math=q(x)"> from which we can draw the samples. <img src="https://render.githubusercontent.com/render/math?math=q(x)"> is called the __importance density__. Hence,

<img src="https://render.githubusercontent.com/render/math?math=E(f(X)) = {\displaystyle \int_a^b f(x)p(x)dx = \int_a^b\dfrac{f(x)p(x)}{q(x)}q(x)dx = E\left(\dfrac{f(x)p(x)}{q(x)}\right)}">

By generating <img src="https://render.githubusercontent.com/render/math?math=n"> samples <img src="https://render.githubusercontent.com/render/math?math=x_i\sim q(x)">, the estimate becomes

<img src="https://render.githubusercontent.com/render/math?math={\displaystyle I = \dfrac{1}{n}\sum_{i=1}^n\dfrac{f(x_i)p(x_i)}{q(x_i)} = \dfrac{1}{n}\sum_{i=1}^nW(x_i)f(x_i)}">

where 

<img src="https://render.githubusercontent.com/render/math?math=W(x_i) = \dfrac{p(x_i)}{q(x_i)}"> 

is called __importance weight__. 

[comment]: <>(Since the normalizing factor <img src="https://render.githubusercontent.com/render/math?math=p(x_i)"> is unknown, we have to normalize the weights such that <img src="https://render.githubusercontent.com/render/math?math=\sum_{i=1}^nW(x_i)=1">.)

[comment]: <>(<img src="https://render.githubusercontent.com/render/math?math={\displaystyle I = \dfrac{1}{n}\cdot\dfrac{\sum_{i=1}^nW(x_i)f(x_i)}{\sum_{i=1}^nW(x_i)} = \dfrac{1}{n}\sum_{i=1}^nW_n(x_i)f(x_i)}">)

[comment]: <>(where )

[comment]: <>(<img src="https://render.githubusercontent.com/render/math?math={\displaystyle W_n(x_i) = \dfrac{W(x_i)}{\sum_{i=1}^nW(x_i)}}"> are the __normalized importance weights__. )

