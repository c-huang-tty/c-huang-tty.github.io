---
title: 'Statistics [27]: Unitary Regression Model'
date: 2021-01-27
permalink: /posts/2021/01/27/unitary-regression-model/
tags:
  - Statistics
---

Basics of unitary regression, especially linear regression.

---
## Review: Correlation
Correlation tells us whether the two variables are linearly dependent. If the two variables <img src="https://render.githubusercontent.com/render/math?math=X"> and <img src="https://render.githubusercontent.com/render/math?math=Y"> are independent, then <img src="https://render.githubusercontent.com/render/math?math=corr(X,Y)=0">; however, if <img src="https://render.githubusercontent.com/render/math?math=corr(X,Y)=0">, it doesn't necessarily mean that <img src="https://render.githubusercontent.com/render/math?math=X"> and <img src="https://render.githubusercontent.com/render/math?math=Y"> are independent, there might still be nonlinear relationships between the two variables. 

For example, <img src="https://render.githubusercontent.com/render/math?math=X\sim N(0,1), Y=X^2">, <img src="https://render.githubusercontent.com/render/math?math=X"> and <img src="https://render.githubusercontent.com/render/math?math=Y"> are obviously not independent. However,

<img src="https://render.githubusercontent.com/render/math?math=cov(X,Y) = cov(X,X^2) = E(X^3) - E(X)E(X^2) = 0">

It doesn't mean that <img src="https://render.githubusercontent.com/render/math?math=X"> and <img src="https://render.githubusercontent.com/render/math?math=Y"> are independent, it only says that <img src="https://render.githubusercontent.com/render/math?math=X"> and <img src="https://render.githubusercontent.com/render/math?math=Y"> are linearly independent. 

<img src="https://render.githubusercontent.com/render/math?math=corr(X,Y)=\pm 1"> if and only if <img src="https://render.githubusercontent.com/render/math?math=X"> and <img src="https://render.githubusercontent.com/render/math?math=Y"> are lienarly dependent almost everywhere. That is 

<img src="https://render.githubusercontent.com/render/math?math=P(Y = aX%2B) = 1">

---
## Least Square
Suppose the __independent variable__ is <img src="https://render.githubusercontent.com/render/math?math=X"> and the __dependent variable__ is <img src="https://render.githubusercontent.com/render/math?math=Y">. The objective of linear fitting is to find a linear relationship between <img src="https://render.githubusercontent.com/render/math?math=X"> and <img src="https://render.githubusercontent.com/render/math?math=Y"> such that <img src="https://render.githubusercontent.com/render/math?math=\hat{Y}_i = b_1X_i %2B b_0">. __Residual__ is defined as the difference between the actual value and the fitted value, that is <img src="https://render.githubusercontent.com/render/math?math=e_i = Y_i-\hat{Y}_i">.

Least square is to choose <img src="https://render.githubusercontent.com/render/math?math=b_0"> and <img src="https://render.githubusercontent.com/render/math?math=b_1"> to minimize <img src="https://render.githubusercontent.com/render/math?math={\displaystyle \sum_{i=1}^Ne_i^2}">, that is

<img src="https://render.githubusercontent.com/render/math?math={\displaystyle L(b_0,b_1) = \min \sum_{i=1}^N(y_i-(b_0%2B b_1x_i))^2}">

Differentiate <img src="https://render.githubusercontent.com/render/math?math=L(b_0,b_1)"> with respect to <img src="https://render.githubusercontent.com/render/math?math=b_0"> and <img src="https://render.githubusercontent.com/render/math?math=b_1"> respectively, we have

<img src="https://render.githubusercontent.com/render/math?math={\displaystyle \sum_{i=1}^N(y_i - (b_0%2B b_1x_i)) = 0 \Rightarrow b_0 = \dfrac{\sum_{i=1}^Ny_i - \sum_{i=1}^Nx_ib_1}{N}}">

<img src="https://render.githubusercontent.com/render/math?math={\displaystyle \sum_{i=1}^N[(y_i - (b_0%2B b_1x_i))x_i] = 0 \Rightarrow b_1 = \dfrac{\sum_{i=1}^Nx_i\sum_{i=1}^Ny_i - N\sum_{i=1}^N(x_iy_i)}{\left(\sum_{i=1}^Nx_i\right)^2 - N\sum_{i=1}^Nx_i^2}}">

Thus,

<img src="https://render.githubusercontent.com/render/math?math={\displaystyle b_1 = \dfrac{\sum_{i=1}^N(x_i-\bar{x})(y_i-\bar{y})}{\sum_{i=1}^N(x_i-\bar{x})^2} = \dfrac{S_{XY}}{S_X^2}=r_{xy}\cdot\dfrac{S_y}{S_x}, \ \ b_0 = \bar{y} - b_1\bar{x}}">

---
## Analysis of Variance
<img src="https://render.githubusercontent.com/render/math?math={\displaystyle \sum_{i=1}^N(y_i-\bar{y})^2 = \sum_{i=1}^N(y_i-\hat{y}_i) %2B 2\sum_{i=1}^N(y_i-\hat{y}_i)(\hat{y}_i - \bar{y}) %2B \sum_{i=1}^N(\hat{y}_i - \bar{y})^2}">

where 

<img src="https://render.githubusercontent.com/render/math?math={\displaystyle \sum_{i=1}^N(y_i-\hat{y}_i)(\hat{y}_i - \bar{y}) = \sum_{i-1}^Ne_i\hat{y}_i - \bar{y}\sum_{i=1}^Ne_i = 0}">

Therefore,

<img src="https://render.githubusercontent.com/render/math?math={\displaystyle \sum_{i=1}^N(y_i-\bar{y})^2 = \sum_{i=1}^N(y_i-\hat{y}_i) %2B \sum_{i=1}^N(\hat{y}_i - \bar{y})^2}">

where 

- <img src="https://render.githubusercontent.com/render/math?math={\displaystyle \sum_{i=1}^N(y_i-\bar{y})^2}"> is the total sum of squares (TSS)
- <img src="https://render.githubusercontent.com/render/math?math={\displaystyle \sum_{i=1}^N(\hat{y}_i - \bar{y})^2}"> is the regression sum of squares (RegSS)
- <img src="https://render.githubusercontent.com/render/math?math={\displaystyle \sum_{i=1}^N(y_i-\hat{y}_i)^2}"> is the residual sum of squares (RSS)

A good fitting has a big RegSS and a small RSS. If TSS = RegSS, the fitting is  perfect. The goodness of fit is determined by the coefficient of determination <img src="https://render.githubusercontent.com/render/math?math=R^2">,

<img src="https://render.githubusercontent.com/render/math?math=R^2 = \dfrac{\text{RegSS}}{\text{TSS}} = 1 - \dfrac{\text{RSS}}{\text{RegSS}}">

---
## Simple Linear Regression Model
The linear regression model is usually expressed as

<img src="https://render.githubusercontent.com/render/math?math=Y = \beta_0 %2B \beta_1 X %2B \varepsilon">

where <img src="https://render.githubusercontent.com/render/math?math=\varepsilon \sim N(0,\sigma^2)"> is the error term.

The least square model <img src="https://render.githubusercontent.com/render/math?math=\hat{Y} = b_0%2B b_1X"> is an approximation of this linear regression model, where

<img src="https://render.githubusercontent.com/render/math?math=b_0 \sim N\left(\beta_0, \sigma^2\left(\dfrac{1}{n}%2B \dfrac{\bar{x}^2}{L_{XX}}\right)\right), b_1 \sim N\left(\beta_1,\dfrac{\sigma^2}{L_{XX}}\right)">

where <img src="https://render.githubusercontent.com/render/math?math=L_{XX} = {\displaystyle \sum_{i=1}^N(x_i-\bar{x})^2}">

To prove this, 

<img src="https://render.githubusercontent.com/render/math?math=b_1 = \dfrac{\sum_{i=1}^N(x_i-\bar{x})(y_i-\bar{y})}{\sum_{i=1}^N(x_i-\bar{x})^2} = \dfrac{\sum_{i=1}^N(x_i-\bar{x})y_i}{\sum_{i=1}^N(x_i-\bar{x})^2} - \dfrac{\sum_{i=1}^N(x_i-\bar{x})\bar{y}}{\sum_{i=1}^N(x_i-\bar{x})^2}">

<img src="https://render.githubusercontent.com/render/math?math== \dfrac{\sum_{i=1}^N(x_i-\bar{x})(\beta_0%2B\beta_1x_i%2B\varepsilon_i)}{\sum_{i=1}^N(x_i-\bar{x})^2} - \dfrac{\sum_{i=1}^N(x_i-\bar{x})(\beta_0 %2B \beta_1\bar{x})}{\sum_{i=1}^N(x_i-\bar{x})^2} = \dfrac{\sum_{i=1}^N(x_i-\bar{x})(\beta_1x_i - \beta_1\bar{x} %2B \varepsilon_i)}{\sum_{i=1}^N(x_i-\bar{x})^2}">

<img src="https://render.githubusercontent.com/render/math?math== \dfrac{\beta_1\sum_{i=1}^N(x_i-\bar{x})^2}{\sum_{i=1}^N(x_i-\bar{x})^2} %2B \dfrac{\sum_{i=1}^N(x_i-\bar{x})\varepsilon_i}{\sum_{i=1}^N(x_i-\bar{x})^2} = \beta_1 %2B \dfrac{\sum_{i=1}^N(x_i-\bar{x})\varepsilon_i}{\sum_{i=1}^N(x_i-\bar{x})^2}">

Hence,

<img src="https://render.githubusercontent.com/render/math?math=E(b_1) = \beta_1">

<img src="https://render.githubusercontent.com/render/math?math=var(b_1)=  var\left(\dfrac{\sum_{i=1}^N(x_i-\bar{x})\varepsilon_i}{\sum_{i=1}^N(x_i-\bar{x})^2}\right) = \sum_{i=1}^N\left(\dfrac{x_i-\bar{x}}{\sum_{i=1}^N(x_i-\bar{x})^2}\right)^2\sigma^2 = \dfrac{\sigma^2}{\sum_{i=1}^N(x_i-\bar{x})^2} = \dfrac{\sigma^2}{L_{XX}}">

With <img src="https://render.githubusercontent.com/render/math?math=b_1">,

<img src="https://render.githubusercontent.com/render/math?math=b_0 = \bar{y} - b_1\bar{x} = \beta_0 %2B \beta_1\bar{x} %2B \bar{\varepsilon} - b_1\bar{x}">

Therefore,

<img src="https://render.githubusercontent.com/render/math?math=E(b_0) = \beta_0">

<img src="https://render.githubusercontent.com/render/math?math=var(b_0) = var\left(\bar{\varepsilon} - b_1\bar{x}\right) = \dfrac{var(\varepsilon)}{n}%2B \bar{x}^2var(b_1) = \sigma^2\left(\dfrac{1}{n}%2B \dfrac{\bar{x}^2}{L_{XX}}\right)">

---
## Properties
Least square estimation has the following properties:
1. <img src="https://render.githubusercontent.com/render/math?math=b_0 \sim N\left(\beta_0, \sigma^2\left(\dfrac{1}{n}%2B \dfrac{\bar{x}^2}{(n-1)s_x^2}\right)\right)">
2. <img src="https://render.githubusercontent.com/render/math?math=b_1 \sim N\left(\beta_1, \dfrac{\sigma^2}{(n-1)s_x^2}\right)">
3. <img src="https://render.githubusercontent.com/render/math?math=\dfrac{\text{RSS}}{\sigma^2}\sim \chi^2(n-2)">
4. <img src="https://render.githubusercontent.com/render/math?math=\bar{y},b_1,\text{RSS}"> are mutually independent.

---
## Significance Test

---
## Prediction


---
