---
title: 'Statistics [29]: Factor and Principle Component Analysis'
date: 2021-01-29
permalink: /posts/2021/01/29/factor-principle-component-analysis/
tags:
  - Statistics
---

In practice, data may contain many variables, however, not all of them have significant influence on the results we want to analysis or predict. Factor analysis and principle component analysis are two basic methods that meant to reduce the dimension of the data to make it easier to understand and analyze.

## Notations
Observabale variable (with <img src="https://render.githubusercontent.com/render/math?math=p"> traits): 

<img src="https://render.githubusercontent.com/render/math?math=X = (X_1, X_2, ..., X_p)^T">

Common factors (with <img src="https://render.githubusercontent.com/render/math?math=m"> factors): 

<img src="https://render.githubusercontent.com/render/math?math=f = (f_1, f_2, ..., f_m),\ \ m\ll p">

The factor model can be thought of as a series of multiple regressions:
  
<img src="https://render.githubusercontent.com/render/math?math=X_1 = \mu_1 %2B l_{11}f_1 %2B l_{12}f_2 %2B \cdots %2B l_{1m}f_m %2B \epsilon_1">

<img src="https://render.githubusercontent.com/render/math?math=X_2 = \mu_2 %2B l_{21}f_1 %2B l_{22}f_2 %2B \cdots %2B l_{2m}f_m %2B \epsilon_2">

<img src="https://render.githubusercontent.com/render/math?math=\vdots">

<img src="https://render.githubusercontent.com/render/math?math=X_p = \mu_1 %2B l_{p1}f_1 %2B l_{p2}f_2 %2B \cdots %2B l_{pm}f_m %2B \epsilon_p">

where <img src="https://render.githubusercontent.com/render/math?math=l_{ij}"> are called factor loadings.

In matrix form:

<img src="https://render.githubusercontent.com/render/math?math=X = \mu %2B Lf %2B \epsilon">

where 

<img src="https://render.githubusercontent.com/render/math?math=\Lambda = \diag(\lambda_1,\lambda_2,...,\lambda_p), \ \ V = [v_1,v_2,...,v_p]">

## Assumptions
### Mean
<img src="https://render.githubusercontent.com/render/math?math=E(\epsilon_i) = 0, i=1,2,...,p">

<img src="https://render.githubusercontent.com/render/math?math=E(f_j) = 0, j=1,2,...,m">

Hence,

<img src="https://render.githubusercontent.com/render/math?math=E(X_i) = \mu_i, i=1,2,...,p">

### Variance
<img src="https://render.githubusercontent.com/render/math?math=var(\epsilon_i) = \psi_i, i=1,2,...,p">, where <img src="https://render.githubusercontent.com/render/math?math=\psi_i"> is the specific variance.

<img src="https://render.githubusercontent.com/render/math?math=var(f_j) = 1, j=1,2,...,m">

### Correlation
<img src="https://render.githubusercontent.com/render/math?math=cov(f_i,f_j) = 0,\ \ \text{for}\ \ i\neq j">

<img src="https://render.githubusercontent.com/render/math?math=cov(\epsilon_i,\epsilon_j) = 0,\ \ \text{for}\ \ i\neq j">

<img src="https://render.githubusercontent.com/render/math?math=cov(\epsilon_i,f_j) = 0,\ \ \text{for}\ \ i=1,2,...,p%3B j=1,2,...,m">

### Model Properties
<img src="https://render.githubusercontent.com/render/math?math={\displaystyle \sigma_i^2 = var(X_i) = E[(X_i-E(X_i))^2] = E[(l_{i1}f_1 %2B l_{i2}f_2 %2B \cdots %2B l_{im}f_m %2B \epsilon_i)^2] = \sum_{j=1}^ml_{ij}^2 %2B \psi_i}">

where <img src="https://render.githubusercontent.com/render/math?math={\displaystyle \sum_{j=1}^ml_{ij}^2}"> is called the communality of the variable <img src="https://render.githubusercontent.com/render/math?math=i">.

<img src="https://render.githubusercontent.com/render/math?math={\displaystyle \sigma_{ij} = cov(X_i,X_j) = E[(X_i-E(X_i))(X_j-E(X_j))] = E[(l_{i1}f_1 %2B l_{i2}f_2 %2B \cdots %2B l_{im}f_m %2B \epsilon_i)(l_{j1}f_1 %2B l_{j2}f_2 %2B \cdots %2B l_{jm}f_m %2B \epsilon_j)] = \sum_{k=1}^ml_{ik}l_{jk}}">

<img src="https://render.githubusercontent.com/render/math?math={\displaystyle cov(X_i,f_j) = l_{ij}}">

In matrix form:

<img src="https://render.githubusercontent.com/render/math?math={\displaystyle \Sigma = LL^T %2B \Psi}">

where <img src="https://render.githubusercontent.com/render/math?math={\displaystyle \Psi = \diag(\psi_1,\psi_2,...,\psi_p)}">

As <img src="https://render.githubusercontent.com/render/math?math={\displaystyle \Sigma}"> is a symmetric matrix, there are <img src="https://render.githubusercontent.com/render/math?math={\displaystyle \dfrac{p(p%2B 1)}{2}}"> parameters in total, which are to be approximated by <img src="https://render.githubusercontent.com/render/math?math={\displaystyle mp%2B p}"> parameters. 


## Principle Component Method
Let <img src="https://render.githubusercontent.com/render/math?math=X_i,i=1,2,...,n"> denote the samples, with

<img src="https://render.githubusercontent.com/render/math?math=X_i = (X_{i1},X_{i2},...,X_{ip})^T">

<img src="https://render.githubusercontent.com/render/math?math=S"> denotes the sample variance matrix,

<img src="https://render.githubusercontent.com/render/math?math=S = \dfrac{1}{n-1}{\displaystyle \sum_{i=1}^n(X_i-\bar{X})(X_i-\bar{X})^T}">

We have <img src="https://render.githubusercontent.com/render/math?math=p"> eigenvalues and <img src="https://render.githubusercontent.com/render/math?math=p"> eigenvectors for this matrix.

Eigenvalues: <img src="https://render.githubusercontent.com/render/math?math=\lambda_1,\lambda_2,...\lambda_p">

Eigenvectors: <img src="https://render.githubusercontent.com/render/math?math=v_1,v_2,...,v_p">

Then we can express the covariance matrix with eigenvalues and eigenfactors as

<img src="https://render.githubusercontent.com/render/math?math=S = V\Lambda V^T = {\displaystyle \sum_{i=1}^p\lambda_iv_iv_i^T}">

The idea behind the principal component method is to approximate this expression. Instead of summing from <img src="https://render.githubusercontent.com/render/math?math=1"> to <img src="https://render.githubusercontent.com/render/math?math=p">, we sum from 1 to <img src="https://render.githubusercontent.com/render/math?math=m">, ignoring the <img src="https://render.githubusercontent.com/render/math?math=p-m"> terms. 

We can rewrite the covariance matrix as 

<img src="https://render.githubusercontent.com/render/math?math={\displaystyle S \approx \sum_{i=1}^m\lambda_iv_iv_i^T = \sum_{i=1}^m\hat{l}_i\hat{l}_i^T, \hat{l}_i = (\hat{l}_{1i},\hat{l}_{2i},...,\hat{l}_{pi})^T}">

This yields the estimator for the factor loadings:

<img src="https://render.githubusercontent.com/render/math?math={\displaystyle \hat{l}_{ij} = v_{ji}\sqrt{\lambda_j}\ \ \text{or}\ \ \hat{l}_{ji} = v_{ij}\sqrt{\lambda_i}}">

To estimate the specific variances <img src="https://render.githubusercontent.com/render/math?math=\Psi">, we only need to estimate the diagonal elements,

<img src="https://render.githubusercontent.com/render/math?math=\hat{\psi}_i = s_i^2 - \sum_{j=1}^m l_{ij}^2 = s_i^2 - {\displaystyle \sum_{j=1}^m\lambda_jv_{ji}^2}">

## Maximum Likelihood Estimation Method


---
## Table of Contents
- [Probability vs Statistics](https://c-huang-tty.github.io/posts/2021/01/01/probability-and-statistics/)
- [Shakespear's New Poem](https://c-huang-tty.github.io/posts/2021/01/02/application-of-statistics/)
- [Some Common Discrete Distributions](https://c-huang-tty.github.io/posts/2021/01/03/some-common-discrete-distributions/)
- [Some Common Continuous Distributions](https://c-huang-tty.github.io/posts/2021/01/04/some-common-continuous-distributions/)
- [Statistical Quantities](https://c-huang-tty.github.io/posts/2021/01/05/statistical-quantities/)
- [Order Statistics](https://c-huang-tty.github.io/posts/2021/01/06/order-statistics/)
- [Multivariate Normal Distributions](https://c-huang-tty.github.io/posts/2021/01/07/multivariate-normal-distributions/)
- [Conditional Distributions and Expectation](https://c-huang-tty.github.io/posts/2021/01/08/conditonal-distributions-and-expectation/)
- [Problem Set [01] - Probabilities](https://c-huang-tty.github.io/posts/2021/01/21/problem-set-probabilities/)
- [Parameter Point Estimation](https://c-huang-tty.github.io/posts/2021/01/09/parameter-point-estimation/)
- [Evaluation of Point Estimation](https://c-huang-tty.github.io/posts/2021/01/10/evaluation-point-estimation/)
- [Parameter Interval Estimation](https://c-huang-tty.github.io/posts/2021/01/11/parameter-interval-estimation/)
- [Problem Set [02] - Parameter Estimation](https://c-huang-tty.github.io/posts/2021/01/22/problem-set-parameter-estimation/)
- [Parameter Hypothesis Test](https://c-huang-tty.github.io/posts/2021/01/12/parameter-hypothesis-test/)
- [t Test](https://c-huang-tty.github.io/posts/2021/01/13/t-test/)
- [Chi-Squared Test](https://c-huang-tty.github.io/posts/2021/01/14/chi-squared-test/)
- [Analysis of Variance](https://c-huang-tty.github.io/posts/2021/01/15/analysis-of-variance/)
- [Summary of Statistical Tests](https://c-huang-tty.github.io/posts/2021/01/16/summary-of-statistical-tests/)
- [Python [01] - Data Representation](https://c-huang-tty.github.io/posts/2021/01/17/statistics-python-data-representation/)
- [Python [02] - t Test & F Test](https://c-huang-tty.github.io/posts/2021/01/18/statistics-python-t-F-test/)
- [Python [03] - Chi-Squared Test](https://c-huang-tty.github.io/posts/2021/01/19/statistics-chi-squared-test/)
- [Experimental Design](https://c-huang-tty.github.io/posts/2021/01/20/experimental-design/)
- [Monte Carlo](https://c-huang-tty.github.io/posts/2021/01/23/monte-carlo/)
- [Variance Reducing Techniques](https://c-huang-tty.github.io/posts/2021/01/24/variance-reducing-techniques/)
- [From Uniform to General Distributions](https://c-huang-tty.github.io/posts/2021/01/25/from-uniform-to-general-distributions/)
- [Problem Set [03] - Monte Carlo](https://c-huang-tty.github.io/posts/2021/01/26/problem-set-monte-carlo/)
- [Unitary Regression Model](https://c-huang-tty.github.io/posts/2021/01/27/unitary-regression-model/)
- [Multiple Regression Model](https://c-huang-tty.github.io/posts/2021/01/28/multiple-regression-model/)
- [Factor and Principle Component Analysis](https://c-huang-tty.github.io/posts/2021/01/29/factor-principle-component-analysis/)
- [Clustering Analysis](https://c-huang-tty.github.io/posts/2021/01/30/clustering-analysis/)
- [Summary](https://c-huang-tty.github.io/posts/2021/01/31/summary/)

