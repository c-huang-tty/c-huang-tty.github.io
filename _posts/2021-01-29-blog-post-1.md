---
title: 'Statistics [29]: Factor and Principle Component Analysis'
date: 2021-01-29
permalink: /posts/2021/01/29/factor-principle-component-analysis/
tags:
  - Statistics
---

In practice, data may contain many variables, however, not all of them have significant influence on the results we want to analysis or predict. Factor analysis and principle component analysis are two basic methods that aim to reduce the dimension of the data to make it easier to understand and analyze. This psot summarizes the basics of factor analysis.

---
---
# Factor Analysis

---
## Notations
Observabale variable (with <img src="https://render.githubusercontent.com/render/math?math=p"> traits): 

<img src="https://render.githubusercontent.com/render/math?math=X = (X_1, X_2, ..., X_p)^T">

Common factors (with <img src="https://render.githubusercontent.com/render/math?math=m"> factors): 

<img src="https://render.githubusercontent.com/render/math?math=f = (f_1, f_2, ..., f_m),\ \ m\ll p">

The factor model can be thought of as a series of multiple regressions:
  
<img src="https://render.githubusercontent.com/render/math?math=X_1 = \mu_1 %2B l_{11}f_1 %2B l_{12}f_2 %2B \cdots %2B l_{1m}f_m %2B \epsilon_1">

<img src="https://render.githubusercontent.com/render/math?math=X_2 = \mu_2 %2B l_{21}f_1 %2B l_{22}f_2 %2B \cdots %2B l_{2m}f_m %2B \epsilon_2">

<img src="https://render.githubusercontent.com/render/math?math=\vdots">

<img src="https://render.githubusercontent.com/render/math?math=X_p = \mu_1 %2B l_{p1}f_1 %2B l_{p2}f_2 %2B \cdots %2B l_{pm}f_m %2B \epsilon_p">

where <img src="https://render.githubusercontent.com/render/math?math=l_{ij}"> are called factor loadings.

In matrix form:

<img src="https://render.githubusercontent.com/render/math?math=X = \mu %2B Lf %2B \epsilon">

where 

<img src="https://render.githubusercontent.com/render/math?math=\Lambda = \diag(\lambda_1,\lambda_2,...,\lambda_p), \ \ V = [v_1,v_2,...,v_p]">

---
## Assumptions
### Mean
<img src="https://render.githubusercontent.com/render/math?math=E(\epsilon_i) = 0, i=1,2,...,p">

<img src="https://render.githubusercontent.com/render/math?math=E(f_j) = 0, j=1,2,...,m">

Hence,

<img src="https://render.githubusercontent.com/render/math?math=E(X_i) = \mu_i, i=1,2,...,p">

### Variance
<img src="https://render.githubusercontent.com/render/math?math=var(\epsilon_i) = \psi_i, i=1,2,...,p">, where <img src="https://render.githubusercontent.com/render/math?math=\psi_i"> is the specific variance.

<img src="https://render.githubusercontent.com/render/math?math=var(f_j) = 1, j=1,2,...,m">

### Correlation
<img src="https://render.githubusercontent.com/render/math?math=cov(f_i,f_j) = 0,\ \ \text{for}\ \ i\neq j">

<img src="https://render.githubusercontent.com/render/math?math=cov(\epsilon_i,\epsilon_j) = 0,\ \ \text{for}\ \ i\neq j">

<img src="https://render.githubusercontent.com/render/math?math=cov(\epsilon_i,f_j) = 0,\ \ \text{for}\ \ i=1,2,...,p%3B j=1,2,...,m">

### Model Properties
<img src="https://render.githubusercontent.com/render/math?math={\displaystyle \sigma_i^2 = var(X_i) = E[(X_i-E(X_i))^2] = E[(l_{i1}f_1 %2B l_{i2}f_2 %2B \cdots %2B l_{im}f_m %2B \epsilon_i)^2] = \sum_{j=1}^ml_{ij}^2 %2B \psi_i}">

where <img src="https://render.githubusercontent.com/render/math?math={\displaystyle \sum_{j=1}^ml_{ij}^2}"> is called the __communality__ of the variable <img src="https://render.githubusercontent.com/render/math?math=i">.

<img src="https://render.githubusercontent.com/render/math?math={\displaystyle \sigma_{ij} = cov(X_i,X_j) = E[(X_i-E(X_i))(X_j-E(X_j))] = E[(l_{i1}f_1 %2B l_{i2}f_2 %2B \cdots %2B l_{im}f_m %2B \epsilon_i)(l_{j1}f_1 %2B l_{j2}f_2 %2B \cdots %2B l_{jm}f_m %2B \epsilon_j)] = \sum_{k=1}^ml_{ik}l_{jk}}">

<img src="https://render.githubusercontent.com/render/math?math={\displaystyle cov(X_i,f_j) = l_{ij}}">

In matrix form:

<img src="https://render.githubusercontent.com/render/math?math={\displaystyle \Sigma = LL^T %2B \Psi}">

where <img src="https://render.githubusercontent.com/render/math?math={\displaystyle \Psi = \diag(\psi_1,\psi_2,...,\psi_p)}">

As <img src="https://render.githubusercontent.com/render/math?math={\displaystyle \Sigma}"> is a symmetric matrix, there are <img src="https://render.githubusercontent.com/render/math?math={\displaystyle \dfrac{p(p%2B 1)}{2}}"> parameters in total, which are to be approximated by <img src="https://render.githubusercontent.com/render/math?math={\displaystyle mp%2B p}"> parameters. 

---
## Principle Component Method
Let <img src="https://render.githubusercontent.com/render/math?math=X_i,i=1,2,...,n"> denote the samples, with

<img src="https://render.githubusercontent.com/render/math?math=X_i = (X_{i1},X_{i2},...,X_{ip})^T">

<img src="https://render.githubusercontent.com/render/math?math=S"> denotes the sample variance matrix,

<img src="https://render.githubusercontent.com/render/math?math=S = \dfrac{1}{n-1}{\displaystyle \sum_{i=1}^n(X_i-\bar{X})(X_i-\bar{X})^T}">

We have <img src="https://render.githubusercontent.com/render/math?math=p"> eigenvalues and <img src="https://render.githubusercontent.com/render/math?math=p"> eigenvectors for this matrix.

Eigenvalues: <img src="https://render.githubusercontent.com/render/math?math=\lambda_1,\lambda_2,...\lambda_p">

Eigenvectors: <img src="https://render.githubusercontent.com/render/math?math=v_1,v_2,...,v_p">

Then we can express the covariance matrix with eigenvalues and eigenfactors as

<img src="https://render.githubusercontent.com/render/math?math=S = V\Lambda V^T = {\displaystyle \sum_{i=1}^p\lambda_iv_iv_i^T}">

The idea behind the principal component method is to approximate this expression. Instead of summing from <img src="https://render.githubusercontent.com/render/math?math=1"> to <img src="https://render.githubusercontent.com/render/math?math=p">, we sum from 1 to <img src="https://render.githubusercontent.com/render/math?math=m">, ignoring the <img src="https://render.githubusercontent.com/render/math?math=p-m"> terms. 

We can rewrite the covariance matrix as 

<img src="https://render.githubusercontent.com/render/math?math={\displaystyle S \approx \sum_{i=1}^m\lambda_iv_iv_i^T = \sum_{i=1}^m\hat{l}_i\hat{l}_i^T, \hat{l}_i = (\hat{l}_{1i},\hat{l}_{2i},...,\hat{l}_{pi})^T}">

This yields the estimator for the factor loadings:

<img src="https://render.githubusercontent.com/render/math?math={\displaystyle \hat{l}_{ij} = v_{ji}\sqrt{\lambda_j}\ \ \text{or}\ \ \hat{l}_{ji} = v_{ij}\sqrt{\lambda_i}}">

To estimate the specific variances <img src="https://render.githubusercontent.com/render/math?math=\Psi">, we only need to estimate the diagonal elements,

<img src="https://render.githubusercontent.com/render/math?math=\hat{\psi}_i = s_i^2 - \sum_{j=1}^m l_{ij}^2 = s_i^2 - {\displaystyle \sum_{j=1}^m\lambda_jv_{ji}^2}">

---
## Maximum Likelihood Estimation Method
Using the Maximum Likelihood Estimation Method, we must assume that the data are independently sampled from a multivariate normal distribution with mean vector <img src="https://render.githubusercontent.com/render/math?math=\mu"> and covariance matrix of the form:

<img src="https://render.githubusercontent.com/render/math?math=\Sigma = LL^T %2B \Psi">

The maximum likelihood estimator for the mean vector <img src="https://render.githubusercontent.com/render/math?math=\mu">, the factor loading matrix <img src="https://render.githubusercontent.com/render/math?math=L"> and the specific variance <img src="https://render.githubusercontent.com/render/math?math=\Psi"> are obtained by finding <img src="https://render.githubusercontent.com/render/math?math=\hat{\mu}">, <img src="https://render.githubusercontent.com/render/math?math=\hat{L}"> and <img src="https://render.githubusercontent.com/render/math?math=\hat{\Psi}"> that maximize the maximize the log likelihood given by:

<img src="https://render.githubusercontent.com/render/math?math=l(\mu,L,\Psi) = -\dfrac{np}{2}\log 2\pi -\dfrac{n}{2}\log|LL^T%2B \Psi| - \dfrac{1}{2}{\displaystyle \sum_{i=1}^n(X_i-\mu)^T(LL^T%2B \Psi)^{-1}(X_i-\mu)} ">

which can be obtained using the multivariate normal distribution given in [this](https://c-huang-tty.github.io/posts/2021/01/07/multivariate-normal-distributions/) post.

---
## Goodness of Fit
To assess goodness-of-fit, we use the Bartlett-Corrected Likelihood Ratio Test Statistic ([link](https://online.stat.psu.edu/stat505/lesson/12/12.9)):

<img src="https://render.githubusercontent.com/render/math?math=X^2 = \left(n-1-\dfrac{2p%2B 4m -5}{6}\right)\log\dfrac{\left|\hat{L}\hat{L}^T%2B \Psi\right|}{|\hat{\Sigma}|}">

In the numerator we have the determinant of the fitted factor model for the variance-covariance matrix, and below, we have a sample estimate of the variance-covariance matrix assuming no structure where:

<img src="https://render.githubusercontent.com/render/math?math=\hat{\Sigma} = \dfrac{n-1}{n}S">

and <img src="https://render.githubusercontent.com/render/math?math=S"> is the sample covariance matrix.

If the factor model fits well then these two determinants should be about the same and you will get a small value for <img src="https://render.githubusercontent.com/render/math?math=X^2">. Otherwise, <img src="https://render.githubusercontent.com/render/math?math=X^2"> would be large.

Under the null hypothesis that the factor model adequately describes the relationships among the variables, there is

<img src="https://render.githubusercontent.com/render/math?math=X^2\sim \chi^2((p^2-p-2pm)\text{/}2)">

where the degrees of freedom are the difference of unique parameters in the two models.

---
## Factor Rotations
The problem with the analysis above is that some of the variables are highlighted by more than one factors, and some even indicate contradictory results. This does not provide a very clean, simple interpretation of the data. __Ideally, each variable would appear as a significant contributer in one column (factor)__. This is the purpose of factor rotations.

Factor rotation is motivated by the fact that factor models are not unique. To see this, let <img src="https://render.githubusercontent.com/render/math?math=T"> be any <img src="https://render.githubusercontent.com/render/math?math=m\times m"> orthogonal matrix. We can write the factor model as:

<img src="https://render.githubusercontent.com/render/math?math=X=\mu %2B Lf %2B \epsilon =\mu %2B LTT^Tf %2B \epsilon = \mu %2B L^*f^* %2B \epsilon\ \ (L^*=LT, f^*=T^Tf)">

We can verify that the assumptions still hold under the trasform. 

### Varimax Rotation
Varimax rotation finds the rotation that maximizes the sum of the variances of the squared loadings, namely, find <img src="https://render.githubusercontent.com/render/math?math=T">, such that  

<img src="https://render.githubusercontent.com/render/math?math=V = {\displaystyle \dfrac{1}{p} \sum_{j=1}^m\left\{\sum_{i=1}^p(\tilde{l}_{ij}^*)^4 - \dfrac{1}{p}\left(\sum_{i=1}^p(\tilde{l}_{ij}^*)^2\right)^2\right\}}">

is maximized, where 

<img src="https://render.githubusercontent.com/render/math?math=\tilde{l}_{ij}^* = \dfrac{\hat{l}_{ij}^*}{\hat{h}_i}">

and 

<img src="https://render.githubusercontent.com/render/math?math=\hat{h}_i^2 = {\displaystyle \sum_{j=1}^m\hat{l}_{ij}^2}">

---
## Estimation of Factor Scores
Given the factor model:

<img src="https://render.githubusercontent.com/render/math?math=Y_i = \mu %2B Lf_i %2B \epsilon_i, i=1,2,...,n">

estimate the vectors of factor scores

<img src="https://render.githubusercontent.com/render/math?math=f_1,f_2,...,f_n">

for each observation.

### Ordinary Least Squares (OLS)
The factor scores is found by minimizing the sum of the residual squares:

<img src="https://render.githubusercontent.com/render/math?math=\epsilon_i^T\epsilon_i = (Y_i-\mu-Lf_i)^T(Y_i-\mu-Lf_i)">

Thus, 

<img src="https://render.githubusercontent.com/render/math?math=\hat{f}_i = (L^TL)^{-1}L^T(Y_i-\mu)">

### Weightead Least Squares (WLS)
The difference between WLS and OLS is that the squared residuals are divided by the specific variances <img src="https://render.githubusercontent.com/render/math?math=\psi">. This is going to give more weight, in this estimation, to variables that have low specific variances.  

The factor scores is found by minimizing the sum of the residual squares:

<img src="https://render.githubusercontent.com/render/math?math=\epsilon_i^T\Psi^{-1}\epsilon_i = (Y_i-\mu-Lf_i)^T\Psi^{-1}(Y_i-\mu-Lf_i)">

Thus, 

<img src="https://render.githubusercontent.com/render/math?math=\hat{f}_i = (L^TL)^{-1}L^T\Psi^{-1}(Y_i-\mu)">

---
## Example 
### Data
[Here](https://towardsdatascience.com/factor-analysis-a-complete-tutorial-1b7621890e42) is an example using package `factor_analyzer`. Let repeat the example using `statsmodel`. 

The dataset contains an airline passenger satisfaction survey, which has 25976 observations and 25 columns. 
```
       Unnamed: 0     id  ... Arrival Delay in Minutes             satisfaction
0               0  19556  ...                     44.0                satisfied
1               1  90035  ...                      0.0                satisfied
2               2  12360  ...                      0.0  neutral or dissatisfied
3               3  77959  ...                      6.0                satisfied
4               4  36875  ...                     20.0                satisfied
          ...    ...  ...                      ...                      ...
25971       25971  78463  ...                      0.0  neutral or dissatisfied
25972       25972  71167  ...                      0.0                satisfied
25973       25973  37675  ...                      0.0  neutral or dissatisfied
25974       25974  90086  ...                      0.0                satisfied
25975       25975  34799  ...                      0.0  neutral or dissatisfied

[25976 rows x 25 columns]
```
Here is a list of the 25 columns, where the columns 8 to 21 represent customers response on a scale of 1 to 5 to a survey evaluating different aspects of the flights. 
```
['Unnamed: 0',
 'id',
 'Gender',
 'Customer Type',
 'Age',
 'Type of Travel',
 'Class',
 'Flight Distance',
 'Inflight wifi service',
 'Departure/Arrival time convenient',
 'Ease of Online booking',
 'Gate location',
 'Food and drink',
 'Online boarding',
 'Seat comfort',
 'Inflight entertainment',
 'On-board service',
 'Leg room service',
 'Baggage handling',
 'Checkin service',
 'Inflight service',
 'Cleanliness',
 'Departure Delay in Minutes',
 'Arrival Delay in Minutes',
 'satisfaction']
 ```
The first step of any factor analysis is to look at a correlation plot of all the variables to see if any variables are useless or too correlated with others.
```python
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

data = pd.read_csv('airline.csv',sep=',')

c = data.corr()
ax = sns.heatmap(c,square=True)
ax.figure.subplots_adjust(bottom = 0.25)
```
<img src="/images/statistics/FA3.png" alt="drawing" width="800"/>

As "Departure Delay in Minutes" and the "Arrival Delay in Minutes" are highly correlated, remove one of the columns from the dataset.

```python
data.drop(['Arrival Delay in Minutes'], axis=1, inplace=True)
```

### Factor Analysis
```python
class statsmodels.multivariate.factor.Factor(endog=None, n_factor=1, corr=None, method='pa', smc=True, endog_names=None, nobs=None, missing='drop')

Parameters
  endog: array_like
     Variables in columns, observations in rows. May be None if corr is not None.

  n_factor: int
    The number of factors to extract
    
  method: str
    The method to extract factors, currently must be either ‘pa’ for principal axis factor analysis or ‘ml’ for maximum likelihood estimation.
```
```python
# 14 columns of the customers's responses to the survey
x =data[data.columns[8:22]] 
# set the number of factor as 3
results = sm.Factor(x,3).fit()
```
Eigenvalues:
```python
results.eigenvals

Out[116]: 
array([ 3.39287070e+00,  1.99752234e+00,  1.74684264e+00,  4.03928420e-01,
        2.25980639e-01,  4.79457525e-02,  1.14410507e-02, -1.96191104e-03,
       -5.44379913e-03, -2.43802755e-02, -4.67822926e-02, -1.43716548e-01,
       -2.23380565e-01, -2.43630473e-01])
```
<img src="/images/statistics/FA4.png" alt="drawing" width="500"/>

Communality：
```python
results.communality

Out[124]: 
array([0.62136998, 0.26925258, 0.85027123, 0.27214144, 0.57158413,
       0.2980277 , 0.63998574, 0.78040315, 0.50684261, 0.25212025,
       0.5932482 , 0.10115273, 0.6385212 , 0.74231474])
```
<img src="/images/statistics/FA5.png" alt="drawing" width="500"/>

Unrotated loadings:
```python
results.loadings

Out[123]: 
array([[ 0.41623784, -0.65850723,  0.12035065],
       [ 0.14214463, -0.4986264 ,  0.0204743 ],
       [ 0.30185314, -0.8578344 ,  0.15256493],
       [ 0.0965834 , -0.50311275,  0.09844108],
       [ 0.57728164,  0.25446329,  0.41662749],
       [ 0.46516202, -0.21285358,  0.19064456],
       [ 0.65348712,  0.24435365,  0.3914481 ],
       [ 0.84415447,  0.25585916,  0.04839901],
       [ 0.48650244,  0.02597749, -0.51911767],
       [ 0.36742193, -0.03771079, -0.34014596],
       [ 0.48111541,  0.01022714, -0.60139137],
       [ 0.27483868,  0.03324022, -0.15656155],
       [ 0.49262959,  0.0194701 , -0.62885468],
       [ 0.69879915,  0.29553957,  0.40822891]])
``` 

<img src="/images/statistics/FA9.png" alt="drawing" width="500"/>
<img src="/images/statistics/FA10.png" alt="drawing" width="500"/>
<img src="/images/statistics/FA11.png" alt="drawing" width="500"/>

Rotated loadings:
```python
results.rotate('varimax')
results.loadings

Out[132]: 
array([[ 0.14787756, -0.76735192, -0.10331135],
       [-0.05930589, -0.51341631, -0.04625023],
       [ 0.001848  , -0.92203905, -0.01057408],
       [-0.05756442, -0.51660793,  0.04409107],
       [ 0.75467628, -0.02013316,  0.04052768],
       [ 0.38346871, -0.38016993, -0.08031359],
       [ 0.79818003, -0.04952743, -0.02100997],
       [ 0.77983455, -0.04002052, -0.41310965],
       [ 0.13074484, -0.04093086, -0.69862226],
       [ 0.103968  , -0.09334843, -0.48228309],
       [ 0.07949962, -0.03954792, -0.76509083],
       [ 0.1492195 , -0.02964238, -0.27929841],
       [ 0.07811092, -0.02983718, -0.79468839],
       [ 0.86076469, -0.0192643 , -0.03205879]])
```
The higher a factor loading, the more important a variable is for said factor. Usually, a loading cutoff of 0.5 is used. This cutoff decides which variables belongs to which factor. Hence, 
- The first factor caontains variables 5, 7, 8, 14
- The second factor contains variables 1, 2, 3, 4
- The third factor contains variables 9, 11, 13

<img src="/images/statistics/FA6.png" alt="drawing" width="500"/>
<img src="/images/statistics/FA7.png" alt="drawing" width="500"/>
<img src="/images/statistics/FA8.png" alt="drawing" width="500"/>

Recall that the 14 variables considered are:
```
['Inflight wifi service',
 'Departure/Arrival time convenient',
 'Ease of Online booking',
 'Gate location',
 'Food and drink',
 'Online boarding',
 'Seat comfort',
 'Inflight entertainment',
 'On-board service',
 'Leg room service',
 'Baggage handling',
 'Checkin service',
 'Inflight service',
 'Cleanliness']
 ```
Therefore, the three factors can be explained as:
- __Comfort__: Food and Drink, Seat comfort, Inflight entertainment, Cleanliness
- __Convenience__: In flight Wifi, Departure/Arrival time convenience, Online Booking, Gate Location.
- __Service__: Onboard service, Baggage Handling, Inflight Service

### Factor Scores
Factor scoring coefficient matrix:
```python
results.factor_score_params()

Out[139]: 
array([[ 0.01729959, -0.24417132, -0.01322147],
       [-0.01900684, -0.08682621, -0.00894957],
       [-0.0535145 , -0.76064598,  0.05825796],
       [-0.0126019 , -0.0891501 ,  0.01693065],
       [ 0.22051131,  0.00780611,  0.10849854],
       [ 0.05676871, -0.06106693,  0.01001721],
       [ 0.26798272,  0.0022086 ,  0.10067651],
       [ 0.33992157,  0.02923623, -0.20999439],
       [-0.03919222,  0.00858151, -0.27956411],
       [-0.01631891, -0.00670421, -0.12531997],
       [-0.07114288,  0.01168981, -0.37920286],
       [ 0.00451548,  0.0009571 , -0.05573909],
       [-0.08440538,  0.01740522, -0.44418667],
       [ 0.403036  ,  0.02014393,  0.14256182]])
```
Factor scores:
```python
results.factor_scoring()

Out[140]: 
array([[ 0.5894652 , -0.61037357, -1.52316734],
       [ 1.26600869,  0.46717871, -0.06557448],
       [-1.07729015,  0.62376949,  0.83407618],
       ...,
       [-1.3706222 ,  0.88601783, -0.62703381],
       [ 0.50028647, -0.14472913, -0.67078162],
       [-1.10915367,  0.18519492,  2.65380433]])
```

---
---
# Principle Component Analysis


---
## Table of Contents
- [Probability vs Statistics](https://c-huang-tty.github.io/posts/2021/01/01/probability-and-statistics/)
- [Shakespear's New Poem](https://c-huang-tty.github.io/posts/2021/01/02/application-of-statistics/)
- [Some Common Discrete Distributions](https://c-huang-tty.github.io/posts/2021/01/03/some-common-discrete-distributions/)
- [Some Common Continuous Distributions](https://c-huang-tty.github.io/posts/2021/01/04/some-common-continuous-distributions/)
- [Statistical Quantities](https://c-huang-tty.github.io/posts/2021/01/05/statistical-quantities/)
- [Order Statistics](https://c-huang-tty.github.io/posts/2021/01/06/order-statistics/)
- [Multivariate Normal Distributions](https://c-huang-tty.github.io/posts/2021/01/07/multivariate-normal-distributions/)
- [Conditional Distributions and Expectation](https://c-huang-tty.github.io/posts/2021/01/08/conditonal-distributions-and-expectation/)
- [Problem Set [01] - Probabilities](https://c-huang-tty.github.io/posts/2021/01/21/problem-set-probabilities/)
- [Parameter Point Estimation](https://c-huang-tty.github.io/posts/2021/01/09/parameter-point-estimation/)
- [Evaluation of Point Estimation](https://c-huang-tty.github.io/posts/2021/01/10/evaluation-point-estimation/)
- [Parameter Interval Estimation](https://c-huang-tty.github.io/posts/2021/01/11/parameter-interval-estimation/)
- [Problem Set [02] - Parameter Estimation](https://c-huang-tty.github.io/posts/2021/01/22/problem-set-parameter-estimation/)
- [Parameter Hypothesis Test](https://c-huang-tty.github.io/posts/2021/01/12/parameter-hypothesis-test/)
- [t Test](https://c-huang-tty.github.io/posts/2021/01/13/t-test/)
- [Chi-Squared Test](https://c-huang-tty.github.io/posts/2021/01/14/chi-squared-test/)
- [Analysis of Variance](https://c-huang-tty.github.io/posts/2021/01/15/analysis-of-variance/)
- [Summary of Statistical Tests](https://c-huang-tty.github.io/posts/2021/01/16/summary-of-statistical-tests/)
- [Python [01] - Data Representation](https://c-huang-tty.github.io/posts/2021/01/17/statistics-python-data-representation/)
- [Python [02] - t Test & F Test](https://c-huang-tty.github.io/posts/2021/01/18/statistics-python-t-F-test/)
- [Python [03] - Chi-Squared Test](https://c-huang-tty.github.io/posts/2021/01/19/statistics-chi-squared-test/)
- [Experimental Design](https://c-huang-tty.github.io/posts/2021/01/20/experimental-design/)
- [Monte Carlo](https://c-huang-tty.github.io/posts/2021/01/23/monte-carlo/)
- [Variance Reducing Techniques](https://c-huang-tty.github.io/posts/2021/01/24/variance-reducing-techniques/)
- [From Uniform to General Distributions](https://c-huang-tty.github.io/posts/2021/01/25/from-uniform-to-general-distributions/)
- [Problem Set [03] - Monte Carlo](https://c-huang-tty.github.io/posts/2021/01/26/problem-set-monte-carlo/)
- [Unitary Regression Model](https://c-huang-tty.github.io/posts/2021/01/27/unitary-regression-model/)
- [Multiple Regression Model](https://c-huang-tty.github.io/posts/2021/01/28/multiple-regression-model/)
- [Factor and Principle Component Analysis](https://c-huang-tty.github.io/posts/2021/01/29/factor-principle-component-analysis/)
- [Clustering Analysis](https://c-huang-tty.github.io/posts/2021/01/30/clustering-analysis/)
- [Summary](https://c-huang-tty.github.io/posts/2021/01/31/summary/)

