---
title: 'Statistics [10]: Evaluation of Point Estimation'
date: 2021-01-10
permalink: /posts/2021/01/10/evaluation-point-estimation/
tags:
  - Statistics
---

Properties of point estimation, mean squre error and minimum variance unbiased estimation.

---
## Properties of Point Estimation
### Consistency
Assume <img src="https://render.githubusercontent.com/render/math?math=\theta\in\Theta"> is an unknown parameter, <img src="https://render.githubusercontent.com/render/math?math=\hat{\theta}_n=\hat{\theta}_n(x_1,x_2,...,x_n)"> is an estimation obtained from <img src="https://render.githubusercontent.com/render/math?math=n"> samples. If for any <img src="https://render.githubusercontent.com/render/math?math=\varepsilon > 0">, there exists <img src="https://render.githubusercontent.com/render/math?math=n"> that satisfies <img src="https://render.githubusercontent.com/render/math?math={\displaystyle \lim_{n\to\infty}P(|\hat{\theta}-\theta| > \varepsilon) = 0}">. Then <img src="https://render.githubusercontent.com/render/math?math=\hat{\theta}"> is called a consistent estimation of <img src="https://render.githubusercontent.com/render/math?math=\theta">.

### Bias
Assume <img src="https://render.githubusercontent.com/render/math?math=\theta\in\Theta"> is an unknown parameter, <img src="https://render.githubusercontent.com/render/math?math=\hat{\theta}_n=\hat{\theta}_n(x_1,x_2,...,x_n)"> is an estimation obtained from <img src="https://render.githubusercontent.com/render/math?math=n"> samples. If for any <img src="https://render.githubusercontent.com/render/math?math=\theta\in\Theta"> there is <img src="https://render.githubusercontent.com/render/math?math=E(\hat{\theta})=\theta">. Then <img src="https://render.githubusercontent.com/render/math?math=\hat{\theta}"> is called an unbiased estimation of <img src="https://render.githubusercontent.com/render/math?math=\theta">.

### Efficiency
Assume <img src="https://render.githubusercontent.com/render/math?math=\hat{\theta}_1"> and <img src="https://render.githubusercontent.com/render/math?math=\hat{\theta}_2"> are two unbiased estimation. if for any <img src="https://render.githubusercontent.com/render/math?math=\theta\in\Theta"> there is <img src="https://render.githubusercontent.com/render/math?math=var(\hat{\theta}_1) \leq var(\hat{\theta}_2)">, and there exists at least one <img src="https://render.githubusercontent.com/render/math?math=\theta\in\Theta"> so that <img src="https://render.githubusercontent.com/render/math?math=var(\hat{\theta}_1) < var(\hat{\theta}_2)">. Then <img src="https://render.githubusercontent.com/render/math?math=var(\hat{\theta}_1)"> is more efficient than <img src="https://render.githubusercontent.com/render/math?math=var(\hat{\theta}_2)">.

---
### Example 1
Assume <img src="https://render.githubusercontent.com/render/math?math=X_1,X_2,...,X_n"> are samples from uniform distribution <img src="https://render.githubusercontent.com/render/math?math=U(0,\theta)">, the moment estimation is <img src="https://render.githubusercontent.com/render/math?math=\hat{\theta}=2\bar{X}"> and the maximum likelihood estimation is <img src="https://render.githubusercontent.com/render/math?math=\tilde{\theta}={\displaystyle \max_{1\leq k\leq n} X_k}">, consider the unbiasedness of <img src="https://render.githubusercontent.com/render/math?math=\hat{\theta}"> and <img src="https://render.githubusercontent.com/render/math?math=\tilde{\theta}">.

__Solution__. Fisrt, <img src="https://render.githubusercontent.com/render/math?math=E(\hat{\theta}) = E(2\bar{X}) = 2\times\dfrac{\theta}{2} \theta">, hence, <img src="https://render.githubusercontent.com/render/math?math=\hat{\theta}"> is an unbiased estimation.

As for <img src="https://render.githubusercontent.com/render/math?math=\tilde{\theta}">, when <img src="https://render.githubusercontent.com/render/math?math=0\leq y \leq \theta">, the distribution function of <img src="https://render.githubusercontent.com/render/math?math=y"> is 

<img src="https://render.githubusercontent.com/render/math?math=F_{\tilde{\theta}}(y) = P(\tilde{\theta}\leq y) = {\displaystyle P( \max_{1\leq k\leq n} X_k\leq y) = \prod_{k=1}^n P(X_k\leq y) = \left(\dfrac{y}{\theta}\right)^n}">

Then, the probability function would be

<img src="https://render.githubusercontent.com/render/math?math=f_{\tilde{\theta}}(y) = {\displaystyle \dfrac{n}{\theta^n}y^{n-1}}">

Hence, 

<img src="https://render.githubusercontent.com/render/math?math=E(\tilde{\theta}) = {\displaystyle \int_{-\infty}^\infty yf_{\tilde{\theta}}(y)dy = \int_{0}^yy\dfrac{n}{\theta^n}y^{n-1}dy} = \dfrac{n}{n%2B1}\theta">

Hence, <img src="https://render.githubusercontent.com/render/math?math=\tilde{\theta}"> is a biased estimation.

### Example 2
Assume <img src="https://render.githubusercontent.com/render/math?math=X_1,X_2,...,X_n"> are samples from uniform distribution <img src="https://render.githubusercontent.com/render/math?math=U(0,\theta)">, <img src="https://render.githubusercontent.com/render/math?math=\hat{\theta}_1=2\bar{X}"> and <img src="https://render.githubusercontent.com/render/math?math=\hat{\theta}_2={\displaystyle \dfrac{n%2B1}{n} \max_{1\leq k\leq n} X_k}">, compare their efficiency.

__Solution__. For <img src="https://render.githubusercontent.com/render/math?math=\hat{\theta}_1">, we have

<img src="https://render.githubusercontent.com/render/math?math=var(\hat{\theta}_1) = var(2\bar{X}) = 4var(\bar{X}) = 4\cdot\dfrac{\theta^2}{12n} = \dfrac{\theta^2}{3n}">

For <img src="https://render.githubusercontent.com/render/math?math=\hat{\theta}_2">, firstly, from example 1, we have

<img src="https://render.githubusercontent.com/render/math?math=E(\tilde{\theta}^2) = {\displaystyle \int_{0}^{\theta}y^2f_{\tilde{\theta}}(y)dy = \int_{0}^\theta y^2\dfrac{n}{\theta^n}y^{n-1}dy = \dfrac{n}{n%2B2}\theta^2}">

Then, we have

<img src="https://render.githubusercontent.com/render/math?math=var(\tilde{\theta}) = E(\tilde{\theta}^2) - E^2(\tilde{\theta}) = \dfrac{n}{n%2B2}\theta^2-\dfrac{n^2}{(n%2B 1)^2\theta^2} = \dfrac{n}{(n%2B1)^2(n%2B 2)}\theta^2">

Therefore, 

<img src="https://render.githubusercontent.com/render/math?math=var(\hat{\theta}_2) = \dfrac{(n%2B1)^2}{n^2}var(\tilde{\theta}) = \dfrac{1}{n(n%2B2)}\theta^2">

---
## Mean Square Error (MSE)

<img src="https://render.githubusercontent.com/render/math?math=E((\hat{\theta}-\theta)^2) = E\left([\hat{\theta} - E(\hat{\theta})] %2B [E(\hat{\theta}) - \theta]^2\right) = var(\hat{\theta}) %2B (E(\hat{\theta}) - \theta)^2">

### Example 3
Assume <img src="https://render.githubusercontent.com/render/math?math=X1, X_2, ..., X_n"> are samples from <img src="https://render.githubusercontent.com/render/math?math=N(\mu,\sigma^2)">, calculate MSE of moment estimation and maximum likelihood estimation of <img src="https://render.githubusercontent.com/render/math?math=\mu"> and <img src="https://render.githubusercontent.com/render/math?math=\sigma^2">.

__Solution__. Fisrtly, moment estimation of <img src="https://render.githubusercontent.com/render/math?math=\mu"> and <img src="https://render.githubusercontent.com/render/math?math=\sigma^2"> are <img src="https://render.githubusercontent.com/render/math?math=\bar{\mu}=\bar{X}"> and <img src="https://render.githubusercontent.com/render/math?math=\bar{\sigma}^2=S^2">, and maximum likelihood estimation of <img src="https://render.githubusercontent.com/render/math?math=\mu"> and <img src="https://render.githubusercontent.com/render/math?math=\sigma^2"> are <img src="https://render.githubusercontent.com/render/math?math=\hat{\mu} = \bar{X}"> and <img src="https://render.githubusercontent.com/render/math?math=\hat{\sigma}^2={\displaystyle \dfrac{1}{n}\sum_{i=1}^n(X_i-\bar{X})^2}">. 

For <img src="https://render.githubusercontent.com/render/math?math=\bar{\mu}"> and <img src="https://render.githubusercontent.com/render/math?math=\bar{\sigma}^2">, 

<img src="https://render.githubusercontent.com/render/math?math=E((\bar{\mu} - \mu)^2) = E((\bar{X} - \mu)^2) = var(\bar{X}) = \dfrac{\sigma^2}{n}">

<img src="https://render.githubusercontent.com/render/math?math=E((\bar{\sigma}^2 - \sigma^2)^2) = E((S^2 - \sigma^2)^2) = var(S^2)">

To obtain <img src="https://render.githubusercontent.com/render/math?math=var(S^2)">, we can use the fact that <img src="https://render.githubusercontent.com/render/math?math=\dfrac{(n-1)s^2}{\sigma^2}\sim \chi^2(n-1)"> and <img src="https://render.githubusercontent.com/render/math?math=var(\chi^2(n-1))=2(n-1)">, so that 

<img src="https://render.githubusercontent.com/render/math?math=var(S^2) = \dfrac{\sigma^4}{(n-1)^2}\cdot 2(n-1) = \dfrac{2\sigma^4}{n-1}">

For <img src="https://render.githubusercontent.com/render/math?math=\hat{\mu}"> and <img src="https://render.githubusercontent.com/render/math?math=\hat{\sigma}^2=\dfrac{n-1}{n}S^2">,

<img src="https://render.githubusercontent.com/render/math?math=E((\hat{\mu} - \mu)^2) = E((\bar{X} - \mu)^2) = var(\bar{X}) = \dfrac{\sigma^2}{n}">

<img src="https://render.githubusercontent.com/render/math?math=E((\hat{\sigma}^2 - \sigma^2)^2) = E((\dfrac{n-1}{n}S^2 - \sigma^2)^2) = E\left(\left(\dfrac{n-1}{n}(S^2-\sigma^2)-\dfrac{1}{n}\sigma^2\right)\right)">

<img src="https://render.githubusercontent.com/render/math?math==\left(\dfrac{n-1}{n}\right)^2E\left((S^2-\sigma^2)^2\right) - 2E\left(\dfrac{n-1}{n}(S^2-\sigma^2)\cdot \dfrac{1}{n}\sigma^2\right) %2B \dfrac{1}{n^2}\sigma^4">

<img src="https://render.githubusercontent.com/render/math?math==\left(\dfrac{n-1}{n}\right)^2 var(S^2) %2B \dfrac{1}{n^2}\sigma^4 = \left(\dfrac{n-1}{n}\right)^2 \dfrac{2\sigma^4}{n-1} %2B \dfrac{1}{n^2}\sigma^4 = \dfrac{2n-1}{n^2}\sigma^4 ">

Therefore,

<img src="https://render.githubusercontent.com/render/math?math=E((\hat{\sigma}^2 - \sigma^2)^2) = \dfrac{2n-1}{n^2}\sigma^4 \leq \dfrac{2\sigma^4}{n-1} = E((\bar{\sigma}^2 - \sigma^2)^2)">

---
## Minimum Variance Unbiased (MVU)

Assume <img src="https://render.githubusercontent.com/render/math?math=\hat{\theta}"> is an unbiased estimation of <img src="https://render.githubusercontent.com/render/math?math=g(\theta)">, if for any unbiased estimation <img src="https://render.githubusercontent.com/render/math?math=\hat{\theta}_1">, <img src="https://render.githubusercontent.com/render/math?math=var_{\theta}(\hat{\theta})\leq var_{\theta}(\hat{\theta}_1)"> holds for any <img src="https://render.githubusercontent.com/render/math?math=\theta">, then <img src="https://render.githubusercontent.com/render/math?math=\hat{\theta}"> is called __minimum variance unbiased__ estimation of <img src="https://render.githubusercontent.com/render/math?math=g(\theta)">. 
