---
title: 'Statistics [10]: Evaluation of Point Estimation'
date: 2021-01-10
permalink: /posts/2021/01/10/evaluation-point-estimation/
tags:
  - Statistics
---

Properties of point estimation, mean squre error and minimum variance unbiased estimation.

---
## Properties of Point Estimation
### Consistency
Assume <img src="https://render.githubusercontent.com/render/math?math=\theta\in\Theta"> is an unknown parameter, <img src="https://render.githubusercontent.com/render/math?math=\hat{\theta}_n=\hat{\theta}_n(x_1,x_2,...,x_n)"> is an estimation obtained from <img src="https://render.githubusercontent.com/render/math?math=n"> samples. If for any <img src="https://render.githubusercontent.com/render/math?math=\varepsilon > 0">, there exists <img src="https://render.githubusercontent.com/render/math?math=n"> that satisfies <img src="https://render.githubusercontent.com/render/math?math={\displaystyle \lim_{n\to\infty}P(|\hat{\theta}-\theta| > \varepsilon) = 0}">. Then <img src="https://render.githubusercontent.com/render/math?math=\hat{\theta}"> is called a consistent estimation of <img src="https://render.githubusercontent.com/render/math?math=\theta">.

### Bias
Assume <img src="https://render.githubusercontent.com/render/math?math=\theta\in\Theta"> is an unknown parameter, <img src="https://render.githubusercontent.com/render/math?math=\hat{\theta}_n=\hat{\theta}_n(x_1,x_2,...,x_n)"> is an estimation obtained from <img src="https://render.githubusercontent.com/render/math?math=n"> samples. If for any <img src="https://render.githubusercontent.com/render/math?math=\theta\in\Theta"> there is <img src="https://render.githubusercontent.com/render/math?math=E(\hat{\theta})=\theta">. Then <img src="https://render.githubusercontent.com/render/math?math=\hat{\theta}"> is called an unbiased estimation of <img src="https://render.githubusercontent.com/render/math?math=\theta">.

### Efficiency
Assume <img src="https://render.githubusercontent.com/render/math?math=\hat{\theta}_1"> and <img src="https://render.githubusercontent.com/render/math?math=\hat{\theta}_2"> are two unbiased estimation. if for any <img src="https://render.githubusercontent.com/render/math?math=\theta\in\Theta"> there is <img src="https://render.githubusercontent.com/render/math?math=var(\hat{\theta}_1) \leq var(\hat{\theta}_2)">, and there exists at least one <img src="https://render.githubusercontent.com/render/math?math=\theta\in\Theta"> so that <img src="https://render.githubusercontent.com/render/math?math=var(\hat{\theta}_1) < var(\hat{\theta}_2)">. Then <img src="https://render.githubusercontent.com/render/math?math=var(\hat{\theta}_1)"> is more efficient than <img src="https://render.githubusercontent.com/render/math?math=var(\hat{\theta}_2)">.

---
### Example 1
Assume <img src="https://render.githubusercontent.com/render/math?math=X_1,X_2,...,X_n"> are samples from uniform distribution <img src="https://render.githubusercontent.com/render/math?math=U(0,\theta)">, the moment estimation is <img src="https://render.githubusercontent.com/render/math?math=\hat{\theta}=2\bar{X}"> and the maximum likelihood estimation is <img src="https://render.githubusercontent.com/render/math?math=\tilde{\theta}={\displaystyle \max_{1\leq k\leq n} X_k}">, consider the unbiasedness of <img src="https://render.githubusercontent.com/render/math?math=\hat{\theta}"> and <img src="https://render.githubusercontent.com/render/math?math=\tilde{\theta}">.

__Solution__. Fisrt, <img src="https://render.githubusercontent.com/render/math?math=E(\hat{\theta}) = E(2\bar{X}) = 2\times\dfrac{\theta}{2} = \theta">, hence, <img src="https://render.githubusercontent.com/render/math?math=\hat{\theta}"> is an unbiased estimation.

As for <img src="https://render.githubusercontent.com/render/math?math=\tilde{\theta}">, when <img src="https://render.githubusercontent.com/render/math?math=0\leq y \leq \theta">, the distribution function of <img src="https://render.githubusercontent.com/render/math?math=y"> is 

<img src="https://render.githubusercontent.com/render/math?math=F_{\tilde{\theta}}(y) = P(\tilde{\theta}\leq y) = {\displaystyle P( \max_{1\leq k\leq n} X_k\leq y) = \prod_{k=1}^n P(X_k\leq y) = \left(\dfrac{y}{\theta}\right)^n}">

Then, the probability function would be

<img src="https://render.githubusercontent.com/render/math?math=f_{\tilde{\theta}}(y) = {\displaystyle \dfrac{n}{\theta^n}y^{n-1}}">

Hence, 

<img src="https://render.githubusercontent.com/render/math?math=E(\tilde{\theta}) = {\displaystyle \int_{-\infty}^\infty yf_{\tilde{\theta}}(y)dy = \int_{0}^yy\dfrac{n}{\theta^n}y^{n-1}dy} = \dfrac{n}{n%2B1}\theta">

Hence, <img src="https://render.githubusercontent.com/render/math?math=\tilde{\theta}"> is a biased estimation.

### Example 2
Assume <img src="https://render.githubusercontent.com/render/math?math=X_1,X_2,...,X_n"> are samples from uniform distribution <img src="https://render.githubusercontent.com/render/math?math=U(0,\theta)">, <img src="https://render.githubusercontent.com/render/math?math=\hat{\theta}_1=2\bar{X}"> and <img src="https://render.githubusercontent.com/render/math?math=\hat{\theta}_2={\displaystyle \dfrac{n%2B1}{n} \max_{1\leq k\leq n} X_k}">, compare their efficiency.

__Solution__. For <img src="https://render.githubusercontent.com/render/math?math=\hat{\theta}_1">, we have

<img src="https://render.githubusercontent.com/render/math?math=var(\hat{\theta}_1) = var(2\bar{X}) = 4var(\bar{X}) = 4\cdot\dfrac{\theta^2}{12n} = \dfrac{\theta^2}{3n}">

For <img src="https://render.githubusercontent.com/render/math?math=\hat{\theta}_2">, firstly, from example 1, we have

<img src="https://render.githubusercontent.com/render/math?math=E(\tilde{\theta}^2) = {\displaystyle \int_{0}^{\theta}y^2f_{\tilde{\theta}}(y)dy = \int_{0}^\theta y^2\dfrac{n}{\theta^n}y^{n-1}dy = \dfrac{n}{n%2B2}\theta^2}">

Then, we have

<img src="https://render.githubusercontent.com/render/math?math=var(\tilde{\theta}) = E(\tilde{\theta}^2) - E^2(\tilde{\theta}) = \dfrac{n}{n%2B2}\theta^2-\dfrac{n^2}{(n%2B 1)^2}\theta^2 = \dfrac{n}{(n%2B1)^2(n%2B 2)}\theta^2">

Therefore, 

<img src="https://render.githubusercontent.com/render/math?math=var(\hat{\theta}_2) = \dfrac{(n%2B1)^2}{n^2}var(\tilde{\theta}) = \dfrac{1}{n(n%2B2)}\theta^2">

---
## Mean Square Error (MSE)

<img src="https://render.githubusercontent.com/render/math?math=E((\hat{\theta}-\theta)^2) = E\left([\hat{\theta} - E(\hat{\theta})] %2B [E(\hat{\theta}) - \theta]^2\right) = var(\hat{\theta}) %2B (E(\hat{\theta}) - \theta)^2">

### Example 3
Assume <img src="https://render.githubusercontent.com/render/math?math=X1, X_2, ..., X_n"> are samples from <img src="https://render.githubusercontent.com/render/math?math=N(\mu,\sigma^2)">, calculate MSE of moment estimation and maximum likelihood estimation of <img src="https://render.githubusercontent.com/render/math?math=\mu"> and <img src="https://render.githubusercontent.com/render/math?math=\sigma^2">.

__Solution__. Fisrtly, moment estimation of <img src="https://render.githubusercontent.com/render/math?math=\mu"> and <img src="https://render.githubusercontent.com/render/math?math=\sigma^2"> are <img src="https://render.githubusercontent.com/render/math?math=\bar{\mu}=\bar{X}"> and <img src="https://render.githubusercontent.com/render/math?math=\bar{\sigma}^2=S^2">, and maximum likelihood estimation of <img src="https://render.githubusercontent.com/render/math?math=\mu"> and <img src="https://render.githubusercontent.com/render/math?math=\sigma^2"> are <img src="https://render.githubusercontent.com/render/math?math=\hat{\mu} = \bar{X}"> and <img src="https://render.githubusercontent.com/render/math?math=\hat{\sigma}^2={\displaystyle \dfrac{1}{n}\sum_{i=1}^n(X_i-\bar{X})^2}">. 

For <img src="https://render.githubusercontent.com/render/math?math=\bar{\mu}"> and <img src="https://render.githubusercontent.com/render/math?math=\bar{\sigma}^2">, 

<img src="https://render.githubusercontent.com/render/math?math=E((\bar{\mu} - \mu)^2) = E((\bar{X} - \mu)^2) = var(\bar{X}) = \dfrac{\sigma^2}{n}">

<img src="https://render.githubusercontent.com/render/math?math=E((\bar{\sigma}^2 - \sigma^2)^2) = E((S^2 - \sigma^2)^2) = var(S^2)">

To obtain <img src="https://render.githubusercontent.com/render/math?math=var(S^2)">, we can use the fact that <img src="https://render.githubusercontent.com/render/math?math=\dfrac{(n-1)s^2}{\sigma^2}\sim \chi^2(n-1)"> and <img src="https://render.githubusercontent.com/render/math?math=var(\chi^2(n-1))=2(n-1)">, so that 

<img src="https://render.githubusercontent.com/render/math?math=var(S^2) = \dfrac{\sigma^4}{(n-1)^2}\cdot 2(n-1) = \dfrac{2\sigma^4}{n-1}">

For <img src="https://render.githubusercontent.com/render/math?math=\hat{\mu}"> and <img src="https://render.githubusercontent.com/render/math?math=\hat{\sigma}^2=\dfrac{n-1}{n}S^2">,

<img src="https://render.githubusercontent.com/render/math?math=E((\hat{\mu} - \mu)^2) = E((\bar{X} - \mu)^2) = var(\bar{X}) = \dfrac{\sigma^2}{n}">

<img src="https://render.githubusercontent.com/render/math?math=E((\hat{\sigma}^2 - \sigma^2)^2) = E((\dfrac{n-1}{n}S^2 - \sigma^2)^2) = E\left(\left(\dfrac{n-1}{n}(S^2-\sigma^2)-\dfrac{1}{n}\sigma^2\right)\right)">

<img src="https://render.githubusercontent.com/render/math?math==\left(\dfrac{n-1}{n}\right)^2E\left((S^2-\sigma^2)^2\right) - 2E\left(\dfrac{n-1}{n}(S^2-\sigma^2)\cdot \dfrac{1}{n}\sigma^2\right) %2B \dfrac{1}{n^2}\sigma^4">

<img src="https://render.githubusercontent.com/render/math?math==\left(\dfrac{n-1}{n}\right)^2 var(S^2) %2B \dfrac{1}{n^2}\sigma^4 = \left(\dfrac{n-1}{n}\right)^2 \dfrac{2\sigma^4}{n-1} %2B \dfrac{1}{n^2}\sigma^4 = \dfrac{2n-1}{n^2}\sigma^4 ">

Therefore,

<img src="https://render.githubusercontent.com/render/math?math=E((\hat{\sigma}^2 - \sigma^2)^2) = \dfrac{2n-1}{n^2}\sigma^4 \leq \dfrac{2\sigma^4}{n-1} = E((\bar{\sigma}^2 - \sigma^2)^2)">

---
## Minimum Variance Unbiased (MVU)

Assume <img src="https://render.githubusercontent.com/render/math?math=\hat{\theta}"> is an unbiased estimation of <img src="https://render.githubusercontent.com/render/math?math=g(\theta)">, if for any unbiased estimation <img src="https://render.githubusercontent.com/render/math?math=\hat{\theta}_1">, <img src="https://render.githubusercontent.com/render/math?math=var_{\theta}(\hat{\theta})\leq var_{\theta}(\hat{\theta}_1)"> holds for any <img src="https://render.githubusercontent.com/render/math?math=\theta">, then <img src="https://render.githubusercontent.com/render/math?math=\hat{\theta}"> is called __minimum variance unbiased__ estimation of <img src="https://render.githubusercontent.com/render/math?math=g(\theta)">. 

### Cramer-Rao Inequality
Suppose <img src="https://render.githubusercontent.com/render/math?math=T = T(x_1,x_2,...,x_n)"> is an unbiased estimator of <img src="https://render.githubusercontent.com/render/math?math=g(\theta)">, the variance of any unbiased estimator is then bounded by

<img src="https://render.githubusercontent.com/render/math?math=var(T) \geq \dfrac{(g\text{'}(\theta))^2}{nI(\theta)}">

where 

<img src="https://render.githubusercontent.com/render/math?math=I(\theta) = E\left(\left(\dfrac{\partial}{\partial\theta}\ln p(x%3B\theta)\right)^2\right)">

is called __fisher information__, <img src="https://render.githubusercontent.com/render/math?math=n"> is the number of the samples and <img src="https://render.githubusercontent.com/render/math?math=p(x%3B\theta)"> is the probability density function.

__Proof__. Firstly, 

<img src="https://render.githubusercontent.com/render/math?math={\displaystyle \int_{-\infty}^\infty p(x_k %3B \theta)dx_k = 1 \Rightarrow \dfrac{\partial}{\partial\theta}\int_{-\infty}^\infty p(x_k %3B \theta)dx_k = \int_{-\infty}^\infty \dfrac{\partial}{\partial\theta} p(x_k %3B \theta)dx_k = 0}">

<img src="https://render.githubusercontent.com/render/math?math={\displaystyle \int_{-\infty}^\infty \dfrac{\partial}{\partial\theta} p(x_k %3B \theta)dx_k = \int_{-\infty}^\infty \left[\dfrac{\partial}{\partial\theta}\ln p(x_k %3B \theta)\right] p(x_k %3B \theta)dx_k =  E\left(\dfrac{\partial}{\partial\theta}\ln p(x_k %3B \theta)\right) = 0} ">

Denoting 

<img src="https://render.githubusercontent.com/render/math?math={\displaystyle  Z = \dfrac{\partial}{\partial\theta}\ln \left(\prod_{k=1}^{n}p(x_k %3B \theta)\right) = \sum_{k=1}^{n}\dfrac{\partial}{\partial\theta}\ln p(x_k %3B \theta)} ">

Then

<img src="https://render.githubusercontent.com/render/math?math={\displaystyle E(Z) =\sum_{k=1}^{n} E\left(\dfrac{\partial}{\partial\theta}\ln p(x_k %3B \theta)\right) = 0} ">

<img src="https://render.githubusercontent.com/render/math?math={\displaystyle E(Z^2) = var(Z) = \sum_{k=1}^{n} var\left(\dfrac{\partial}{\partial\theta}\ln p(x_k %3B \theta)\right) = \sum_{k=1}^{n} E\left(\left(\dfrac{\partial}{\partial\theta}\ln p(x_k %3B \theta)\right)^2\right)=nI(\theta)} ">

On the other hand,

<img src="https://render.githubusercontent.com/render/math?math={\displaystyle g(\theta) = \int_{-\infty}^\infty\cdots\int_{-\infty}^\infty T(x_1,x_2,...,x_n)\prod_{k=1}^{n}p(x_k%3B \theta)dx_1dx_2\cdots dx_n } ">

<img src="https://render.githubusercontent.com/render/math?math={\displaystyle g\text{'}(\theta) = \int_{-\infty}^\infty\cdots\int_{-\infty}^\infty T(x_1,x_2,...,x_n)\dfrac{\partial}{\partial \theta}\prod_{k=1}^{n}p(x_k%3B \theta)dx_1dx_2\cdots dx_n} ">

<img src="https://render.githubusercontent.com/render/math?math={\displaystyle = \int_{-\infty}^\infty\cdots\int_{-\infty}^\infty T(x_1,x_2,...,x_n)\left[\dfrac{\partial}{\partial \theta}\ln \left(\prod_{k=1}^{n}p(x_k%3B \theta)\right)\right]\prod_{k=1}^{n}p(x_k%3B \theta)dx_1dx_2\cdots dx_n} ">

<img src="https://render.githubusercontent.com/render/math?math={\displaystyle = E\left(T(x_1,x_2,...,x_n) \dfrac{\partial}{\partial \theta}\ln \left(\prod_{k=1}^{n}p(x_k%3B \theta)\right) \right) = E(T\cdot Z)} ">

<img src="https://render.githubusercontent.com/render/math?math={\displaystyle g\text{'}(\theta) = E(T\cdot Z) = E\left((T-g(\theta))\cdot Z \right)=cov(T-g(\theta), Z)} ">

<img src="https://render.githubusercontent.com/render/math?math={\displaystyle (g\text{'}(\theta))^2 = \left(cov(T-g(\theta), Z)\right)^2 \leq var(T-g(\theta))var(Z) = var(T)var(Z)} ">

Therefore,

<img src="https://render.githubusercontent.com/render/math?math={\displaystyle var(T) \geq \dfrac{(g\text{'}(\theta))^2}{var(Z)} = \dfrac{(g\text{'}(\theta))^2}{nI(\theta)}}">

### Example 4
Assume <img src="https://render.githubusercontent.com/render/math?math=X1, X_2, ..., X_n"> are samples from exponential distribution <img src="https://render.githubusercontent.com/render/math?math=Exp(\lambda)">, verify that <img src="https://render.githubusercontent.com/render/math?math=g(\lambda) = \dfrac{1}{\lambda}"> is a minimum variance unbiased estimation.

__Solution__. 

<img src="https://render.githubusercontent.com/render/math?math=E(\bar(X)) = g(\lambda) = \dfrac{1}{\lambda}, var(\bar{X}) = \dfrac{1}{n\lambda^2}">

<img src="https://render.githubusercontent.com/render/math?math=I(\lambda) = E\left(\left(\dfrac{\partial}{\partial \lambda}\ln p(X%3B \lambda)\right)^2\right) = E\left(\left(\dfrac{\partial}{\partial \lambda}\ln (\lambda e^{-\lambda X})\right)^2\right) = E\left(\left( \dfrac{1}{\lambda} - X\right)^2\right) = \dfrac{1}{\lambda^2}">

Therefore,

<img src="https://render.githubusercontent.com/render/math?math=\dfrac{(g\text{'}(\lambda))^2}{nI(\lambda)} = \dfrac{1}{n\lambda^2} = var(\bar{X})">

### Example 5
Assume <img src="https://render.githubusercontent.com/render/math?math=X1, X_2, ..., X_n"> are samples from Poisson distribution <img src="https://render.githubusercontent.com/render/math?math=P(\lambda)">, verify that <img src="https://render.githubusercontent.com/render/math?math=g(\lambda) = \lambda"> is a minimum variance unbiased estimation.

__Solution__. 

<img src="https://render.githubusercontent.com/render/math?math=E(\bar(X)) = g(\lambda) = \lambda, var(\bar{X}) = \dfrac{\lambda}{n}">

<img src="https://render.githubusercontent.com/render/math?math=I(\lambda) = E\left(\left(\dfrac{\partial}{\partial \lambda}\ln p(X%3B \lambda)\right)^2\right) = E\left(\left(\dfrac{\partial}{\partial \lambda}\ln \left(e^{-\lambda}\dfrac{\lambda^X}{X!}\right)\right)^2\right) = E\left(\left( \dfrac{X}{\lambda} - 1\right)^2\right) = \dfrac{1}{\lambda}">

Therefore,

<img src="https://render.githubusercontent.com/render/math?math=\dfrac{(g\text{'}(\lambda))^2}{nI(\lambda)} = \dfrac{\lambda}{n} = var(\bar{X})">

### Example 6
Assume <img src="https://render.githubusercontent.com/render/math?math=X1, X_2, ..., X_n"> are samples from normal distribution <img src="https://render.githubusercontent.com/render/math?math=N(\mu,\sigma^2)">, <img src="https://render.githubusercontent.com/render/math?math=\sigma^2"> is known, verify that <img src="https://render.githubusercontent.com/render/math?math=g(\mu) = \mu"> is a minimum variance unbiased estimation.

__Solution__. 

<img src="https://render.githubusercontent.com/render/math?math=E(\bar(X)) = g(\mu) = \mu, var(\bar{X}) = \dfrac{\sigma^2}{n}">

<img src="https://render.githubusercontent.com/render/math?math=I(\mu) = E\left(\left(\dfrac{\partial}{\partial \mu}\ln \left( \dfrac{1}{\sqrt{2\pi\sigma}}exp\left[-\dfrac{1}{2\sigma^2}(x-\mu)^2\right]\right)\right)^2\right) = E\left(\left( \dfrac{1}{\sigma^2}(x-\mu)\right)^2\right) = \dfrac{1}{\sigma^2}">

Therefore,

<img src="https://render.githubusercontent.com/render/math?math=\dfrac{(g\text{'}(\mu))^2}{nI(\mu)} = \dfrac{\sigma^2}{n} = var(\bar{X})">
