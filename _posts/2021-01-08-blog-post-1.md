---
title: 'Statistics [08]: Conditional Distributions and Expectation'
date: 2021-01-08
permalink: /posts/2021/01/08/conditonal-distributions-and-expectation/
tags:
  - Statistics
---

Conditional distributions and expectation of discrete and continuous random variables.

## Conditional Distributions of Discrete Random Variables
Conditional probability of discrete random variables can be expressed as 

<img src="https://render.githubusercontent.com/render/math?math=p_{i|j} = P(X=x_i|Y=y_j) = \dfrac{P(X=x_i, Y=y_j)}{P(Y=y_j)} = \dfrac{p_{ij}}{p_{.j}}">

where 

<img src="https://render.githubusercontent.com/render/math?math=P(Y=y_j) = p_{.j} = {\displaystyle \sum_{i=1}^\infty p_{ij}}">

Then, the distribution function of <img src="https://render.githubusercontent.com/render/math?math=X"> given <img src="https://render.githubusercontent.com/render/math?math=Y=y_j"> is 

<img src="https://render.githubusercontent.com/render/math?math=F(x|y_j) = {\displaystyle \sum_{x_i\leq x} P(X=x_i|Y=y_j)}">

### Additivity of Possion Distributions
Assume <img src="https://render.githubusercontent.com/render/math?math=X_1\sim P(\lambda_1)">, <img src="https://render.githubusercontent.com/render/math?math=X_2\sim P(\lambda_2)">, ..., <img src="https://render.githubusercontent.com/render/math?math=X_m\sim P(\lambda_m)"> and <img src="https://render.githubusercontent.com/render/math?math=X_1">, <img src="https://render.githubusercontent.com/render/math?math=X_2">, ..., <img src="https://render.githubusercontent.com/render/math?math=X_m"> are independent, then

<img src="https://render.githubusercontent.com/render/math?math=X_1 \%2B X_2 \%2B ... \%2B X_m \sim P(\lambda_1 \%2B \lambda_2 \%2B ... \%2B \lambda_m)">

---
__Proof__. For any non-negative <img src="https://render.githubusercontent.com/render/math?math=n">,

<img src="https://render.githubusercontent.com/render/math?math=P(X%2B Y = n) = {\displaystyle \sum_{k=0}^n P(X=k, Y=n-k) = \sum_{k=0}^nP(X=k)P(Y=n-k)}">

Hence,

<img src="https://render.githubusercontent.com/render/math?math=P(X%2B Y = n) = {\displaystyle \sum_{k=0}^n\dfrac{\lambda_1^k}{k!}e^{-\lambda_1}\dfrac{\lambda_2^{n-k}}{(n-k)!}e^{-\lambda_2}} = \dfrac{e^{-(\lambda_1 %2B \lambda_2)}}{n!}{\displaystyle \sum_{k=0}^n \dfrac{n!}{k!(n-k)!}\lambda_1^k\lambda_2^{n-k}} =  \dfrac{e^{-(\lambda_1 %2B \lambda_2)}}{n!}(\lambda_1%2B\lambda_2)^n">

Therefore,

<img src="https://render.githubusercontent.com/render/math?math=X%2B Y\sim P(\lambda_1 %2B \lambda_2)">

### Additivity of Binomial Distributions
Assume <img src="https://render.githubusercontent.com/render/math?math=X_1\sim B(n_1,p)">, <img src="https://render.githubusercontent.com/render/math?math=X_2\sim B(n_2,p)">, ..., <img src="https://render.githubusercontent.com/render/math?math=X_m\sim B(n_m,p)"> and <img src="https://render.githubusercontent.com/render/math?math=X_1">, <img src="https://render.githubusercontent.com/render/math?math=X_2">, ..., <img src="https://render.githubusercontent.com/render/math?math=X_m"> are independent, then

<img src="https://render.githubusercontent.com/render/math?math=X_1 \%2B X_2 \%2B ... \%2B X_m \sim B(n_1 \%2B n_1 \%2B ... \%2B n_m, p)">

---
__Proof__. For any non-negative <img src="https://render.githubusercontent.com/render/math?math=n">,

<img src="https://render.githubusercontent.com/render/math?math=P(X%2B Y = n) = {\displaystyle \sum_{k=0}^n \dbinom{n_1}{k}p^k(1-p)^{n_1-k}\dbinom{n_2}{n-k}p^{n-k}(1-p)^{n_2%2Bk-n} = \sum_{k=0}^n \dbinom{n}{n_1%2B n_2}p^n(1-p)^{n_2%2Bn_2-n}}">

Therefore,

<img src="https://render.githubusercontent.com/render/math?math=X%2B Y\sim B(n_1 %2B n_2, p)">

### Example 1
Assume <img src="https://render.githubusercontent.com/render/math?math=X"> and <img src="https://render.githubusercontent.com/render/math?math=Y"> are independent, <img src="https://render.githubusercontent.com/render/math?math=X\sim P(\lambda_1)">, <img src="https://render.githubusercontent.com/render/math?math=Y\sim P(\lambda_2)">. Find the conditional distribution of <img src="https://render.githubusercontent.com/render/math?math=X"> given <img src="https://render.githubusercontent.com/render/math?math=X%2B Y=n">.

__Solution__.

<img src="https://render.githubusercontent.com/render/math?math=P(X=k|X%2B Y=n) = \dfrac{P(X=k, Y=n-k)}{P(X%2B Y =n)} = \dfrac{ \dfrac{\lambda_1^k}{k!}e^{-\lambda_1}\dfrac{\lambda_2^{n-k}}{(n-k)!}e^{-\lambda_2}}{\dfrac{(\lambda_1 %2B \lambda_2)^n}{n!}e^{-(\lambda_1%2B\lambda_2)}} = \dbinom{n}{k}\dfrac{\lambda_1^k\lambda_2^{n-k}}{(\lambda_1%2B\lambda_2)^n}">

Therefore,


<img src="https://render.githubusercontent.com/render/math?math=P(X=k|X%2B Y=n) =B\left(n, \dfrac{\lambda_1}{\lambda_1 %2B \lambda_2} \right)">

---
## Conditional Distribution of Continuous Random Variables
For any <img src="https://render.githubusercontent.com/render/math?math=p_Y(y) > 0">, the conditional distribution function of <img src="https://render.githubusercontent.com/render/math?math=X"> given <img src="https://render.githubusercontent.com/render/math?math=Y=y"> is 

<img src="https://render.githubusercontent.com/render/math?math=F(x|y) = P(X\leq x| Y=y) = {\displaystyle \lim_{h\to 0}P(X\leq x|y\leq Y\leq Y%2B h) = \lim_{h\to 0}\dfrac{P(X\leq x, y \leq Y\leq y%2B h)}{P(y\leq Y \leq y%2B h)}}">

where 

<img src="https://render.githubusercontent.com/render/math?math={\displaystyle \lim_{h\to 0}P(X\leq x|y\leq Y\leq Y%2B h) = \lim_{h\to 0}\int_{-\infty}^x du\int_y^{y%2B h}p(u,v)dv = \lim_{h\to 0}\int_{-\infty}^x(p(u,y %2Bc_1h)h)du}">

and 

<img src="https://render.githubusercontent.com/render/math?math={\displaystyle \lim_{h\to 0}P(y\leq Y \leq y%2B h) = \lim_{h\to 0}\int_y^{y%2B h}p_Y(v)dv =\lim_{h\to 0} \int_y^{y %2B h}p_Y(y %2B c_2h)h}">

Hence,

<img src="https://render.githubusercontent.com/render/math?math=F(x|y) ={\displaystyle \dfrac{\int_{-\infty}^xp(u,y)du}{p_Y(y)} = \int_{-\infty}^x\dfrac{p(u,y)}{p_Y(y)}du}">

Therefore, the probability density function is 

<img src="https://render.githubusercontent.com/render/math?math=P(x|y) ={\displaystyle \dfrac{p(x,y)}{p_Y(y)} \sim p_{X|Y}(x|y)}">

### Example 2
Assume <img src="https://render.githubusercontent.com/render/math?math=(X,Y)"> has a uniform distribution on <img src="https://render.githubusercontent.com/render/math?math=G=\left\{ (x,y)%3B x^2 %2B y^2 \leq 1 \right\}">, find <img src="https://render.githubusercontent.com/render/math?math=p(x|y)">.

__Solution__.

<img src="https://render.githubusercontent.com/render/math?math=p(x, y) = \dfrac{1}{\pi}, x^2 %2B y^2 \leq 1">

<img src="https://render.githubusercontent.com/render/math?math=p_Y(y) = {\displaystyle \int_{-\infty}^{\infty}p(x,y)dx = \int_{-\sqrt{1-y^2}}^{\sqrt{1-y^2}}\dfrac{1}{\pi}dx = \dfrac{2}{\pi}\sqrt{1-y^2}, -1\leq y \leq 1}">

Therefore, 

<img src="https://render.githubusercontent.com/render/math?math=p(x|y) = \dfrac{p(x,y)}{p_Y(y)} = \dfrac{1}{2\sqrt{1-y^2}}, |y|\leq 1, x^2 %2B y^2 \leq 1">

### Example 3
Assume <img src="https://render.githubusercontent.com/render/math?math=(X,Y)"> has a bivariate normal distribution <img src="https://render.githubusercontent.com/render/math?math=N(\mu_1,\mu_2,\sigma_1^2,\sigma_2^2,\rho)">, find <img src="https://render.githubusercontent.com/render/math?math=p(y|x)">.

__Solution__.

<img src="https://render.githubusercontent.com/render/math?math=p(x, y) = \dfrac{1}{2\pi\sigma_1\sigma_2\sqrt{1-\rho^2}}\exp\left\{\dfrac{-1}{2(1-\rho^2)}\left[\dfrac{(x-\mu_1)^2}{\sigma_1^2} - 2\rho\dfrac{(x-\mu_1)(y-\mu_2)}{\sigma_1\sigma_2} %2B \dfrac{(y-\mu_2)^2}{\sigma_2^2}\right]\right\}">

<img src="https://render.githubusercontent.com/render/math?math=p_X(x) =  \dfrac{1}{\sqrt{2\pi}\sigma_1}\exp\left\{-\dfrac{(x-\mu_1)^2}{2\sigma_1^2}\right\}">

Hence,

<img src="https://render.githubusercontent.com/render/math?math=p(y|x) = \dfrac{1}{\sqrt{2\pi}\sigma_2\sqrt{1-\rho^2}}\exp\left\{\dfrac{-1}{2(1-\rho^2)}\left[\dfrac{(x-\mu_1)^2}{\sigma_1^2} - 2\rho\dfrac{(x-\mu_1)(y-\mu_2)}{\sigma_1\sigma_2} %2B \dfrac{(y-\mu_2)^2}{\sigma_2^2} - \dfrac{(1-\rho^2)(x-\mu_1)^2}{\sigma_1^2}\right]\right\}">

<img src="https://render.githubusercontent.com/render/math?math=p(y|x) = \dfrac{1}{\sqrt{2\pi}\sigma_2\sqrt{1-\rho^2}}\exp\left\{\dfrac{-1}{2\sigma_2^2(1-\rho^2)}\left[(y-\mu_2) - \rho\dfrac{\sigma_2}{\sigma_1}(x-\mu_1)\right]^2\right\}">

Therefore,

<img src="https://render.githubusercontent.com/render/math?math=p(y|x) \sim N\left( \mu_2 %2B \rho\dfrac{\sigma_2}{\sigma_1}(x-\mu_1), \sigma_2^2(1-\rho^2) \right)">

---
## Conditional Expectation
For __discrete__ random variables,

<img src="https://render.githubusercontent.com/render/math?math=E(X|Y=y) = {\displaystyle \sum_ix_iP(X=x_i|Y=y)}">

For __continuous__ random variables,

<img src="https://render.githubusercontent.com/render/math?math=E(X|Y=y) = {\displaystyle \int_{-\infty}^{\infty}xP(x|y)dx}">

### Example 4
Assume <img src="https://render.githubusercontent.com/render/math?math=X\sim Ge(\dfrad{1}{4})">, find <img src="https://render.githubusercontent.com/render/math?math=E(X|X>3)">.

__Solution__. 

<img src="https://render.githubusercontent.com/render/math?math=E(X|X>3) = {\displaystyle \sum_{k=1}^\infty (k%2B3)P(X=k%2B3|X>3) = \sum_{k=1}^\infty (k%2B3)P(X=k)}">

<img src="https://render.githubusercontent.com/render/math?math=E(X|X>3) =  {\displaystyle \sum_{k=1}^\infty kP(X=k) %2B \sum_{k=1}^\infty 3 P(X=k) = 4 %2B 3 = 7}">

Note that <img src="https://render.githubusercontent.com/render/math?math=E(X|Y)"> is a random variable!

### Example 5
Assume <img src="https://render.githubusercontent.com/render/math?math=X\sim Ge(p)"> and <img src="https://render.githubusercontent.com/render/math?math=\{Y=1,X=1%3BY=0,X>1\}">, find <img src="https://render.githubusercontent.com/render/math?math=E(X|Y)">.

__Solution__.

<img src="https://render.githubusercontent.com/render/math?math=E(X|Y=0) = E(X|X>1) = 1 %2B \dfrac{1}{p}, E(X|Y=0) = E(X|X=1) = 1">

<img src="https://render.githubusercontent.com/render/math?math=P(Y=1) = 0, P(Y=0) = 1-p">

Therefore, 

<img src="/images/statistics/ex2.png" alt="drawing" width="150"/>

From example 5, we have

<img src="https://render.githubusercontent.com/render/math?math=E(E(X|Y)) = (1 %2B \dfrac{1}{p})(1-p) %2B p = \dfrac{1}{p}">

This equation is called __law of total expectation__, which says

<img src="https://render.githubusercontent.com/render/math?math=E(E(X|Y)) E(X)">

__Proof__.

<img src="https://render.githubusercontent.com/render/math?math=E(X) = {\displaystyle \int_{-\infty}^\infty xp_x(x)dx = \int_{-\infty}^\infty x\int_{-\infty}^\infty p(x,y)dydx = \int_{-\infty}^\infty x\int_{-\infty}^\infty p_Y(y)p(x|y)dydx = \int_{-\infty}^\infty p_Y(y)dy\int_{-\infty}^\infty xp(x|y)dx}">

Hence,

<img src="https://render.githubusercontent.com/render/math?math=E(X) =  {\displaystyle \int_{-\infty}^\infty E(X|Y=y)p_Y(y)dy = E(E(X|Y))}">

### Example 6
For example 5, find <img src="https://render.githubusercontent.com/render/math?math=var(X)">.

__Solution__.

<img src="https://render.githubusercontent.com/render/math?math=E(X^2) = E(E(X^2|Y)) = P(X=1|)E(X^2|X=1) %2B P(X>1)E(X^2|X>1)">

where 

<img src="https://render.githubusercontent.com/render/math?math=E(X^2|X=1) = 1, E(X^2|X>1)=E((1%2BX)^2) = 1 %2B 2E(X) %2B E(X^2)">

Hence,

<img src="https://render.githubusercontent.com/render/math?math=E(X^2) = \dfrac{2}{p^2} - \dfrac{1}{p}">

Therefore,

<img src="https://render.githubusercontent.com/render/math?math=var(X) = E(X^2)-E^2(X) = \dfrac{1}{p^2} - \dfrac{1}{p}">

### Example 7
Assume <img src="https://render.githubusercontent.com/render/math?math=X_1,X_2,...,X_n"> are commonly independent random variables, <img src="https://render.githubusercontent.com/render/math?math=N"> is a integer random variable. Prove that 
<img src="https://render.githubusercontent.com/render/math?math=E{\displaystyle \left(  \sum_{k=1}^N X_k \right) = E(X_1)E(N)}">.

__Proof__.

<img src="https://render.githubusercontent.com/render/math?math=E{\displaystyle \left( \sum_{k=1}^N X_k \right) = E\left(  E\left( \sum_{k=1}^N X_k|N \right) \right) = \sum_{n=1}^\infty P(N=n)E\left(  \sum_{k=1}^N X_k|N=n \right)}">

As <img src="https://render.githubusercontent.com/render/math?math=X_1,X_2,...,X_n"> are commonly independent, we have 

<img src="https://render.githubusercontent.com/render/math?math=E{\displaystyle \left( \sum_{k=1}^N X_k \right) = \sum_{n=1}^\infty P(N=n)nE(X_1) = E(X_1)E(N)}">

